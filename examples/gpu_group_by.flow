// gpu_group_by.flow — GPU GROUP BY: Dispatch Fan-Out
//
// "Relational algebra becomes dispatch graph."
//
// 65,536 employees across 8 departments processed ENTIRELY on GPU.
// 64 dispatches, 0 CPU row iterations.
// Multi-pass parallel reduction per group.
//
// SQL equivalent:
//   SELECT dept, SUM(salary), COUNT(*), AVG(salary)
//   FROM employees
//   GROUP BY dept
//
// GPU dispatch fan-out (per group):
//   [eq_scalar: dept==g] → [mul_ab: salary×mask] →
//   [reduce pass 1] → [reduce pass 2] → [store_at: sums[g]] →
//   [reduce pass 1] → [reduce pass 2] → [store_at: counts[g]]
//
// 8 groups × 8 dispatches = 64 dispatches
//
// Run: octoflow run examples/gpu_group_by.flow --allow-ffi --allow-read

let N = 65536.0
let K = 8.0
let GROUP_SIZE = N / K
let WG_SIZE = 256.0
let NUM_WG = N / WG_SIZE

// ── Boot VMs for each shader type ────────────────────────────────────
let vm_eq = loom_boot(1.0, 2.0, N)
let vm_mul = loom_boot(1.0, 3.0, N)
let vm_r1s = loom_boot(1.0, 2.0, N)
let vm_r2s = loom_boot(1.0, 2.0, NUM_WG)
let vm_ss = loom_boot(1.0, 2.0, NUM_WG)
let vm_r1c = loom_boot(1.0, 2.0, N)
let vm_r2c = loom_boot(1.0, 2.0, NUM_WG)
let vm_sc = loom_boot(1.0, 2.0, NUM_WG)

loom_prefetch("tests/gpu_shaders/17_eq_scalar.spv")
loom_prefetch("tests/gpu_shaders/18_mul_ab.spv")
loom_prefetch("stdlib/loom/kernels/reduce/reduce_sum.spv")
loom_prefetch("tests/gpu_shaders/19_store_at.spv")

// Upload employee data: dept[i] = floor(i/GROUP_SIZE), salary[i] = i + 1
let mut depts = []
let mut salaries = []
let mut i = 0.0
while i < N
  push(depts, floor(i / GROUP_SIZE))
  push(salaries, i + 1.0)
  i = i + 1.0
end
vm_write_register(vm_eq, 0.0, 0.0, depts)
vm_write_register(vm_mul, 0.0, 0.0, salaries)

// Zero arrays
let mut zeros = []
let mut i = 0.0
while i < N
  push(zeros, 0.0)
  i = i + 1.0
end

let mut pz = []
let mut i = 0.0
while i < NUM_WG
  push(pz, 0.0)
  i = i + 1.0
end

// Initialize result accumulators (persist across groups on GPU)
vm_write_register(vm_ss, 0.0, 1.0, pz)
vm_write_register(vm_sc, 0.0, 1.0, pz)

// ── GROUP BY dispatch fan-out ──────────────────────────────────────
// 8 groups × 8 dispatches = 64 dispatches, 0 CPU row iterations
let t0 = time()

let mut g = 0.0
while g < K
  // Stage 1: filter — dept == g → boolean mask [256 workgroups]
  vm_write_register(vm_eq, 0.0, 1.0, zeros)
  loom_dispatch(vm_eq, "tests/gpu_shaders/17_eq_scalar.spv", [g], NUM_WG)
  let pe = loom_build(vm_eq)
  loom_run(pe)
  loom_free(pe)
  let mask = loom_read(vm_eq, 0.0, 1.0, N)

  // Stage 2: mask multiply — salary × mask → masked [256 workgroups]
  vm_write_register(vm_mul, 0.0, 1.0, mask)
  vm_write_register(vm_mul, 0.0, 2.0, zeros)
  loom_dispatch(vm_mul, "tests/gpu_shaders/18_mul_ab.spv", [], NUM_WG)
  let pm = loom_build(vm_mul)
  loom_run(pm)
  loom_free(pm)
  let masked = loom_read(vm_mul, 0.0, 2.0, N)

  // Stage 3: reduce SUM pass 1 — masked → 256 partials [256 workgroups]
  vm_write_register(vm_r1s, 0.0, 0.0, masked)
  vm_write_register(vm_r1s, 0.0, 1.0, pz)
  loom_dispatch(vm_r1s, "stdlib/loom/kernels/reduce/reduce_sum.spv", [], NUM_WG)
  let ps1 = loom_build(vm_r1s)
  loom_run(ps1)
  loom_free(ps1)
  let partial_sum = loom_read(vm_r1s, 0.0, 1.0, NUM_WG)

  // Stage 4: reduce SUM pass 2 — 256 partials → 1 value [1 workgroup]
  vm_write_register(vm_r2s, 0.0, 0.0, partial_sum)
  let mut rz = [0.0]
  vm_write_register(vm_r2s, 0.0, 1.0, rz)
  loom_dispatch(vm_r2s, "stdlib/loom/kernels/reduce/reduce_sum.spv", [], 1.0)
  let ps2 = loom_build(vm_r2s)
  loom_run(ps2)
  loom_free(ps2)
  let temp_sum = loom_read(vm_r2s, 0.0, 1.0, 1.0)

  // Stage 5: scatter — temp[0] → sums[g]
  vm_write_register(vm_ss, 0.0, 0.0, temp_sum)
  loom_dispatch(vm_ss, "tests/gpu_shaders/19_store_at.spv", [g], 1.0)
  let pss = loom_build(vm_ss)
  loom_run(pss)
  loom_free(pss)

  // Stage 6: reduce COUNT pass 1 — mask → 256 partials [256 workgroups]
  vm_write_register(vm_r1c, 0.0, 0.0, mask)
  vm_write_register(vm_r1c, 0.0, 1.0, pz)
  loom_dispatch(vm_r1c, "stdlib/loom/kernels/reduce/reduce_sum.spv", [], NUM_WG)
  let pc1 = loom_build(vm_r1c)
  loom_run(pc1)
  loom_free(pc1)
  let partial_count = loom_read(vm_r1c, 0.0, 1.0, NUM_WG)

  // Stage 7: reduce COUNT pass 2 — 256 partials → 1 value [1 workgroup]
  vm_write_register(vm_r2c, 0.0, 0.0, partial_count)
  let mut rz2 = [0.0]
  vm_write_register(vm_r2c, 0.0, 1.0, rz2)
  loom_dispatch(vm_r2c, "stdlib/loom/kernels/reduce/reduce_sum.spv", [], 1.0)
  let pc2 = loom_build(vm_r2c)
  loom_run(pc2)
  loom_free(pc2)
  let temp_count = loom_read(vm_r2c, 0.0, 1.0, 1.0)

  // Stage 8: scatter — temp[0] → counts[g]
  vm_write_register(vm_sc, 0.0, 0.0, temp_count)
  loom_dispatch(vm_sc, "tests/gpu_shaders/19_store_at.spv", [g], 1.0)
  let psc = loom_build(vm_sc)
  loom_run(psc)
  loom_free(psc)

  g = g + 1.0
end

let t1 = time()
let total_ms = (t1 - t0) * 1000.0

// ── Download results (K sums + K counts) ───────────────────────────
let gpu_sums = loom_read(vm_ss, 0.0, 1.0, K)
let gpu_counts = loom_read(vm_sc, 0.0, 1.0, K)

// ── CPU verification ───────────────────────────────────────────────
// dept d: employees d*8192..(d+1)*8192-1, salaries d*8192+1..(d+1)*8192
// SUM_d = (first + last) * count / 2
// COUNT_d = 8192
print("")
print("OctoFlow GPU GROUP BY Engine")
print("  Records: {N}")
print("  Departments: {K}")
print("  Query: SELECT dept, SUM(salary), COUNT(*), AVG(salary) GROUP BY dept")
print("  Dispatches: 64 (8 groups x 8 dispatches)")
print("  CPU row iterations: 0")
print("  Total time: {total_ms} ms")
print("")

let mut all_ok = 1.0
let mut g = 0.0
while g < K
  let first_sal = g * GROUP_SIZE + 1.0
  let last_sal = (g + 1.0) * GROUP_SIZE
  let exp_sum = (first_sal + last_sal) * GROUP_SIZE / 2.0
  let exp_count = GROUP_SIZE
  let gpu_sum = gpu_sums[int(g)]
  let gpu_count = gpu_counts[int(g)]
  let gpu_avg = gpu_sum / gpu_count

  let count_err = abs(gpu_count - exp_count)
  let sum_rel = abs(gpu_sum - exp_sum) / exp_sum

  let mut status = "OK"
  if count_err >= 1.0
    status = "FAIL"
    all_ok = 0.0
  end
  if sum_rel >= 0.001
    status = "FAIL"
    all_ok = 0.0
  end

  print("  dept {g}: SUM={gpu_sum}  COUNT={gpu_count}  AVG={gpu_avg}  [{status}]")
  g = g + 1.0
end

print("")
if all_ok == 1.0
  print("  ALL GROUPS MATCH (counts exact, sums within 0.1%)")
else
  print("  SOME GROUPS FAILED")
end

// Compute grand total as cross-check
let mut total_sum = 0.0
let mut total_count = 0.0
let mut i = 0.0
while i < K
  total_sum = total_sum + gpu_sums[int(i)]
  total_count = total_count + gpu_counts[int(i)]
  i = i + 1.0
end
let expected_total = N * (N + 1.0) / 2.0
let total_rel = abs(total_sum - expected_total) / expected_total
print("  Grand total: SUM={total_sum}  COUNT={total_count}  (expected SUM={expected_total})")
print("  Grand total relative error: {total_rel}")

print("")
loom_shutdown(vm_eq)
loom_shutdown(vm_mul)
loom_shutdown(vm_r1s)
loom_shutdown(vm_r2s)
loom_shutdown(vm_ss)
loom_shutdown(vm_r1c)
loom_shutdown(vm_r2c)
loom_shutdown(vm_sc)
