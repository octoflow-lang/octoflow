// ====================================================================
//  GPU PARALLEL SIEVE v2 — BIT-PACKED + ASYNC PIPELINE
//
//  Successor to v1, using 4 key optimizations:
//    1. Bit-packed odds-only: 1 bit per odd candidate (64x less memory)
//    2. Hardware popcount: OpBitCount + atomic add (replaces sequential)
//    3. Uint32 buffers: native integer ops throughout (no f32 conversion)
//    4. Async pipeline: vm_execute_async + vm_poll (overlapped execution)
//
//  Architecture: segmented trial-division sieve
//    - CPU computes small primes <= sqrt(N)
//    - 16 GPU VMs each sieve segments of 1M odd candidates
//    - Bit-packed: 31,250 uint32 words per segment (125 KB vs 4 MB in v1)
//    - 4 kernels per segment: init → mark(×batches) → count → accum
//
//  Memory layout per VM (uint32 buffer, reg_size=32768):
//    R0[0..31249]  — bitmap (1M odd candidates, 1 bit each)
//    R0[31250]     — segment popcount accumulator
//    R0[31251]     — running total accumulator
//
//  Kernels (emitted via IR builder with bitwise ops):
//    stdlib/loom/kernels/sieve/sieve_init_v2.spv   — init bitmap (all bits set)
//    stdlib/loom/kernels/sieve/sieve_mark_v2.spv   — mark composites (bit-clear)
//    stdlib/loom/kernels/sieve/sieve_count_v2.spv  — parallel popcount + atomic
//    stdlib/loom/kernels/sieve/sieve_accum_v2.spv  — total += seg_count, reset
// ====================================================================

print("")
print("====================================================================")
print("  GPU PARALLEL SIEVE v2 — BIT-PACKED + ASYNC")
print("  16 worker VMs × batch dispatch chains")
print("====================================================================")
print("")

// ════════════════════════════════════════════════════════════════════
//  CONFIGURATION
// ════════════════════════════════════════════════════════════════════

let N = 1000000000.0            // 10^9
let CANDS_PER_SEG = 1000000.0   // 1M odd candidates per segment
let NUM_VMS = 16.0
let PRIMES_PER_BATCH = 100.0

// Each segment covers CANDS_PER_SEG odd numbers = 2 * CANDS_PER_SEG actual numbers
let SEG_RANGE = CANDS_PER_SEG * 2.0    // numerical range per segment

// Bitmap: 1M candidates / 32 bits per word = 31250 words
let NUM_WORDS = floor(CANDS_PER_SEG / 32.0)

// Register buffer: bitmap + count + accum
let REG_SIZE = NUM_WORDS + 2.0   // 31252
let COUNT_OFF = NUM_WORDS        // 31250
let ACCUM_OFF = NUM_WORDS + 1.0  // 31251

// Number of segments (odds-only: segment k covers odd numbers in [k*SEG_RANGE+1, (k+1)*SEG_RANGE))
// Total odd numbers up to N: floor((N-1)/2)
let TOTAL_ODDS = floor((N - 1.0) / 2.0)
let NUM_SEGS = floor((TOTAL_ODDS + CANDS_PER_SEG - 1.0) / CANDS_PER_SEG)

// Workgroups for bitmap dispatch
let WG_BITMAP = floor((NUM_WORDS + 255.0) / 256.0)

// Kernel paths
let K_INIT  = "stdlib/loom/kernels/sieve/sieve_init_v2.spv"
let K_MARK  = "stdlib/loom/kernels/sieve/sieve_mark_v2.spv"
let K_COUNT = "stdlib/loom/kernels/sieve/sieve_count_v2.spv"
let K_ACCUM = "stdlib/loom/kernels/sieve/sieve_accum_v2.spv"

print("  Target N = {N:.0}")
print("  Candidates per segment = {CANDS_PER_SEG:.0} (odds only)")
print("  Segment range = {SEG_RANGE:.0}")
print("  Bitmap words = {NUM_WORDS:.0} ({NUM_WORDS:.0} × 4 = {REG_SIZE:.0} words)")
print("  Total segments = {NUM_SEGS:.0}")
print("  Worker VMs = {NUM_VMS:.0}")
print("")

// ════════════════════════════════════════════════════════════════════
//  STEP 1: CPU SIEVE — Compute small primes up to √N
// ════════════════════════════════════════════════════════════════════

let t0_cpu = now_ms()
let sqrt_n = floor(sqrt(N)) + 1.0

// Simple CPU sieve
let mut cpu_sieve = []
let mut csi = 0.0
while csi < sqrt_n + 1.0
  push(cpu_sieve, 1.0)
  csi = csi + 1.0
end
cpu_sieve[0] = 0.0
cpu_sieve[1] = 0.0

let mut p = 2.0
while p * p <= sqrt_n
  if cpu_sieve[p] > 0.5
    let mut m = p * p
    while m <= sqrt_n
      cpu_sieve[m] = 0.0
      m = m + p
    end
  end
  p = p + 1.0
end

// Collect ODD primes only (skip 2, since we only sieve odd candidates)
let mut small_primes = []
let mut spi = 3.0
while spi <= sqrt_n
  if cpu_sieve[spi] > 0.5
    push(small_primes, spi)
  end
  spi = spi + 2.0
end

let num_primes = len(small_primes)
let t_cpu = now_ms() - t0_cpu

print("  STEP 1: CPU sieve to sqrt({N:.0}) = {sqrt_n:.0}")
print("    Found {num_primes:.0} odd primes (excluding 2)")
if num_primes > 0.5
  let first_prime = small_primes[0]
  let last_prime = small_primes[num_primes - 1.0]
  print("    Range: {first_prime:.0} .. {last_prime:.0}")
end
print("    Time: {t_cpu:.0}ms")
print("")

// Pad primes to globals_size (4096)
let GLOBALS_SIZE = 4096.0
while len(small_primes) < GLOBALS_SIZE
  push(small_primes, 0.0)
end

// Number of mark dispatches per segment
let mark_dispatches = floor((num_primes + PRIMES_PER_BATCH - 1.0) / PRIMES_PER_BATCH)
let dispatches_per_seg = 1.0 + mark_dispatches + 1.0 + 1.0

print("  Dispatch plan:")
print("    Mark dispatches per segment: {mark_dispatches:.0}")
print("    Total per segment: {dispatches_per_seg:.0}")

// ════════════════════════════════════════════════════════════════════
//  STEP 2: BOOT VMs + WRITE PRIMES
// ════════════════════════════════════════════════════════════════════

let t0_boot = now_ms()

let mut vms = []
let mut bi = 0.0
while bi < NUM_VMS
  let r = try(vm_boot(1.0, REG_SIZE, GLOBALS_SIZE))
  if r.ok > 0.5
    push(vms, r.value)
    let _w = vm_write_globals(r.value, 0.0, small_primes)
  else
    print("    BOOT FAILED at VM #{bi:.0}: {r.error}")
    bi = NUM_VMS
  end
  bi = bi + 1.0
end

let t_boot = now_ms() - t0_boot
let booted = len(vms)
let mem_kb = booted * REG_SIZE * 4.0 / 1024.0

let mut active_vms = booted
if active_vms > NUM_SEGS
  active_vms = NUM_SEGS
end

print("")
print("  STEP 2: Booted {booted:.0} VMs in {t_boot:.0}ms (~{mem_kb:.0}KB VRAM)")
print("    Active VMs: {active_vms:.0}")
print("")

// ════════════════════════════════════════════════════════════════════
//  STEP 3: RECORD DISPATCH CHAINS
// ════════════════════════════════════════════════════════════════════

let t0_chain = now_ms()
let segs_per_vm = floor((NUM_SEGS + active_vms - 1.0) / active_vms)

// Recompute: how many VMs actually needed? (avoid empty dispatch chains)
let needed_vms = floor((NUM_SEGS + segs_per_vm - 1.0) / segs_per_vm)
if active_vms > needed_vms
  active_vms = needed_vms
end

let mut total_dispatches = 0.0
let mut vi = 0.0

while vi < active_vms
  let seg_start_idx = vi * segs_per_vm
  let mut seg_end_idx = (vi + 1.0) * segs_per_vm
  if seg_end_idx > NUM_SEGS
    seg_end_idx = NUM_SEGS
  end

  // Push constant arrays
  let mut pc_init  = [0.0, 0.0, 0.0, 0.0]
  let mut pc_mark  = [0.0, 0.0, 0.0, 0.0]
  let pc_count = [NUM_WORDS, COUNT_OFF]
  let pc_accum = [COUNT_OFF, ACCUM_OFF]

  let mut si = seg_start_idx
  while si < seg_end_idx
    // Pass segment INDEX (small int, exact in f32) instead of seg_start
    // The GPU computes seg_start = seg_idx * SEG_RANGE + 1 using uint32 arithmetic

    // Init: set all bitmap bits
    pc_init[0] = si              // seg_idx (small, exact in f32)
    pc_init[1] = NUM_WORDS       // num_words
    pc_init[2] = N               // limit
    pc_init[3] = 0.0
    let _d0 = vm_dispatch(vms[vi], K_INIT, pc_init, WG_BITMAP)
    total_dispatches = total_dispatches + 1.0

    // Mark: batch through all primes
    let mut pi = 0.0
    while pi < num_primes
      let mut pe = pi + PRIMES_PER_BATCH
      if pe > num_primes
        pe = num_primes
      end
      pc_mark[0] = si            // seg_idx (small, exact in f32)
      pc_mark[1] = NUM_WORDS
      pc_mark[2] = pi
      pc_mark[3] = pe
      let _dm = vm_dispatch(vms[vi], K_MARK, pc_mark, WG_BITMAP)
      total_dispatches = total_dispatches + 1.0
      pi = pe
    end

    // Count: parallel popcount + atomic
    let _dc = vm_dispatch(vms[vi], K_COUNT, pc_count, WG_BITMAP)
    total_dispatches = total_dispatches + 1.0

    // Accum: total += seg_count, reset
    let _da = vm_dispatch(vms[vi], K_ACCUM, pc_accum, 1.0)
    total_dispatches = total_dispatches + 1.0

    si = si + 1.0
  end

  vi = vi + 1.0
end

let t_chain = now_ms() - t0_chain
let per_vm = total_dispatches / active_vms

print("  STEP 3: Recorded dispatch chains in {t_chain:.0}ms")
print("    Total dispatches: {total_dispatches:.0}")
print("    Per VM: ~{per_vm:.0}")
print("")

// ════════════════════════════════════════════════════════════════════
//  STEP 4: BUILD + EXECUTE (ASYNC)
// ════════════════════════════════════════════════════════════════════

let t0_build = now_ms()
let mut progs = []
let mut bdi = 0.0
while bdi < active_vms
  let prog = vm_build(vms[bdi])
  push(progs, prog)
  bdi = bdi + 1.0
end
let t_build = now_ms() - t0_build

// Async execution: launch all VMs, then poll until all done
let t0_exec = now_ms()

let mut ei = 0.0
while ei < active_vms
  let _ea = vm_execute_async(progs[ei])
  ei = ei + 1.0
end

// Poll loop: check each VM until all complete
let mut done_flags = []
let mut di = 0.0
while di < active_vms
  push(done_flags, 0.0)
  di = di + 1.0
end

let mut done_count = 0.0
while done_count < active_vms
  let mut pi = 0.0
  while pi < active_vms
    if done_flags[pi] < 0.5
      let status = vm_poll(progs[pi])
      if status > 0.5
        done_flags[pi] = 1.0
        done_count = done_count + 1.0
      end
    end
    pi = pi + 1.0
  end
end

let t_exec = now_ms() - t0_exec

print("  STEP 4: GPU execution (async)")
print("    Build: {t_build:.0}ms")
print("    Execute: {t_exec:.0}ms  ({total_dispatches:.0} dispatches)")
print("")

// ════════════════════════════════════════════════════════════════════
//  STEP 5: READ RESULTS + TALLY
// ════════════════════════════════════════════════════════════════════

// Count = 1 for prime 2 (not in GPU sieve since we only do odd primes)
let mut total_count = 1.0

// Read ACCUM_OFF from each VM's register buffer
// The value is stored as uint32, read back via float_to_bits
let mut ri = 0.0
while ri < active_vms
  // Read register at ACCUM_OFF (offset in the uint32 buffer)
  // vm_read_register(vm, instance, register, count) — reads f32 values
  // For uint32 buffers, the raw bits come through as f32; use float_to_bits to interpret
  // Register 0, offset = ACCUM_OFF, count = 1
  let result = vm_read_register(vms[ri], 0.0, 0.0, REG_SIZE)
  // The accum is at index ACCUM_OFF in the buffer
  let accum_raw = result[ACCUM_OFF]
  let vm_count = float_to_bits(accum_raw)

  let seg_start_idx = ri * segs_per_vm
  let mut seg_end_idx = (ri + 1.0) * segs_per_vm
  if seg_end_idx > NUM_SEGS
    seg_end_idx = NUM_SEGS
  end
  let vm_segs = seg_end_idx - seg_start_idx
  print("    VM {ri:.0}: {vm_count:.0} primes ({vm_segs:.0} segments)")

  total_count = total_count + vm_count
  ri = ri + 1.0
end

let t_total = now_ms() - t0_cpu

print("")
print("  ════════════════════════════════════════════")
print("  RESULT: pi({N:.0}) = {total_count:.0}")
print("  ════════════════════════════════════════════")

// ════════════════════════════════════════════════════════════════════
//  STEP 6: AUDIT AGAINST KNOWN VALUES
// ════════════════════════════════════════════════════════════════════

let mut expected = 0.0
if N > 999000000.0
  expected = 50847534.0
elif N > 99000000.0
  expected = 5761455.0
elif N > 9900000.0
  expected = 664579.0
elif N > 990000.0
  expected = 78498.0
end

if expected > 0.5
  let diff = total_count - expected
  if diff > -0.5 && diff < 0.5
    print("  AUDIT: PASS  (exact match: {total_count:.0})")
  else
    print("  AUDIT: FAIL  (expected {expected:.0} got {total_count:.0}, diff={diff:.0})")
  end
end

print("")
print("  TIMING SUMMARY")
print("  ──────────────────────────────────────────")
print("    CPU primes:     {t_cpu:.0}ms")
print("    Boot VMs:       {t_boot:.0}ms")
print("    Record chain:   {t_chain:.0}ms")
print("    Build programs: {t_build:.0}ms")
print("    GPU execute:    {t_exec:.0}ms (async)")
print("    TOTAL:          {t_total:.0}ms")
print("  ──────────────────────────────────────────")
print("    Dispatches:     {total_dispatches:.0}")
print("    GPU submits:    {active_vms:.0} (all async)")
print("")

// ════════════════════════════════════════════════════════════════════
//  CLEANUP
// ════════════════════════════════════════════════════════════════════

let mut si2 = 0.0
while si2 < booted
  let _s = vm_shutdown(vms[si2])
  si2 = si2 + 1.0
end

print("====================================================================")
print("")
