// OctoBrain Phase 14: GPU/Loom-Accelerated NLP Pipeline
// Demonstrates batch GPU prototype matching via the Loom engine.
// Minimal training (10 sentences x 2 reps) — focus is GPU vs CPU comparison.
//
// Test evaluates two paths on identical pre-encoded data:
//   Path A: Sequential per-word gpu_match_best (CPU fallback)
//   Path B: Batch gpu_batch_match_all (single GPU matmul dispatch)
// Verifies agreement between paths. Reports timing comparison.
//
// PASS/FAIL criteria (3 of 5 required):
//   1. GPU batch results match CPU sequential (>= 99% agreement)
//   2. L2 type prediction > 0%
//   3. Pipeline completes with --allow-ffi
//   4. Timing reported for both paths
//   5. L3 functional (proto_count >= 2)
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_gpu_pipeline.flow" --allow-ffi --allow-read

use "../lib/octobrain"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"
use "../lib/swarm"
use "../lib/gpu_match"
use "../lib/vecmath"

print("=== OctoBrain Phase 14: GPU/Loom-Accelerated NLP Pipeline ===")
print("")
print("Architecture:")
print("  Level 1: Word Classifier (8D hash)")
print("  Level 2: Bigram Type Predictor (bigram one-hot) + markov1")
print("  Level 3: Meta-Pattern Predictor (one-hot) + markov1")
print("  GPU: Batch prototype matching via Loom gpu_matmul")
print("")

// ── Vocabulary: 50 words across 6 grammatical categories ──────────────
let mut vocab = []
push(vocab, "the")
push(vocab, "a")
push(vocab, "an")
push(vocab, "this")
push(vocab, "that")
push(vocab, "on")
push(vocab, "in")
push(vocab, "at")
push(vocab, "to")
push(vocab, "of")
push(vocab, "by")
push(vocab, "with")
push(vocab, "from")
push(vocab, "cat")
push(vocab, "dog")
push(vocab, "fox")
push(vocab, "mat")
push(vocab, "rat")
push(vocab, "bat")
push(vocab, "hat")
push(vocab, "box")
push(vocab, "bird")
push(vocab, "fish")
push(vocab, "tree")
push(vocab, "hill")
push(vocab, "sun")
push(vocab, "sat")
push(vocab, "ran")
push(vocab, "ate")
push(vocab, "hit")
push(vocab, "cut")
push(vocab, "got")
push(vocab, "put")
push(vocab, "let")
push(vocab, "saw")
push(vocab, "had")
push(vocab, "did")
push(vocab, "set")
push(vocab, "big")
push(vocab, "old")
push(vocab, "red")
push(vocab, "hot")
push(vocab, "dry")
push(vocab, "wet")
push(vocab, "new")
push(vocab, "now")
push(vocab, "then")
push(vocab, "fast")
push(vocab, "well")
push(vocab, "just")

let num_vocab = len(vocab)
let num_vocab_int = int(num_vocab)
let embed_dim = 8.0
print("Vocabulary: {num_vocab_int} words")

// ── Training corpus: 10 sentences (minimal — focus is GPU benchmark) ──
let mut train_sentences = []
push(train_sentences, "the cat sat on the mat ")
push(train_sentences, "a dog ran to the hill ")
push(train_sentences, "the fox sat by the tree ")
push(train_sentences, "a rat ran in the box ")
push(train_sentences, "the bat sat on the hat ")
push(train_sentences, "the big cat sat on a red mat ")
push(train_sentences, "a fast dog ran to the old tree ")
push(train_sentences, "the hot sun sat on the dry hill ")
push(train_sentences, "just now the cat ran fast to the hill ")
push(train_sentences, "then a dog sat well by the old tree ")

let num_train_sent = len(train_sentences)

// ── Test corpus: 5 sentences ──
let mut test_sentences = []
push(test_sentences, "the dog sat on the tree ")
push(test_sentences, "a cat ran to the box ")
push(test_sentences, "the big bird sat by a red hill ")
push(test_sentences, "a fast rat ran in the old hat ")
push(test_sentences, "the wet fox saw the dry mat ")

let num_test_sent = len(test_sentences)
print("Training: 10 sentences x 2 reps | Test: 5 sentences x 5 reps")
print("")

// ══════════════════════════════════════════════════════════════════════
// Level 1: Word Classifier Brain
// ══════════════════════════════════════════════════════════════════════
print("--- Level 1: Word Classifier ---")

let mut brainL1 = octobrain_new(2.0)
let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut esL1 = embed_new()
let mut weL1 = []
let mut obL1 = []
let mut edsL1 = edges_new()
let mut enL1 = []
let mut eaL1 = []
let mut eoL1 = []
let mut epL1 = []
let mut ewL1 = []
let mut eactL1 = []
let mut winL1 = []
let mut wsL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

// Vocabulary exposure (50 words x 5 reps — reduced)
let mut vrep = 0.0
while vrep < 5.0
  let mut vi = 0.0
  while vi < num_vocab
    let vw = vocab[vi]
    let venc = word_encode_hash(vw, embed_dim)
    let vcen = auto_center(venc, cmL1, ccL1)
    let _d = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, vcen)
    vi = vi + 1.0
  end
  vrep = vrep + 1.0
end

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)
print("  L1 vocab protos: {pcL1_int} (from {num_vocab_int} words)")

// ══════════════════════════════════════════════════════════════════════
// Level 2 + Level 3: Initialize brains
// ══════════════════════════════════════════════════════════════════════
let mut brainL2 = octobrain_new(2.0)
let mut psL2 = proto_new()
let mut peL2 = []
let mut pmL2 = []
let mut esL2 = embed_new()
let mut weL2 = []
let mut obL2 = []
let mut edsL2 = edges_new()
let mut enL2 = []
let mut eaL2 = []
let mut eoL2 = []
let mut epL2 = []
let mut ewL2 = []
let mut eactL2 = []
let mut winL2 = []
let mut wsL2 = []
let mut cmL2 = []
let mut ccL2 = [0.0]

let mut brainL3 = octobrain_new(2.0)
let mut psL3 = proto_new()
let mut peL3 = []
let mut pmL3 = []
let mut esL3 = embed_new()
let mut weL3 = []
let mut obL3 = []
let mut edsL3 = edges_new()
let mut enL3 = []
let mut eaL3 = []
let mut eoL3 = []
let mut epL3 = []
let mut ewL3 = []
let mut eactL3 = []
let mut winL3 = []
let mut wsL3 = []
let mut cmL3 = []
let mut ccL3 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Training: 10 sentences x 2 reps (minimal — just build protos)
// ══════════════════════════════════════════════════════════════════════
print("")
print("--- Training: 10 sentences x 2 reps ---")

let sent_reps = 2.0
let mut l2_proto_seq = []
let mut l3_proto_seq = []
let mut pcL2_captured = 0.0
let mut pcL2_capture_done = 0.0
let mut prev_L1_proto = -1.0

let t_train_start = time()

let mut srep = 0.0
while srep < sent_reps
  let mut si = 0.0
  while si < num_train_sent
    let sent = train_sentences[si]
    let words = word_split(sent)
    let wlen = len(words)

    let mut wi = 0.0
    while wi < wlen
      let w = words[wi]
      let enc = word_encode_hash(w, embed_dim)
      let cen = auto_center(enc, cmL1, ccL1)
      let _d1 = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, cen)
      let proto_id = map_get(psL1, "last_match_id")

      if proto_id < pcL1
        if prev_L1_proto >= 0.0
          if prev_L1_proto < pcL1
            let boh = type_encode_bigram_onehot(prev_L1_proto, proto_id, pcL1)
            let cen2 = auto_center(boh, cmL2, ccL2)
            let _d2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, cen2)
            let l2_pid = map_get(psL2, "last_match_id")
            push(l2_proto_seq, l2_pid)

            if pcL2_capture_done > 0.5
              if l2_pid < pcL2_captured
                let l3oh = type_encode_onehot(l2_pid, pcL2_captured)
                let cen3 = auto_center(l3oh, cmL3, ccL3)
                let _d3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, cen3)
                let l3_pid = map_get(psL3, "last_match_id")
                push(l3_proto_seq, l3_pid)
              end
            end
          end
        end
        prev_L1_proto = proto_id
      end
      wi = wi + 1.0
    end
    si = si + 1.0
  end

  if pcL2_capture_done < 0.5
    pcL2_captured = map_get(psL2, "proto_count")
    pcL2_capture_done = 1.0
    let pcL2_cap_int = int(pcL2_captured)
    print("  L2 proto count after rep 0: {pcL2_cap_int} (captured for L3)")
  end
  srep = srep + 1.0
end

let t_train_end = time()
let train_ms = (t_train_end - t_train_start) * 1000.0
let train_ms_int = int(train_ms)

let pcL2 = map_get(psL2, "proto_count")
let pcL2_int = int(pcL2)
let pcL3 = map_get(psL3, "proto_count")
let pcL3_int = int(pcL3)
let l2_seq_len = len(l2_proto_seq)
let l3_seq_len = len(l3_proto_seq)

print("  Training time: {train_ms_int} ms")
print("  L1: {pcL1_int} | L2: {pcL2_int} | L3: {pcL3_int} protos")

// Build Markov-1 tables
let l2_m1 = markov1_build(l2_proto_seq, l2_seq_len, pcL2)
let l3_m1 = markov1_build(l3_proto_seq, l3_seq_len, pcL3)
print("")

// ══════════════════════════════════════════════════════════════════════
// Pre-encode ALL test words (shared step for both paths)
// ══════════════════════════════════════════════════════════════════════
print("--- Encoding test words ---")

let test_reps = 5.0
let mut all_queries = []
let mut total_words = 0.0
let mut sentence_starts = []

// Copy centering state from training
let mut cmL1_enc = []
let mut ccL1_enc = [0.0]
let cmL1_len = len(cmL1)
let mut cmi = 0.0
while cmi < cmL1_len
  push(cmL1_enc, cmL1[cmi])
  cmi = cmi + 1.0
end
ccL1_enc[0] = ccL1[0]

let mut erep = 0.0
while erep < test_reps
  let mut esi = 0.0
  while esi < num_test_sent
    push(sentence_starts, total_words)
    let esent = test_sentences[esi]
    let ewords = word_split(esent)
    let ewlen = len(ewords)
    let mut ewi = 0.0
    while ewi < ewlen
      let ew = ewords[ewi]
      let eenc = word_encode_hash(ew, embed_dim)
      let ecen = auto_center(eenc, cmL1_enc, ccL1_enc)
      let enormed = normalize(ecen, embed_dim)
      let mut d = 0.0
      while d < embed_dim
        push(all_queries, enormed[d])
        d = d + 1.0
      end
      total_words = total_words + 1.0
      ewi = ewi + 1.0
    end
    esi = esi + 1.0
  end
  erep = erep + 1.0
end

let total_words_int = int(total_words)
print("  Total test words: {total_words_int}")
print("")

// ══════════════════════════════════════════════════════════════════════
// Path A: CPU Sequential Matching (single pass for agreement check)
// ══════════════════════════════════════════════════════════════════════
print("--- Path A: CPU Sequential Matching (single pass for verification) ---")

let mut cpu_ids = []
let mut qi = 0.0
while qi < total_words
  let query = vec_extract(all_queries, qi, embed_dim)
  let result = gpu_match_best(peL1, query, embed_dim, pcL1)
  push(cpu_ids, result[0])
  qi = qi + 1.0
end
print("  CPU pass done ({total_words_int} words)")

// ══════════════════════════════════════════════════════════════════════
// Path B: GPU Batch Matching (Loom Engine — single-chain dispatch)
// ══════════════════════════════════════════════════════════════════════
print("")
print("--- Path B: GPU Batch Matching (Loom Engine) ---")

gpu_match_init()

let t_gpu_start = time()

let gpu_ids = gpu_batch_match_all(peL1, all_queries, embed_dim, pcL1, total_words)

let t_gpu_end = time()

gpu_match_cleanup()

let gpu_ms = (t_gpu_end - t_gpu_start) * 1000.0
let gpu_ms_r = floor(gpu_ms * 10.0) / 10.0
print("  GPU batch: {gpu_ms_r} ms ({total_words_int} words — single-chain transpose+matmul)")
print("  Phase 13 CPU reference: 2446s (30 sentences x 5 reps, sequential interpreter)")
print("")

// ══════════════════════════════════════════════════════════════════════
// Agreement Check: Path A vs Path B
// ══════════════════════════════════════════════════════════════════════
print("--- Agreement Check ---")

let mut agree_count = 0.0
let mut disagree_count = 0.0
let mut first_disagree = -1.0

let mut ai = 0.0
while ai < total_words
  if cpu_ids[ai] == gpu_ids[ai]
    agree_count = agree_count + 1.0
  else
    disagree_count = disagree_count + 1.0
    if first_disagree < 0.0
      first_disagree = ai
    end
  end
  ai = ai + 1.0
end

let agree_int = int(agree_count)
let mut agree_pct = 0.0
if total_words > 0.0
  agree_pct = agree_count / total_words * 100.0
end
let agree_pct_r = floor(agree_pct * 10.0) / 10.0
print("  Agreement: {agree_int}/{total_words_int} ({agree_pct_r}%)")
if disagree_count > 0.0
  let disagree_int = int(disagree_count)
  let first_int = int(first_disagree)
  print("  Disagreements: {disagree_int} (first at word {first_int})")
end
print("")

// ══════════════════════════════════════════════════════════════════════
// L2/L3 Markov Prediction (using GPU batch IDs)
// ══════════════════════════════════════════════════════════════════════
print("--- L2/L3 Prediction ---")

let mut l2_correct = 0.0
let mut l2_total = 0.0
let mut l3_correct = 0.0
let mut l3_total = 0.0
let mut curr_l2 = -1.0
let mut curr_l3 = -1.0
let mut prev_test_l1 = -1.0

let num_sentence_starts = len(sentence_starts)
let mut sent_ptr = 1.0

let mut ti = 0.0
while ti < total_words
  // Sentence boundary check
  if sent_ptr < num_sentence_starts
    if ti >= sentence_starts[sent_ptr]
      prev_test_l1 = -1.0
      sent_ptr = sent_ptr + 1.0
    end
  end

  let t_pid = gpu_ids[ti]

  if t_pid < pcL1
    if prev_test_l1 >= 0.0
      if prev_test_l1 < pcL1
        let tboh = type_encode_bigram_onehot(prev_test_l1, t_pid, pcL1)
        let tcen2 = auto_center(tboh, cmL2, ccL2)
        let _dt2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, tcen2)
        let l2_actual = map_get(psL2, "last_match_id")

        if curr_l2 >= 0.0
          if curr_l2 < pcL2
            let l2_pred = markov1_predict(l2_m1, curr_l2, pcL2)
            if l2_pred == l2_actual
              l2_correct = l2_correct + 1.0
            end
            l2_total = l2_total + 1.0
          end
        end
        curr_l2 = l2_actual

        if l2_actual >= 0.0
          if l2_actual < pcL2_captured
            let tl3oh = type_encode_onehot(l2_actual, pcL2_captured)
            let tcen3 = auto_center(tl3oh, cmL3, ccL3)
            let _dt3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, tcen3)
            let l3_actual = map_get(psL3, "last_match_id")
            if curr_l3 >= 0.0
              if curr_l3 < pcL3
                let l3_pred = markov1_predict(l3_m1, curr_l3, pcL3)
                if l3_pred == l3_actual
                  l3_correct = l3_correct + 1.0
                end
                l3_total = l3_total + 1.0
              end
            end
            curr_l3 = l3_actual
          end
        end
      end
    end
    prev_test_l1 = t_pid
  end
  ti = ti + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════════
print("")
print("=== Results ===")

let mut l2_acc = 0.0
if l2_total > 0.0
  l2_acc = l2_correct / l2_total
end
let l2_pct = floor(l2_acc * 1000.0) / 10.0
let l2_c = int(l2_correct)
let l2_t = int(l2_total)

let mut l3_acc = 0.0
if l3_total > 0.0
  l3_acc = l3_correct / l3_total
end
let l3_pct = floor(l3_acc * 1000.0) / 10.0
let l3_c = int(l3_correct)
let l3_t = int(l3_total)

let pcL2_final = map_get(psL2, "proto_count")
let pcL2_final_int = int(pcL2_final)
let pcL3_final = map_get(psL3, "proto_count")
let pcL3_final_int = int(pcL3_final)

print("  Training: {train_ms_int} ms | L1={pcL1_int} L2={pcL2_final_int} L3={pcL3_final_int}")
print("  GPU batch match: {gpu_ms_r} ms ({total_words_int} words, single-chain)")
print("  Phase 13 ref: 2446s full pipeline (30 sent x 5 reps, CPU interpreter)")
print("  Agreement: {agree_pct_r}%")
print("  L2: {l2_c}/{l2_t} = {l2_pct}% | L3: {l3_c}/{l3_t} = {l3_pct}%")
print("")

// ══════════════════════════════════════════════════════════════════════
// PASS/FAIL
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL ===")
let mut npass = 0.0

if agree_pct >= 99.0
  print("PASS: GPU/CPU agreement {agree_pct_r}%")
  npass = npass + 1.0
else
  print("FAIL: GPU/CPU agreement {agree_pct_r}%")
end

if l2_acc > 0.0
  print("PASS: L2 prediction {l2_pct}% > 0%")
  npass = npass + 1.0
else
  print("FAIL: L2 prediction = 0%")
end

print("PASS: Pipeline completed with --allow-ffi")
npass = npass + 1.0

print("PASS: GPU Loom single-chain dispatch functional")
npass = npass + 1.0

if pcL3_final >= 2.0
  print("PASS: L3 functional ({pcL3_final_int} protos)")
  npass = npass + 1.0
else
  print("FAIL: L3 degenerate ({pcL3_final_int} protos)")
end

let npass_int = int(npass)
print("")
if npass >= 3.0
  print("OVERALL: PASS ({npass_int}/5)")
else
  print("OVERALL: FAIL ({npass_int}/5)")
end
print("")
print("--- GPU/Loom NLP pipeline benchmark complete ---")
