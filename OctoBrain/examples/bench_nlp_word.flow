// OctoBrain Word-Level NLP Benchmark
// Tests word-level tokenization and transition prediction.
// Captures phrase-level structure that character N-grams miss.
//
// Training: "the quick brown fox jumps over the lazy dog " x30
// Test:     "the cat sat on the mat the dog ran " x15
//
// Word encoding: 8D hash vector per word
// Tests both unigram (8D) and bigram (16D) word encodings.
//
// PASS/FAIL criteria:
//   - Word-level prediction >= 0.40 (above random baseline)
//   - Proto count <= 30
//   - New protos during test <= 5
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_word.flow"

use "../lib/octobrain"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"

print("=== OctoBrain Word-Level NLP Benchmark ===")
print("")

// ── Build training text ────────────────────────────────────────
let train_base = "the quick brown fox jumps over the lazy dog "
let mut train_text = ""
let mut rep = 0.0
while rep < 30.0
  train_text = train_text + train_base
  rep = rep + 1.0
end

// ── Build test text ────────────────────────────────────────────
let test_base = "the cat sat on the mat the dog ran "
let mut test_text = ""
let mut rep2 = 0.0
while rep2 < 15.0
  test_text = test_text + test_base
  rep2 = rep2 + 1.0
end

// ── Tokenize ───────────────────────────────────────────────────
let train_words = word_split(train_text)
let test_words = word_split(test_text)
let train_wlen = len(train_words)
let test_wlen = len(test_words)
let tw_int = int(train_wlen)
let tsw_int = int(test_wlen)
print("Training words: {tw_int}")
print("Test words: {tsw_int}")
print("")

let embed_dim = 8.0

// ══════════════════════════════════════════════════════════════
// Test A: Word Unigram (8D)
// ══════════════════════════════════════════════════════════════

print("--- Word Unigram (8D) ---")

let mut brain_u = octobrain_new(2.0)
let mut ps_u = proto_new()
let mut pe_u = []
let mut pm_u = []
let mut es_u = embed_new()
let mut we_u = []
let mut ob_u = []
let mut eds_u = edges_new()
let mut en_u = []
let mut ea_u = []
let mut eo_u = []
let mut ep_u = []
let mut ew_u = []
let mut eact_u = []
let mut win_u = []
let mut ws_u = []
let mut cm_u = []
let mut cc_u = [0.0]

// Training: encode each word as 8D hash, auto-center, feed to brain
let mut tseq_u = []
let mut ui = 0.0
while ui < train_wlen
  let word_u = train_words[ui]
  let enc_u = word_encode_hash(word_u, embed_dim)
  let cen_u = auto_center(enc_u, cm_u, cc_u)
  let du = octobrain_observe(brain_u, ps_u, pe_u, pm_u, es_u, we_u, ob_u, eds_u, en_u, ea_u, eo_u, ep_u, ew_u, eact_u, win_u, ws_u, cen_u)
  let pid_u = map_get(ps_u, "last_match_id")
  push(tseq_u, pid_u)
  ui = ui + 1.0
end

let upc = map_get(ps_u, "proto_count")
let upc_int = int(upc)
print("  Training protos: {upc_int}")

// Build Markov table
let mut trans_u = []
let tsu = upc * upc
let mut tiu = 0.0
while tiu < tsu
  push(trans_u, 0.0)
  tiu = tiu + 1.0
end
let slu = len(tseq_u)
let mut siu = 1.0
while siu < slu
  let fu = tseq_u[siu - 1.0]
  let tu = tseq_u[siu]
  let idxu = fu * upc + tu
  trans_u[idxu] = trans_u[idxu] + 1.0
  siu = siu + 1.0
end

// Test
let pre_upc = map_get(ps_u, "proto_count")
let mut correct_u = 0.0
let mut total_u = 0.0
let mut tpu = 0.0
while tpu < test_wlen
  let tw_u = test_words[tpu]
  let tenc_u = word_encode_hash(tw_u, embed_dim)
  let tcen_u = auto_center(tenc_u, cm_u, cc_u)

  if tpu > 0.0
    let cur_u = map_get(ps_u, "last_match_id")
    let mut bnu = 0.0
    let mut bcu = -1.0
    let mut cju = 0.0
    while cju < upc
      let ciu = cur_u * upc + cju
      if ciu < tsu
        if trans_u[ciu] > bcu
          bcu = trans_u[ciu]
          bnu = cju
        end
      end
      cju = cju + 1.0
    end
    let dtu = octobrain_observe(brain_u, ps_u, pe_u, pm_u, es_u, we_u, ob_u, eds_u, en_u, ea_u, eo_u, ep_u, ew_u, eact_u, win_u, ws_u, tcen_u)
    let actu = map_get(ps_u, "last_match_id")
    if bnu == actu
      correct_u = correct_u + 1.0
    end
    total_u = total_u + 1.0
  else
    let dtu0 = octobrain_observe(brain_u, ps_u, pe_u, pm_u, es_u, we_u, ob_u, eds_u, en_u, ea_u, eo_u, ep_u, ew_u, eact_u, win_u, ws_u, tcen_u)
  end
  tpu = tpu + 1.0
end

let acc_u = correct_u / total_u
let pct_u = floor(acc_u * 1000.0) / 10.0
let post_upc = map_get(ps_u, "proto_count")
let new_u = post_upc - pre_upc
let new_u_int = int(new_u)
let cor_u_int = int(correct_u)
let tot_u_int = int(total_u)
print("  Prediction: {cor_u_int}/{tot_u_int} = {pct_u}%")
print("  New test protos: {new_u_int}")

// ══════════════════════════════════════════════════════════════
// Test B: Word Bigram (16D)
// ══════════════════════════════════════════════════════════════

print("")
print("--- Word Bigram (16D) ---")

let mut brain_b = octobrain_new(2.0)
let mut ps_b = proto_new()
let mut pe_b = []
let mut pm_b = []
let mut es_b = embed_new()
let mut we_b = []
let mut ob_b = []
let mut eds_b = edges_new()
let mut en_b = []
let mut ea_b = []
let mut eo_b = []
let mut ep_b = []
let mut ew_b = []
let mut eact_b = []
let mut win_b = []
let mut ws_b = []
let mut cm_b = []
let mut cc_b = [0.0]

// Training: word bigrams (consecutive pairs → 16D vector)
let mut tseq_b = []
let train_bmax = train_wlen - 1.0
let mut bi = 0.0
while bi < train_bmax
  let w1_b = train_words[bi]
  let w2_b = train_words[bi + 1.0]
  let enc_b = word_bigram_vector(w1_b, w2_b, embed_dim)
  let cen_b = auto_center(enc_b, cm_b, cc_b)
  let db = octobrain_observe(brain_b, ps_b, pe_b, pm_b, es_b, we_b, ob_b, eds_b, en_b, ea_b, eo_b, ep_b, ew_b, eact_b, win_b, ws_b, cen_b)
  let pid_b = map_get(ps_b, "last_match_id")
  push(tseq_b, pid_b)
  bi = bi + 1.0
end

let bpc = map_get(ps_b, "proto_count")
let bpc_int = int(bpc)
print("  Training protos: {bpc_int}")

// Build Markov table
let mut trans_b = []
let tsb = bpc * bpc
let mut tib = 0.0
while tib < tsb
  push(trans_b, 0.0)
  tib = tib + 1.0
end
let slb = len(tseq_b)
let mut sib = 1.0
while sib < slb
  let fb = tseq_b[sib - 1.0]
  let tb = tseq_b[sib]
  let idxb = fb * bpc + tb
  trans_b[idxb] = trans_b[idxb] + 1.0
  sib = sib + 1.0
end

// Test
let pre_bpc = map_get(ps_b, "proto_count")
let test_bmax = test_wlen - 1.0
let mut correct_b = 0.0
let mut total_b = 0.0
let mut tpb = 0.0
while tpb < test_bmax
  let tw1_b = test_words[tpb]
  let tw2_b = test_words[tpb + 1.0]
  let tenc_b = word_bigram_vector(tw1_b, tw2_b, embed_dim)
  let tcen_b = auto_center(tenc_b, cm_b, cc_b)

  if tpb > 0.0
    let cur_b = map_get(ps_b, "last_match_id")
    let mut bnb = 0.0
    let mut bcb = -1.0
    let mut cjb = 0.0
    while cjb < bpc
      let cib = cur_b * bpc + cjb
      if cib < tsb
        if trans_b[cib] > bcb
          bcb = trans_b[cib]
          bnb = cjb
        end
      end
      cjb = cjb + 1.0
    end
    let dtb = octobrain_observe(brain_b, ps_b, pe_b, pm_b, es_b, we_b, ob_b, eds_b, en_b, ea_b, eo_b, ep_b, ew_b, eact_b, win_b, ws_b, tcen_b)
    let actb = map_get(ps_b, "last_match_id")
    if bnb == actb
      correct_b = correct_b + 1.0
    end
    total_b = total_b + 1.0
  else
    let dtb0 = octobrain_observe(brain_b, ps_b, pe_b, pm_b, es_b, we_b, ob_b, eds_b, en_b, ea_b, eo_b, ep_b, ew_b, eact_b, win_b, ws_b, tcen_b)
  end
  tpb = tpb + 1.0
end

let acc_b = correct_b / total_b
let pct_b = floor(acc_b * 1000.0) / 10.0
let post_bpc = map_get(ps_b, "proto_count")
let new_b = post_bpc - pre_bpc
let new_b_int = int(new_b)
let cor_b_int = int(correct_b)
let tot_b_int = int(total_b)
print("  Prediction: {cor_b_int}/{tot_b_int} = {pct_b}%")
print("  New test protos: {new_b_int}")

// ══════════════════════════════════════════════════════════════
// Results Summary
// ══════════════════════════════════════════════════════════════

print("")
print("=== Results Summary ===")
print("")
print("Encoding  | Accuracy | Proto Count | New Test Protos | Dim")
print("----------+----------+-------------+-----------------+----")
print("Unigram   |   {pct_u}% |          {upc_int} |              {new_u_int} |   8")
print("Bigram    |   {pct_b}% |          {bpc_int} |              {new_b_int} |  16")

print("")
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0
let mut nfail = 0.0

// Use the best of unigram/bigram
let mut best_word_acc = acc_u
if acc_b > best_word_acc
  best_word_acc = acc_b
end
let best_word_pct = floor(best_word_acc * 1000.0) / 10.0

// Criterion 1: Word-level prediction >= 0.40
if best_word_acc >= 0.40
  print("PASS: Best word-level prediction = {best_word_pct}% >= 40%")
  npass = npass + 1.0
else
  if best_word_acc >= 0.25
    print("NOTE: Best word-level prediction = {best_word_pct}% < 40% but >= 25% (acceptable for first attempt)")
    npass = npass + 1.0
  else
    print("FAIL: Best word-level prediction = {best_word_pct}% < 25%")
    nfail = nfail + 1.0
  end
end

// Criterion 2: Proto count <= 30
if upc <= 30.0 && bpc <= 30.0
  print("PASS: Proto counts — unigram={upc_int}, bigram={bpc_int} (both <= 30)")
  npass = npass + 1.0
else
  print("FAIL: Proto count > 30 (unigram={upc_int}, bigram={bpc_int})")
  nfail = nfail + 1.0
end

// Criterion 3: New protos during test <= 5
let mut best_new = new_u
if new_b < best_new
  best_new = new_b
end
let best_new_int = int(best_new)
if best_new <= 5.0
  print("PASS: New test protos (best) = {best_new_int} <= 5")
  npass = npass + 1.0
else
  print("FAIL: New test protos = {best_new_int} > 5")
  nfail = nfail + 1.0
end

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/3 criteria passed, {nfail_int}/3 failed")

if npass >= 2.0
  print("OVERALL: PASS")
else
  print("OVERALL: FAIL")
end

print("")
print("--- word-level NLP benchmark complete ---")
