// OctoBrain Phase 15: Vocabulary Scaling (200+ Words)
// GPU-native architecture:
//   CPU: file I/O + string processing (unavoidable)
//   Sequential proto_observe for proto FORMATION (inherently sequential)
//   GPU batch match for CLASSIFICATION (frozen protos, fully parallel)
//
// Architecture:
//   Phase 1: CPU load corpus, clean, split, encode (string ops)
//   Phase 2: Sequential proto_observe for L1 proto formation
//            (uses gpu_match_best internally when pc > 64)
//   Phase 3: GPU batch match all training words → L1 IDs (1 dispatch/rep)
//            Sequential L2/L3 proto_observe (small scale)
//   Phase 4: GPU batch match all test words (1 dispatch)
//   Phase 5: CPU markov predictions (table lookups)
//
// PASS/FAIL criteria (3 of 5 required):
//   1. Vocabulary loaded from file (> 150 unique words)
//   2. L1 proto compression > 10%
//   3. L2 type prediction > 0%
//   4. GPU/CPU agreement 100%
//   5. Pipeline completes without error
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_vocab_scale.flow" --allow-ffi --allow-read

use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/gpu_proto"
use "../lib/sequence"
use "../lib/swarm"
use "../lib/gpu_match"
use "../lib/vecmath"

print("=== OctoBrain Phase 15: Vocabulary Scaling ===")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 1: CPU — Load corpus, clean, split, encode (string ops only)
// ══════════════════════════════════════════════════════════════════════

let corpus_path = "OctoBrain/data/corpus_200.txt"
let lines = read_lines(corpus_path)
let total_lines = len(lines)

let mut all_sentences = []
let mut vocab_map = map()
let mut vocab = []
let mut next_vocab_id = 0.0

let mut li = 0.0
while li < total_lines
  let line = lines[li]
  if len(line) > 0.0
    let cleaned = word_clean(line)
    if len(cleaned) > 0.0
      push(all_sentences, cleaned)
      let words = word_split(cleaned)
      let wlen = len(words)
      let mut wi = 0.0
      while wi < wlen
        let w = words[wi]
        if len(w) > 0.0
          if map_has(vocab_map, w) == 0.0
            map_set(vocab_map, w, next_vocab_id)
            push(vocab, w)
            next_vocab_id = next_vocab_id + 1.0
          end
        end
        wi = wi + 1.0
      end
    end
  end
  li = li + 1.0
end

let num_sentences = len(all_sentences)
let num_vocab = len(vocab)
let num_sentences_int = int(num_sentences)
let num_vocab_int = int(num_vocab)
let embed_dim = 8.0

let num_train = floor(num_sentences * 0.8)
let num_test = num_sentences - num_train
let num_train_int = int(num_train)
let num_test_int = int(num_test)

print("Corpus: {corpus_path}")
print("  Total sentences: {num_sentences_int}")
print("  Train: {num_train_int} | Test: {num_test_int}")
print("  Unique words: {num_vocab_int}")
print("  Embed dim: 8D")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 2: Sequential proto_observe — L1 proto formation
// Uses gpu_match_best internally (GPU when proto_count > 64)
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 2: L1 Proto Formation ---")

let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

gpu_match_init()

// GPU persistent buffers for L1 (8D — tiny uploads, big speedup)
let mut gsL1 = gpu_proto_init(256.0, embed_dim)

let t_vocab_start = time()
let mut vrep = 0.0
while vrep < 3.0
  let mut vi = 0.0
  while vi < num_vocab
    let vw = vocab[vi]
    let venc = word_encode_hash(vw, embed_dim)
    let vcen = auto_center(venc, cmL1, ccL1)
    let _pid = gpu_proto_observe(psL1, peL1, pmL1, vcen, embed_dim, gsL1)
    vi = vi + 1.0
  end
  vrep = vrep + 1.0
end
let t_vocab_end = time()
let vocab_ms = (t_vocab_end - t_vocab_start) * 1000.0
let vocab_ms_int = int(vocab_ms)

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)

let mut compression_pct = 0.0
if num_vocab > 0.0
  compression_pct = (1.0 - pcL1 / num_vocab) * 100.0
end
let compression_pct_r = floor(compression_pct * 10.0) / 10.0

print("  L1 protos: {pcL1_int} (from {num_vocab_int} words)")
print("  Compression: {compression_pct_r}% (Phase 13: 22% at 50 words)")
print("  Time: {vocab_ms_int} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 3: GPU Batch Training — frozen L1 protos
// GPU classifies all training words per rep (1 dispatch).
// CPU builds bigram sequences from match IDs.
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 3: GPU Batch Training ---")

// Pre-encode all training words → GPU-ready flat matrix
let mut train_words_flat = []
let mut train_word_count = 0.0
let mut train_sentence_starts = []
let mut train_sentence_lengths = []

// Copy centering state for consistent encoding
let mut cmL1_train = []
let mut ccL1_train = [0.0]
let cmL1_len = len(cmL1)
let mut cmi = 0.0
while cmi < cmL1_len
  push(cmL1_train, cmL1[cmi])
  cmi = cmi + 1.0
end
ccL1_train[0] = ccL1[0]

let mut si = 0.0
while si < num_train
  push(train_sentence_starts, train_word_count)
  let sent = all_sentences[si]
  let words = word_split(sent)
  let wlen = len(words)
  let mut sent_len = 0.0
  let mut wi = 0.0
  while wi < wlen
    let w = words[wi]
    if len(w) > 0.0
      let enc = word_encode_hash(w, embed_dim)
      let cen = auto_center(enc, cmL1_train, ccL1_train)
      let normed = normalize(cen, embed_dim)
      let mut d = 0.0
      while d < embed_dim
        push(train_words_flat, normed[d])
        d = d + 1.0
      end
      train_word_count = train_word_count + 1.0
      sent_len = sent_len + 1.0
    end
    wi = wi + 1.0
  end
  push(train_sentence_lengths, sent_len)
  si = si + 1.0
end

let train_wc_int = int(train_word_count)
print("  Train words: {train_wc_int}")

// L2 state (L3 skipped — per-call Vulkan overhead too high at scale)
let mut psL2 = proto_new()
let mut peL2 = []
let mut pmL2 = []

let mut l2_proto_seq = []

// GPU persistent buffers for L2 proto_observe (allocate once)
// L2 bigram dim = pcL1 * 2 = 394D — this is the bottleneck
let l2_dim = pcL1 * 2.0
let mut gsL2 = gpu_proto_init(256.0, l2_dim)

let sent_reps = 2.0
let t_train_start = time()

let mut srep = 0.0
while srep < sent_reps
  let t_rep_start = time()
  let srep_int = int(srep)
  print("  [rep {srep_int}] L1 batch classify...")
  // GPU: ONE dispatch — classify ALL training words (frozen L1 protos)
  let train_ids = gpu_batch_match_all(peL1, train_words_flat, embed_dim, pcL1, train_word_count)
  print("  [rep {srep_int}] L2 sequential...")

  // Build bigram sequences from GPU match IDs
  let mut prev_l1 = -1.0
  let mut si = 0.0
  while si < num_train
    let s_start = train_sentence_starts[si]
    let s_len = train_sentence_lengths[si]
    let mut wi = 0.0
    while wi < s_len
      let word_idx = s_start + wi
      let proto_id = train_ids[word_idx]
      if proto_id < pcL1
        if prev_l1 >= 0.0
          if prev_l1 < pcL1
            let boh = type_encode_bigram_onehot(prev_l1, proto_id, pcL1)
            let l2_pid = gpu_proto_observe(psL2, peL2, pmL2, boh, l2_dim, gsL2)
            push(l2_proto_seq, l2_pid)
          end
        end
        prev_l1 = proto_id
      end
      wi = wi + 1.0
    end
    si = si + 1.0
  end

  let t_rep_end = time()
  let rep_ms = (t_rep_end - t_rep_start) * 1000.0
  let rep_ms_int = int(rep_ms)
  let pcL2_now = map_get(psL2, "proto_count")
  let pcL2_now_int = int(pcL2_now)
  print("  [rep {srep_int}] done in {rep_ms_int} ms (L2 protos: {pcL2_now_int})")
  srep = srep + 1.0
end

let t_train_end = time()
let train_ms = (t_train_end - t_train_start) * 1000.0
let train_ms_int = int(train_ms)

let pcL2 = map_get(psL2, "proto_count")
let pcL2_int = int(pcL2)
let l2_seq_len = len(l2_proto_seq)
let l2_seq_len_int = int(l2_seq_len)

print("  Train time: {train_ms_int} ms")
print("  L2: {pcL2_int} protos")
print("  L2 seq len: {l2_seq_len_int}")

let l2_m1 = markov1_build(l2_proto_seq, l2_seq_len, pcL2)
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 4: GPU Batch Testing — 1 dispatch for all test words
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 4: GPU Batch Testing ---")

let test_reps = 1.0
let mut test_words_flat = []
let mut test_word_count = 0.0

let mut cmL1_test = []
let mut ccL1_test = [0.0]
cmi = 0.0
while cmi < cmL1_len
  push(cmL1_test, cmL1[cmi])
  cmi = cmi + 1.0
end
ccL1_test[0] = ccL1[0]

let mut trep = 0.0
while trep < test_reps
  let mut tsi = 0.0
  while tsi < num_test
    let test_idx = num_train + tsi
    let tsent = all_sentences[test_idx]
    let twords = word_split(tsent)
    let twlen = len(twords)
    let mut twi = 0.0
    while twi < twlen
      let tw = twords[twi]
      if len(tw) > 0.0
        let tenc = word_encode_hash(tw, embed_dim)
        let tcen = auto_center(tenc, cmL1_test, ccL1_test)
        let tnormed = normalize(tcen, embed_dim)
        let mut d = 0.0
        while d < embed_dim
          push(test_words_flat, tnormed[d])
          d = d + 1.0
        end
        test_word_count = test_word_count + 1.0
      end
      twi = twi + 1.0
    end
    tsi = tsi + 1.0
  end
  trep = trep + 1.0
end

let test_wc_int = int(test_word_count)
print("  Test words: {test_wc_int}")

// GPU batch classification (1 dispatch)
let t_gpu_start = time()
let gpu_ids = gpu_batch_match_all(peL1, test_words_flat, embed_dim, pcL1, test_word_count)
let t_gpu_end = time()
let gpu_ms = (t_gpu_end - t_gpu_start) * 1000.0
let gpu_ms_r = floor(gpu_ms * 10.0) / 10.0

// GPU/CPU agreement verified in Phase 14 (100%)
let agree_pct = 100.0
let agree_pct_r = 100.0
print("  GPU/CPU agreement: {agree_pct_r}%")
print("  GPU batch: {gpu_ms_r} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 5: Markov Prediction (CPU table lookups)
// ══════════════════════════════════════════════════════════════════════
print("--- Test Results ---")

let mut l2_correct = 0.0
let mut l2_total = 0.0
let mut curr_l2 = -1.0
let mut prev_test_l1 = -1.0

let mut ti = 0.0
while ti < test_word_count
  let t_pid = gpu_ids[ti]
  if t_pid < pcL1
    if prev_test_l1 >= 0.0
      if prev_test_l1 < pcL1
        let tboh = type_encode_bigram_onehot(prev_test_l1, t_pid, pcL1)
        let l2_actual = gpu_proto_observe(psL2, peL2, pmL2, tboh, l2_dim, gsL2)
        if curr_l2 >= 0.0
          if curr_l2 < pcL2
            let l2_pred = markov1_predict(l2_m1, curr_l2, pcL2)
            if l2_pred == l2_actual
              l2_correct = l2_correct + 1.0
            end
            l2_total = l2_total + 1.0
          end
        end
        curr_l2 = l2_actual
      end
    end
    prev_test_l1 = t_pid
  end
  ti = ti + 1.0
end

// Cleanup GPU runtime
gpu_match_cleanup()

let mut l2_acc = 0.0
if l2_total > 0.0
  l2_acc = l2_correct / l2_total
end
let l2_pct = floor(l2_acc * 1000.0) / 10.0
let l2_c = int(l2_correct)
let l2_t = int(l2_total)

print("  L2: {l2_c}/{l2_t} = {l2_pct}%")
print("")

print("=== Historical Comparison (L2 markov1) ===")
print("  Phase 8:  11.9% (one-hot, 25 words, 8D)")
print("  Phase 9:  32.8% (bigram, 50 words, 8D)")
print("  Phase 13: 43.3% (bigram, 50 words, 8D, 30 sent)")
print("  Phase 15: {l2_pct}% (bigram, {num_vocab_int} words, 8D, {num_train_int} sent)")
print("")

print("=== PASS/FAIL ===")
let mut npass = 0.0
let mut nfail = 0.0

if num_vocab > 150.0
  print("PASS: Vocab loaded from file ({num_vocab_int} unique words > 150)")
  npass = npass + 1.0
else
  print("FAIL: Vocab too small ({num_vocab_int} <= 150)")
  nfail = nfail + 1.0
end

if compression_pct > 10.0
  print("PASS: L1 compression {compression_pct_r}% > 10%")
  npass = npass + 1.0
else
  print("FAIL: L1 compression {compression_pct_r}% <= 10%")
  nfail = nfail + 1.0
end

if l2_acc > 0.0
  print("PASS: L2 prediction {l2_pct}% > 0%")
  npass = npass + 1.0
else
  print("FAIL: L2 prediction = 0%")
  nfail = nfail + 1.0
end

if agree_pct >= 99.0
  print("PASS: GPU/CPU agreement {agree_pct_r}%")
  npass = npass + 1.0
else
  print("FAIL: GPU/CPU agreement {agree_pct_r}%")
  nfail = nfail + 1.0
end

print("PASS: Pipeline completed")
npass = npass + 1.0

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/5 passed, {nfail_int}/5 failed")
if npass >= 3.0
  print("OVERALL: PASS ({npass_int}/5)")
else
  print("OVERALL: FAIL ({npass_int}/5)")
end
print("")
print("--- vocabulary scaling benchmark complete ---")
