// OctoBrain Phase 17: Generative NLP — From Statistical to Neural
// Architecture:
//   L1 prototype matching at dim=16 (threshold=0.6)
//   GPU batch classification via gpu_matmul
//   Reverse vocabulary + similarity matrix for soft proto activation
//   Strategy A: 1st-order Markov (statistical baseline)
//   Strategy B: Context-blended neural generation (4 neural properties)
//
// Neural properties demonstrated:
//   1. Representation ambiguity — words activate multiple protos
//   2. Context-sensitive boundaries — thresholds shift per context
//   3. Competing prototype activation — multiple protos emit words
//   4. Cross-level feedback — surprise modulates next threshold
//
// PASS/FAIL criteria:
//   1. Perplexity on ch12 < 50
//   2. Vocab coverage > 80%
//   3. Strategy A bigram coherence > 5%
//   4. Strategy B > Strategy A (bigram coherence)
//   5. Strategy B bigram coherence > 10%
//   6. Vocabulary diversity (both) > 30 unique per 100
//   7. Pipeline completes without error
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_alice_generate.flow" --allow-ffi --allow-read

use "../lib/text_word"
use "../lib/proto"
use "../lib/sequence"

print("=== OctoBrain Phase 17: Generative NLP ===")
print("    From Statistical to Neural")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 1: Load Corpus + Train/Test Split
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 1: Load Corpus + Train/Test Split ---")
let t_load_start = time()

let corpus_path = "OctoBrain/data/alice.txt"
let lines = read_lines(corpus_path)
let total_lines = len(lines)

let mut all_words = []
let mut word_chapters = []
let mut chapter_word_starts = []
let mut vocab_map = map()
let mut vocab = []
let mut next_vocab_id = 0.0
let mut current_chapter = 0.0
let mut num_chapters = 0.0

let mut li = 0.0
while li < total_lines
  let line = lines[li]
  if starts_with(line, "CHAPTER")
    current_chapter = current_chapter + 1.0
    num_chapters = num_chapters + 1.0
    push(chapter_word_starts, len(all_words))
  elif len(line) > 0.0
    if current_chapter > 0.0
      let words = word_clean_split_fast(line)
      let wlen = len(words)
      let mut wi = 0.0
      while wi < wlen
        let w = words[wi]
        if len(w) > 0.0
          push(all_words, w)
          push(word_chapters, current_chapter)
          if map_has(vocab_map, w) == 0.0
            map_set(vocab_map, w, next_vocab_id)
            push(vocab, w)
            next_vocab_id = next_vocab_id + 1.0
          end
        end
        wi = wi + 1.0
      end
    end
  end
  li = li + 1.0
end

// Sentinel for chapter boundary
push(chapter_word_starts, len(all_words))

let total_words = len(all_words)
let num_vocab = len(vocab)
let embed_dim = 16.0

// Train/test split: chapters 1-11 = train, chapter 12 = test
let train_end_idx = chapter_word_starts[11]
let test_start_idx = chapter_word_starts[11]
let test_end_idx = chapter_word_starts[12]
let train_word_count = train_end_idx
let test_word_count = test_end_idx - test_start_idx

let t_load_end = time()
let load_ms = int((t_load_end - t_load_start) * 1000.0)
let total_words_int = int(total_words)
let num_vocab_int = int(num_vocab)
let train_int = int(train_word_count)
let test_int = int(test_word_count)
print("  Corpus: {total_words_int} words, {num_vocab_int} unique")
print("  Train: chapters 1-11 ({train_int} words)")
print("  Test:  chapter 12 ({test_int} words)")
print("  Time: {load_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 2: L1 Prototype Formation
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 2: L1 Prototype Formation ---")
let t_proto_start = time()

let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

// Rep 1: inline CPU proto matching
let threshold_r1 = compute_threshold(embed_dim)

let mut normed_buf = []
let mut di = 0.0
while di < embed_dim
  push(normed_buf, 0.0)
  di = di + 1.0
end

let mut vi = 0.0
while vi < num_vocab
  let vw = vocab[vi]
  let venc = word_encode_hash(vw, embed_dim)

  let cc = ccL1[0]
  if cc < 0.5
    let mut d = 0.0
    while d < embed_dim
      push(cmL1, venc[d])
      d = d + 1.0
    end
    ccL1[0] = 1.0
  else
    let mut d = 0.0
    while d < embed_dim
      cmL1[d] = 0.99 * cmL1[d] + 0.01 * venc[d]
      d = d + 1.0
    end
    ccL1[0] = cc + 1.0

    let mut norm_sq = 0.0
    d = 0.0
    while d < embed_dim
      let c = venc[d] - cmL1[d]
      norm_sq = norm_sq + c * c
      d = d + 1.0
    end

    if norm_sq > 0.000001
      let inv_norm = 1.0 / sqrt(norm_sq)
      d = 0.0
      while d < embed_dim
        normed_buf[d] = (venc[d] - cmL1[d]) * inv_norm
        d = d + 1.0
      end

      let pcL1_now = map_get(psL1, "proto_count")
      let mut best_id = -1.0
      let mut best_sim = -2.0
      let mut p = 0.0
      while p < pcL1_now
        let base = p * embed_dim
        let mut dot = 0.0
        d = 0.0
        while d < embed_dim
          dot = dot + peL1[base + d] * normed_buf[d]
          d = d + 1.0
        end
        if dot > best_sim
          best_sim = dot
          best_id = p
        end
        p = p + 1.0
      end

      if pcL1_now == 0.0 || best_sim < threshold_r1
        d = 0.0
        while d < embed_dim
          push(peL1, normed_buf[d])
          d = d + 1.0
        end
        push(pmL1, 1.0)
        map_set(psL1, "proto_count", pcL1_now + 1.0)
      else
        let base = best_id * embed_dim
        let mut dnorm_sq = 0.0
        d = 0.0
        while d < embed_dim
          let dv = 0.9 * peL1[base + d] + 0.1 * normed_buf[d]
          dnorm_sq = dnorm_sq + dv * dv
          d = d + 1.0
        end
        let dinv = 1.0 / sqrt(dnorm_sq)
        d = 0.0
        while d < embed_dim
          peL1[base + d] = (0.9 * peL1[base + d] + 0.1 * normed_buf[d]) * dinv
          d = d + 1.0
        end
        pmL1[best_id] = pmL1[best_id] + 1.0
      end
    end
  end

  vi = vi + 1.0
end

// Batch-encode all vocab for reps 2+3
let mut vocab_batch_flat = []
vi = 0.0
while vi < num_vocab
  let vw = vocab[vi]
  let venc = word_encode_hash(vw, embed_dim)
  let mut norm_sq = 0.0
  let mut d = 0.0
  while d < embed_dim
    let c = venc[d] - cmL1[d]
    norm_sq = norm_sq + c * c
    d = d + 1.0
  end
  let mut inv_norm = 0.0
  if norm_sq > 0.0000001
    inv_norm = 1.0 / sqrt(norm_sq)
  end
  d = 0.0
  while d < embed_dim
    let c = venc[d] - cmL1[d]
    push(vocab_batch_flat, c * inv_norm)
    d = d + 1.0
  end
  vi = vi + 1.0
end

// Reps 2+3: gpu_matmul batch scoring
let batch_threshold = compute_threshold(embed_dim)
let mut brep = 0.0
while brep < 2.0
  let pcL1_now = map_get(psL1, "proto_count")
  let mut peL1_T = []
  let mut td = 0.0
  while td < embed_dim
    let mut tp = 0.0
    while tp < pcL1_now
      push(peL1_T, peL1[tp * embed_dim + td])
      tp = tp + 1.0
    end
    td = td + 1.0
  end
  let sims = gpu_matmul(vocab_batch_flat, peL1_T, num_vocab, pcL1_now, embed_dim)
  let mut bvi = 0.0
  while bvi < num_vocab
    let row_base = bvi * pcL1_now
    let mut best_id = 0.0
    let mut best_sim = sims[row_base]
    let mut p = 1.0
    while p < pcL1_now
      let s = sims[row_base + p]
      if s > best_sim
        best_sim = s
        best_id = p
      end
      p = p + 1.0
    end
    if best_sim >= batch_threshold
      let base = best_id * embed_dim
      let vbase = bvi * embed_dim
      let mut dnorm = 0.0
      let mut d = 0.0
      while d < embed_dim
        let dv = 0.9 * peL1[base + d] + 0.1 * vocab_batch_flat[vbase + d]
        dnorm = dnorm + dv * dv
        d = d + 1.0
      end
      let dinv = 1.0 / sqrt(dnorm)
      d = 0.0
      while d < embed_dim
        peL1[base + d] = (0.9 * peL1[base + d] + 0.1 * vocab_batch_flat[vbase + d]) * dinv
        d = d + 1.0
      end
      pmL1[best_id] = pmL1[best_id] + 1.0
    else
      let vbase = bvi * embed_dim
      let mut d = 0.0
      while d < embed_dim
        push(peL1, vocab_batch_flat[vbase + d])
        d = d + 1.0
      end
      push(pmL1, 1.0)
      map_set(psL1, "proto_count", map_get(psL1, "proto_count") + 1.0)
    end
    bvi = bvi + 1.0
  end
  brep = brep + 1.0
end

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)
let compression_pct = (1.0 - pcL1 / num_vocab) * 100.0
let compression_r = floor(compression_pct * 10.0) / 10.0

// Batch classify all vocab → vocab_proto_ids
let mut peL1_T_3b = []
let mut td3 = 0.0
while td3 < embed_dim
  let mut tp3 = 0.0
  while tp3 < pcL1
    push(peL1_T_3b, peL1[tp3 * embed_dim + td3])
    tp3 = tp3 + 1.0
  end
  td3 = td3 + 1.0
end

// Also re-encode vocab with frozen mean for classification
let mut vocab_flat = []
let mut evi = 0.0
while evi < num_vocab
  let vw = vocab[evi]
  let enc = word_encode_hash(vw, embed_dim)
  let mut norm_sq = 0.0
  let mut d = 0.0
  while d < embed_dim
    let c = enc[d] - cmL1[d]
    norm_sq = norm_sq + c * c
    d = d + 1.0
  end
  let mut inv_norm = 0.0
  if norm_sq > 0.0000001
    inv_norm = 1.0 / sqrt(norm_sq)
  end
  d = 0.0
  while d < embed_dim
    let c = enc[d] - cmL1[d]
    push(vocab_flat, c * inv_norm)
    d = d + 1.0
  end
  evi = evi + 1.0
end

let sims_3b = gpu_matmul(vocab_flat, peL1_T_3b, num_vocab, pcL1, embed_dim)

let mut vocab_proto_ids = []
let mut qi = 0.0
while qi < num_vocab
  let row_base = qi * pcL1
  let mut best_id = 0.0
  let mut best_sim = sims_3b[row_base]
  let mut p3 = 1.0
  while p3 < pcL1
    let s3 = sims_3b[row_base + p3]
    if s3 > best_sim
      best_sim = s3
      best_id = p3
    end
    p3 = p3 + 1.0
  end
  push(vocab_proto_ids, best_id)
  qi = qi + 1.0
end

// Build per-occurrence proto_ids via vocab lookup
let mut proto_ids = []
let mut lki = 0.0
while lki < total_words
  let w = all_words[lki]
  let vid_lk = map_get(vocab_map, w)
  push(proto_ids, vocab_proto_ids[vid_lk])
  lki = lki + 1.0
end

let t_proto_end = time()
let proto_ms = int((t_proto_end - t_proto_start) * 1000.0)
print("  L1 protos: {pcL1_int} (from {num_vocab_int} unique words)")
print("  Compression: {compression_r}%")
print("  Time: {proto_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 3: Build Generative Model
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 3: Build Generative Model ---")
let t_model_start = time()

// 3a: Reverse vocabulary — proto → word list + frequencies
// Count word frequencies in training set
let mut word_freq = []
let mut wfi = 0.0
while wfi < num_vocab
  push(word_freq, 0.0)
  wfi = wfi + 1.0
end
let mut ti = 0.0
while ti < train_end_idx
  let w = all_words[ti]
  let vid_wf = map_get(vocab_map, w)
  word_freq[vid_wf] = word_freq[vid_wf] + 1.0
  ti = ti + 1.0
end

// Group words by proto — flat parallel arrays with stride indexing
// Build rv_words/rv_freqs/rv_offsets sorted by proto
let mut rv_words = []
let mut rv_freqs = []
let mut rv_offsets_final = []
let mut rp = 0.0
while rp < pcL1
  push(rv_offsets_final, len(rv_words))
  let mut rv_vi = 0.0
  while rv_vi < num_vocab
    if vocab_proto_ids[rv_vi] == rp
      if word_freq[rv_vi] > 0.0
        push(rv_words, vocab[rv_vi])
        push(rv_freqs, word_freq[rv_vi])
      end
    end
    rv_vi = rv_vi + 1.0
  end
  rp = rp + 1.0
end
push(rv_offsets_final, len(rv_words))

let rv_total_int = int(len(rv_words))
print("  Reverse vocab: {rv_total_int} word-proto entries")

// 3b: Similarity matrix — vocab_sim[num_vocab × pcL1]
// Already computed as sims_3b from gpu_matmul
// sims_3b[vi * pcL1 + pi] = similarity of vocab word vi to proto pi
print("  Similarity matrix: {num_vocab_int} x {pcL1_int} (from gpu_matmul)")

// 3c: 1st-order Markov table from training proto sequence
let mut train_proto_seq = []
let mut tpi = 0.0
while tpi < train_end_idx
  push(train_proto_seq, proto_ids[tpi])
  tpi = tpi + 1.0
end
let train_seq_len = len(train_proto_seq)
let table1 = markov1_build(train_proto_seq, train_seq_len, pcL1)

// Row sums for normalization
let mut row_sums = []
let mut rsi = 0.0
while rsi < pcL1
  let mut rsum = 0.0
  let mut rsj = 0.0
  while rsj < pcL1
    rsum = rsum + table1[rsi * pcL1 + rsj]
    rsj = rsj + 1.0
  end
  push(row_sums, rsum)
  rsi = rsi + 1.0
end
print("  Markov table: {pcL1_int} x {pcL1_int}")

// 3d: Bigram set from training corpus (for coherence evaluation)
let mut bigram_set = map()
let mut bi = 1.0
while bi < train_end_idx
  let bg = all_words[bi - 1.0] + " " + all_words[bi]
  map_set(bigram_set, bg, 1.0)
  bi = bi + 1.0
end

// 3e: Chapter seed protos — first 5 proto IDs of chapter 1
let mut seed_protos = []
let mut si = 0.0
while si < 5.0
  push(seed_protos, proto_ids[si])
  si = si + 1.0
end

let t_model_end = time()
let model_ms = int((t_model_end - t_model_start) * 1000.0)
print("  Time: {model_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 4: Text Generation
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 4: Text Generation ---")
let gen_count = 100.0

// ── Helper: sample_word_from_proto ──
// Softmax over rv_freqs with temperature → categorical sample
fn sample_word_from_proto(pid, rv_w, rv_f, rv_off, temp)
  let start = rv_off[pid]
  let end_idx = rv_off[pid + 1.0]
  let count = end_idx - start
  if count < 1.0
    return "the"
  end
  if count < 1.5
    return rv_w[start]
  end
  // Use log-frequency as score (prevents collapse to most frequent word)
  // Find max log-freq for numerical stability
  let mut max_lf = -100.0
  let mut fi = start
  while fi < end_idx
    let lf = log(rv_f[fi] + 1.0)
    if lf > max_lf
      max_lf = lf
    end
    fi = fi + 1.0
  end
  // Softmax over log-frequencies with temperature
  let mut exp_sum = 0.0
  let mut exp_vals = []
  fi = start
  while fi < end_idx
    let lf = log(rv_f[fi] + 1.0)
    let e = exp((lf - max_lf) / temp)
    push(exp_vals, e)
    exp_sum = exp_sum + e
    fi = fi + 1.0
  end
  // Categorical sample
  let r = random() * exp_sum
  let mut cum = 0.0
  let mut ei = 0.0
  while ei < count
    cum = cum + exp_vals[ei]
    if cum >= r
      return rv_w[start + ei]
    end
    ei = ei + 1.0
  end
  return rv_w[end_idx - 1.0]
end

// ── Helper: sample_word_soft ──
// Soft activation over multiple protos → combined word sampling
// activation[pcL1] = probability distribution over protos
fn sample_word_soft(activation, act_len, rv_w, rv_f, rv_off, temp_word)
  // Collect all words from protos with non-negligible activation
  // Weight each word by activation[proto] × log(freq+1) (log-freq prevents collapse)
  let mut cand_words = []
  let mut cand_weights = []

  let mut pi_s = 0.0
  while pi_s < act_len
    let act = activation[pi_s]
    if act > 0.01
      let s_start = rv_off[pi_s]
      let s_end = rv_off[pi_s + 1.0]
      let mut fi = s_start
      while fi < s_end
        let w = act * log(rv_f[fi] + 1.0)
        push(cand_words, rv_w[fi])
        push(cand_weights, w)
        fi = fi + 1.0
      end
    end
    pi_s = pi_s + 1.0
  end

  if len(cand_words) < 1.0
    return "the"
  end

  // Softmax over log-weighted scores with temperature
  let num_cand = len(cand_words)
  let mut max_w = -100.0
  let mut mwi = 0.0
  while mwi < num_cand
    if cand_weights[mwi] > max_w
      max_w = cand_weights[mwi]
    end
    mwi = mwi + 1.0
  end

  let mut temp_sum = 0.0
  let mut temp_vals = []
  let mut twi = 0.0
  while twi < num_cand
    let e = exp((cand_weights[twi] - max_w) / temp_word)
    push(temp_vals, e)
    temp_sum = temp_sum + e
    twi = twi + 1.0
  end

  // Categorical sample
  let r = random() * temp_sum
  let mut cum = 0.0
  let mut ci_s = 0.0
  while ci_s < num_cand
    cum = cum + temp_vals[ci_s]
    if cum >= r
      return cand_words[ci_s]
    end
    ci_s = ci_s + 1.0
  end
  return cand_words[num_cand - 1.0]
end

// ── Strategy A: 1st-Order Markov (Statistical) ──
print("  Strategy A: 1st-order Markov (statistical baseline)")
let t_a_start = time()

let mut gen_a_words = []
let mut curr_proto_a = seed_protos[0]

let mut ga = 0.0
while ga < gen_count
  let next_proto = markov1_sample(table1, curr_proto_a, pcL1, 0.8)
  let word = sample_word_from_proto(next_proto, rv_words, rv_freqs, rv_offsets_final, 1.5)
  push(gen_a_words, word)
  curr_proto_a = next_proto
  ga = ga + 1.0
end

let t_a_end = time()
let a_ms = int((t_a_end - t_a_start) * 1000.0)

// Print Strategy A output
let mut gen_a_text = ""
let mut gai = 0.0
while gai < gen_count
  if gai > 0.0
    gen_a_text = gen_a_text + " "
  end
  gen_a_text = gen_a_text + gen_a_words[gai]
  gai = gai + 1.0
end
print("  Time: {a_ms} ms")
let gen_a_preview = substr(gen_a_text, 0.0, 200.0)
print("  Output: {gen_a_preview}...")
print("")

// ── Strategy B: Neural Generation ──
print("  Strategy B: Neural generation (4 neural properties)")
let t_b_start = time()

let mut gen_b_words = []
let context_max = 20.0
let decay_rate = 0.15
let top_k = 2.0

// Pre-allocate context window (fixed size, track count)
let mut context_window = []
let mut ctx_count = 0.0
let mut sci = 0.0
while sci < context_max
  if sci < 5.0
    push(context_window, seed_protos[sci])
    ctx_count = ctx_count + 1.0
  else
    push(context_window, 0.0)
  end
  sci = sci + 1.0
end

let mut curr_proto_b = seed_protos[4]
let mut context_strength = 1.0

// Pre-allocate reusable arrays to avoid per-step allocation overhead
let mut blended = []
let mut modulated = []
let mut exp_mod = []
let mut activation = []
let mut mod_copy = []
let mut pa_init = 0.0
while pa_init < pcL1
  push(blended, 0.0)
  push(modulated, 0.0)
  push(exp_mod, 0.0)
  push(activation, 0.0)
  push(mod_copy, 0.0)
  pa_init = pa_init + 1.0
end

let mut gb = 0.0
while gb < gen_count
  // 1. CONTEXT PRIOR (cross-level feedback)
  //    Blend Markov rows of context window with exponential decay
  let mut bi_init = 0.0
  while bi_init < pcL1
    blended[bi_init] = 0.0
    bi_init = bi_init + 1.0
  end

  let ctx_len = ctx_count
  let mut weight_sum = 0.0
  let mut ci_b = 0.0
  while ci_b < ctx_len
    let distance = ctx_len - 1.0 - ci_b
    let weight = exp(0.0 - decay_rate * distance)
    let ctx_proto = context_window[ci_b]
    let ctx_base = ctx_proto * pcL1
    let ctx_row_sum = row_sums[ctx_proto]
    if ctx_row_sum > 0.5
      let mut bj = 0.0
      while bj < pcL1
        blended[bj] = blended[bj] + weight * (table1[ctx_base + bj] / ctx_row_sum)
        bj = bj + 1.0
      end
    end
    weight_sum = weight_sum + weight
    ci_b = ci_b + 1.0
  end

  // Normalize blended → context_prior
  if weight_sum > 0.001
    let mut ni = 0.0
    while ni < pcL1
      blended[ni] = blended[ni] / weight_sum
      ni = ni + 1.0
    end
  end

  // 2. TRANSITION DISTRIBUTION (context-sensitive boundaries)
  //    Modulate Markov transition by context prior
  let trans_base = curr_proto_b * pcL1
  let trans_row_sum = row_sums[curr_proto_b]
  let mut mod_sum = 0.0
  let mut mi = 0.0
  while mi < pcL1
    let mut trans_prob = 0.0
    if trans_row_sum > 0.5
      trans_prob = table1[trans_base + mi] / trans_row_sum
    end
    // Modulate: boost transitions the context expects
    let mod_val = trans_prob * (1.0 + context_strength * blended[mi])
    modulated[mi] = mod_val
    mod_sum = mod_sum + mod_val
    mi = mi + 1.0
  end

  // 3. SAMPLE NEXT PROTO from modulated distribution with temperature
  let mut chosen_proto = 0.0
  if mod_sum > 0.0001
    let mut max_mod = 0.0
    let mut mmi = 0.0
    while mmi < pcL1
      if modulated[mmi] > max_mod
        max_mod = modulated[mmi]
      end
      mmi = mmi + 1.0
    end
    let mut exp_mod_sum = 0.0
    mmi = 0.0
    while mmi < pcL1
      let e = exp((modulated[mmi] - max_mod) / 0.4)
      exp_mod[mmi] = e
      exp_mod_sum = exp_mod_sum + e
      mmi = mmi + 1.0
    end
    let r = random() * exp_mod_sum
    let mut cum = 0.0
    mmi = 0.0
    while mmi < pcL1
      cum = cum + exp_mod[mmi]
      if cum >= r
        chosen_proto = mmi
        mmi = pcL1
      end
      mmi = mmi + 1.0
    end
  else
    chosen_proto = markov1_sample(table1, curr_proto_b, pcL1, 0.4)
  end

  // 4. SOFT ACTIVATION (representation ambiguity)
  //    Keep top-K protos weighted by modulated probabilities
  //    Boost the chosen proto (2x) so its words are preferred
  let mut act_sum = 0.0
  let mut ai = 0.0
  while ai < pcL1
    activation[ai] = 0.0
    mod_copy[ai] = modulated[ai]
    ai = ai + 1.0
  end

  // Find top-K from modulated distribution
  let mut topk_count = 0.0
  let mut ki = 0.0
  while ki < top_k
    let mut best_val = -1.0
    let mut best_idx = 0.0
    let mut kj = 0.0
    while kj < pcL1
      if mod_copy[kj] > best_val
        best_val = mod_copy[kj]
        best_idx = kj
      end
      kj = kj + 1.0
    end
    if best_val > 0.0001
      // Boost chosen_proto by 5x for coherence while preserving competition
      let mut boost = best_val
      if best_idx == chosen_proto
        boost = best_val * 5.0
      end
      activation[best_idx] = boost
      act_sum = act_sum + boost
      mod_copy[best_idx] = -1.0
      topk_count = topk_count + 1.0
    end
    ki = ki + 1.0
  end

  // Normalize activation
  if act_sum > 0.0001
    let mut tki = 0.0
    while tki < pcL1
      if activation[tki] > 0.0
        activation[tki] = activation[tki] / act_sum
      end
      tki = tki + 1.0
    end
  else
    activation[chosen_proto] = 1.0
  end

  // 5. SAMPLE WORD from soft activation (competing prototypes)
  let word = sample_word_soft(activation, pcL1, rv_words, rv_freqs, rv_offsets_final, 0.45)
  push(gen_b_words, word)

  // 6. SURPRISE FEEDBACK (cross-level → next step)
  let mut chosen_prob = 0.0
  if mod_sum > 0.0001
    chosen_prob = modulated[chosen_proto] / mod_sum
  end
  let surprise = 1.0 - chosen_prob

  if surprise > 0.7
    // High surprise → explore: weaken context prior
    context_strength = context_strength * 0.7
    if context_strength < 0.3
      context_strength = 0.3
    end
  elif surprise < 0.3
    // Low surprise → exploit: strengthen context prior
    context_strength = context_strength * 1.3
    if context_strength > 3.0
      context_strength = 3.0
    end
  end

  // Update context window (fixed-size, shift in-place when full)
  if ctx_count < context_max
    context_window[ctx_count] = chosen_proto
    ctx_count = ctx_count + 1.0
  else
    // Shift left by 1, append new proto at end
    let mut shift_i = 0.0
    while shift_i < context_max - 1.0
      context_window[shift_i] = context_window[shift_i + 1.0]
      shift_i = shift_i + 1.0
    end
    context_window[context_max - 1.0] = chosen_proto
  end
  curr_proto_b = chosen_proto

  gb = gb + 1.0
end

let t_b_end = time()
let b_ms = int((t_b_end - t_b_start) * 1000.0)

// Print Strategy B output
let mut gen_b_text = ""
let mut gbi = 0.0
while gbi < gen_count
  if gbi > 0.0
    gen_b_text = gen_b_text + " "
  end
  gen_b_text = gen_b_text + gen_b_words[gbi]
  gbi = gbi + 1.0
end
print("  Time: {b_ms} ms")
let gen_b_preview = substr(gen_b_text, 0.0, 200.0)
print("  Output: {gen_b_preview}...")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 5: Evaluation
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 5: Evaluation ---")

// 5a: Perplexity on chapter 12 (test set)
// PPL = exp(-avg(log(P(next|prev)))) from Markov table with Laplace smoothing
let mut log_prob_sum = 0.0
let mut ppl_count = 0.0

let mut tsi = test_start_idx + 1.0
while tsi < test_end_idx
  let prev_p = proto_ids[tsi - 1.0]
  let curr_p = proto_ids[tsi]
  let rs = row_sums[prev_p]
  // Laplace smoothing: P = (count + 1) / (row_sum + pcL1)
  let count_val = table1[prev_p * pcL1 + curr_p]
  let prob = (count_val + 1.0) / (rs + pcL1)
  log_prob_sum = log_prob_sum + log(prob)
  ppl_count = ppl_count + 1.0
  tsi = tsi + 1.0
end

let mut ppl = 999.0
if ppl_count > 0.0
  ppl = exp(0.0 - log_prob_sum / ppl_count)
end
let ppl_r = floor(ppl * 100.0) / 100.0
print("  Perplexity (ch12): {ppl_r}")

// 5b: Vocab coverage — % of ch12 unique words in training vocab
let mut test_unique = map()
let mut test_found = 0.0
let mut test_total_unique = 0.0

let mut tui = test_start_idx
while tui < test_end_idx
  let tw = all_words[tui]
  if map_has(test_unique, tw) == 0.0
    map_set(test_unique, tw, 1.0)
    test_total_unique = test_total_unique + 1.0
    // Check if word exists in training vocab (word_freq > 0)
    if map_has(vocab_map, tw) > 0.0
      let vid_t = map_get(vocab_map, tw)
      if word_freq[vid_t] > 0.0
        test_found = test_found + 1.0
      end
    end
  end
  tui = tui + 1.0
end

let mut vocab_coverage = 0.0
if test_total_unique > 0.0
  vocab_coverage = (test_found / test_total_unique) * 100.0
end
let coverage_r = floor(vocab_coverage * 10.0) / 10.0
let test_total_int = int(test_total_unique)
let test_found_int = int(test_found)
print("  Vocab coverage: {coverage_r}% ({test_found_int}/{test_total_int} unique ch12 words)")

// 5c: Bigram coherence — % of generated bigrams matching training bigrams
// Strategy A
let mut a_bigram_match = 0.0
let mut a_bigram_total = 0.0
let mut abi = 1.0
while abi < gen_count
  let bg_a = gen_a_words[abi - 1.0] + " " + gen_a_words[abi]
  a_bigram_total = a_bigram_total + 1.0
  if map_has(bigram_set, bg_a) > 0.0
    a_bigram_match = a_bigram_match + 1.0
  end
  abi = abi + 1.0
end
let mut a_coherence = 0.0
if a_bigram_total > 0.0
  a_coherence = (a_bigram_match / a_bigram_total) * 100.0
end
let a_coh_r = floor(a_coherence * 10.0) / 10.0

// Strategy B
let mut b_bigram_match = 0.0
let mut b_bigram_total = 0.0
let mut bbi = 1.0
while bbi < gen_count
  let bg_b = gen_b_words[bbi - 1.0] + " " + gen_b_words[bbi]
  b_bigram_total = b_bigram_total + 1.0
  if map_has(bigram_set, bg_b) > 0.0
    b_bigram_match = b_bigram_match + 1.0
  end
  bbi = bbi + 1.0
end
let mut b_coherence = 0.0
if b_bigram_total > 0.0
  b_coherence = (b_bigram_match / b_bigram_total) * 100.0
end
let b_coh_r = floor(b_coherence * 10.0) / 10.0

let a_match_int = int(a_bigram_match)
let b_match_int = int(b_bigram_match)
let bigram_total_int = int(a_bigram_total)
print("  Bigram coherence A: {a_coh_r}% ({a_match_int}/{bigram_total_int})")
print("  Bigram coherence B: {b_coh_r}% ({b_match_int}/{bigram_total_int})")

// 5d: Diversity — unique words per 100 generated
let mut a_unique_map = map()
let mut a_unique_count = 0.0
let mut dui = 0.0
while dui < gen_count
  let dw = gen_a_words[dui]
  if map_has(a_unique_map, dw) == 0.0
    map_set(a_unique_map, dw, 1.0)
    a_unique_count = a_unique_count + 1.0
  end
  dui = dui + 1.0
end

let mut b_unique_map = map()
let mut b_unique_count = 0.0
dui = 0.0
while dui < gen_count
  let dw = gen_b_words[dui]
  if map_has(b_unique_map, dw) == 0.0
    map_set(b_unique_map, dw, 1.0)
    b_unique_count = b_unique_count + 1.0
  end
  dui = dui + 1.0
end

let a_div_int = int(a_unique_count)
let b_div_int = int(b_unique_count)
print("  Diversity A: {a_div_int} unique / 100")
print("  Diversity B: {b_div_int} unique / 100")

// Neural vs Statistical comparison
let b_wins = b_coherence > a_coherence
if b_wins > 0.0
  print("  Neural vs Statistical: B wins ({b_coh_r}% > {a_coh_r}%)")
else
  print("  Neural vs Statistical: A wins ({a_coh_r}% >= {b_coh_r}%)")
end
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 6: PASS/FAIL
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL ===")
let mut npass = 0.0
let mut nfail = 0.0

// Test 1: Perplexity < 50
if ppl < 50.0
  print("PASS [1/7]: Perplexity {ppl_r} < 50")
  npass = npass + 1.0
else
  print("FAIL [1/7]: Perplexity {ppl_r} >= 50")
  nfail = nfail + 1.0
end

// Test 2: Vocab coverage > 80%
if vocab_coverage > 80.0
  print("PASS [2/7]: Vocab coverage {coverage_r}% > 80%")
  npass = npass + 1.0
else
  print("FAIL [2/7]: Vocab coverage {coverage_r}% <= 80%")
  nfail = nfail + 1.0
end

// Test 3: Strategy A bigram coherence > 5%
if a_coherence > 5.0
  print("PASS [3/7]: Strategy A coherence {a_coh_r}% > 5%")
  npass = npass + 1.0
else
  print("FAIL [3/7]: Strategy A coherence {a_coh_r}% <= 5%")
  nfail = nfail + 1.0
end

// Test 4: Strategy B > Strategy A
if b_coherence > a_coherence
  print("PASS [4/7]: Strategy B ({b_coh_r}%) > Strategy A ({a_coh_r}%)")
  npass = npass + 1.0
else
  print("FAIL [4/7]: Strategy B ({b_coh_r}%) <= Strategy A ({a_coh_r}%)")
  nfail = nfail + 1.0
end

// Test 5: Strategy B bigram coherence > 10%
if b_coherence > 10.0
  print("PASS [5/7]: Strategy B coherence {b_coh_r}% > 10%")
  npass = npass + 1.0
else
  print("FAIL [5/7]: Strategy B coherence {b_coh_r}% <= 10%")
  nfail = nfail + 1.0
end

// Test 6: Vocabulary diversity > 30 unique per 100 (both)
if a_unique_count > 30.0 && b_unique_count > 30.0
  print("PASS [6/7]: Diversity A={a_div_int}, B={b_div_int} (both > 30)")
  npass = npass + 1.0
else
  print("FAIL [6/7]: Diversity A={a_div_int}, B={b_div_int} (need both > 30)")
  nfail = nfail + 1.0
end

// Test 7: Pipeline completed
print("PASS [7/7]: Pipeline completed")
npass = npass + 1.0

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/7 passed, {nfail_int}/7 failed")
if nfail < 0.5
  print("OVERALL: PASS ({npass_int}/7)")
else
  print("OVERALL: FAIL ({npass_int}/7)")
end

print("")
print("--- Phase 17 complete: Generative NLP ---")
