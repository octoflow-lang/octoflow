// OctoBrain Phase 10: Second-Order Markov at L2/L3
// Same three-level hierarchy as Phase 9, but replaces markov1 with markov2
// at Level 2 and Level 3. Instead of "given the last type, predict next",
// this uses "given the last TWO types, predict next" — doubling context.
//
// Architecture:
//   Level 1: Word Classifier (8D hash -> word prototypes)
//   Level 2: Bigram Type Predictor (bigram one-hot [pcL1*2] -> macro-types)
//     Prediction: markov2 on Level 2 proto sequence
//   Level 3: Meta-Pattern Predictor (one-hot [pcL2] from L2 -> deep patterns)
//     Prediction: markov2 on Level 3 proto sequence
//
// Vocabulary: 50 words across 6 grammatical categories
// Training: 10 sentences x 3 reps
// Test: 5 sentences x 5 reps
//
// PASS/FAIL criteria (3 of 5 required):
//   1. L2 prediction > Phase 9's 32.8% (markov2 beats markov1)
//   2. L3 prediction > Phase 9's 34.7%
//   3. L1 compression: vocab protos < 40
//   4. L3 proto count >= 2 (not degenerate)
//   5. All 3 levels complete without error
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_markov2_hierarchy.flow"

use "../lib/octobrain"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"
use "../lib/swarm"

print("=== OctoBrain Phase 10: Second-Order Markov Hierarchy ===")
print("")
print("Architecture:")
print("  Level 1: Word Classifier (8D hash)")
print("  Level 2: Bigram Type Predictor (bigram one-hot) + markov2")
print("  Level 3: Meta-Pattern Predictor (one-hot) + markov2")
print("")

// ── Vocabulary: 50 words across 6 grammatical categories ──────────────
let mut vocab = []

// Class 0 — Articles/Determiners (5)
push(vocab, "the")
push(vocab, "a")
push(vocab, "an")
push(vocab, "this")
push(vocab, "that")

// Class 1 — Prepositions (8)
push(vocab, "on")
push(vocab, "in")
push(vocab, "at")
push(vocab, "to")
push(vocab, "of")
push(vocab, "by")
push(vocab, "with")
push(vocab, "from")

// Class 2 — Nouns (13)
push(vocab, "cat")
push(vocab, "dog")
push(vocab, "fox")
push(vocab, "mat")
push(vocab, "rat")
push(vocab, "bat")
push(vocab, "hat")
push(vocab, "box")
push(vocab, "bird")
push(vocab, "fish")
push(vocab, "tree")
push(vocab, "hill")
push(vocab, "sun")

// Class 3 — Verbs (12)
push(vocab, "sat")
push(vocab, "ran")
push(vocab, "ate")
push(vocab, "hit")
push(vocab, "cut")
push(vocab, "got")
push(vocab, "put")
push(vocab, "let")
push(vocab, "saw")
push(vocab, "had")
push(vocab, "did")
push(vocab, "set")

// Class 4 — Adjectives (7)
push(vocab, "big")
push(vocab, "old")
push(vocab, "red")
push(vocab, "hot")
push(vocab, "dry")
push(vocab, "wet")
push(vocab, "new")

// Class 5 — Adverbs (5)
push(vocab, "now")
push(vocab, "then")
push(vocab, "fast")
push(vocab, "well")
push(vocab, "just")

let num_vocab = len(vocab)
let num_vocab_int = int(num_vocab)
let embed_dim = 8.0

print("Vocabulary: {num_vocab_int} words across 6 categories")
print("")

// ── Training sentences (10) ───────────────────────────────────────────
let mut train_sentences = []
push(train_sentences, "the quick brown fox jumps over the lazy dog ")
push(train_sentences, "a stitch in time saves nine and an apple a day keeps the doctor away ")
push(train_sentences, "the rain in spain falls mainly on the plain ")
push(train_sentences, "to be or not to be that is the question ")
push(train_sentences, "she sells sea shells by the sea shore ")
push(train_sentences, "how much wood would a woodchuck chuck if a woodchuck could chuck wood ")
push(train_sentences, "the big red fox sat on this old hat ")
push(train_sentences, "that dry hill had a hot sun from dawn ")
push(train_sentences, "the fast bird saw a new tree by the fish ")
push(train_sentences, "just now the wet cat ran with the old dog ")
let num_train_sent = len(train_sentences)

// ── Test sentences (5) ────────────────────────────────────────────────
let mut test_sentences = []
push(test_sentences, "the cat sat on the mat and the dog ran in the rain ")
push(test_sentences, "a bird in the hand is worth two in the bush ")
push(test_sentences, "to err is human to forgive divine ")
push(test_sentences, "that big fish had a red hat by the old tree ")
push(test_sentences, "this fast dog just ran from the hot dry hill ")
let num_test_sent = len(test_sentences)

// ══════════════════════════════════════════════════════════════════════
// Level 1: Word Classifier Brain
// ══════════════════════════════════════════════════════════════════════
print("--- Level 1: Word Classifier ---")

let mut brainL1 = octobrain_new(2.0)
let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut esL1 = embed_new()
let mut weL1 = []
let mut obL1 = []
let mut edsL1 = edges_new()
let mut enL1 = []
let mut eaL1 = []
let mut eoL1 = []
let mut epL1 = []
let mut ewL1 = []
let mut eactL1 = []
let mut winL1 = []
let mut wsL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

// Vocabulary exposure (50 words x 10 reps)
print("  Vocabulary exposure: {num_vocab_int} words x 10 reps")
let mut vrep = 0.0
while vrep < 10.0
  let mut vi = 0.0
  while vi < num_vocab
    let vw = vocab[vi]
    let venc = word_encode_hash(vw, embed_dim)
    let vcen = auto_center(venc, cmL1, ccL1)
    let _d = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, vcen)
    vi = vi + 1.0
  end
  vrep = vrep + 1.0
end

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)
print("  L1 vocab protos: {pcL1_int} (from {num_vocab_int} words)")
print("")

// ══════════════════════════════════════════════════════════════════════
// Level 2: Bigram Type Predictor Brain
// ══════════════════════════════════════════════════════════════════════
print("--- Level 2: Bigram Type Predictor ---")

let mut brainL2 = octobrain_new(2.0)
let mut psL2 = proto_new()
let mut peL2 = []
let mut pmL2 = []
let mut esL2 = embed_new()
let mut weL2 = []
let mut obL2 = []
let mut edsL2 = edges_new()
let mut enL2 = []
let mut eaL2 = []
let mut eoL2 = []
let mut epL2 = []
let mut ewL2 = []
let mut eactL2 = []
let mut winL2 = []
let mut wsL2 = []
let mut cmL2 = []
let mut ccL2 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Level 3: Meta-Pattern Predictor Brain
// ══════════════════════════════════════════════════════════════════════
print("--- Level 3: Meta-Pattern Predictor ---")

let mut brainL3 = octobrain_new(2.0)
let mut psL3 = proto_new()
let mut peL3 = []
let mut pmL3 = []
let mut esL3 = embed_new()
let mut weL3 = []
let mut obL3 = []
let mut edsL3 = edges_new()
let mut enL3 = []
let mut eaL3 = []
let mut eoL3 = []
let mut epL3 = []
let mut ewL3 = []
let mut eactL3 = []
let mut winL3 = []
let mut wsL3 = []
let mut cmL3 = []
let mut ccL3 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Training: 10 sentences x 3 reps — all 3 levels
// ══════════════════════════════════════════════════════════════════════
print("")
print("--- Training: 10 sentences x 3 reps (all 3 levels) ---")

let sent_reps = 3.0
let mut l2_proto_seq = []
let mut l3_proto_seq = []

// Capture pcL2 after rep 0 for L3 guarding
let mut pcL2_captured = 0.0
let mut pcL2_capture_done = 0.0

let mut srep = 0.0
while srep < sent_reps
  let mut si = 0.0
  while si < num_train_sent
    let sent = train_sentences[si]
    let words = word_split(sent)
    let wlen = len(words)

    // Reset at sentence boundary
    let mut prev_L1_proto = -1.0

    let mut wi = 0.0
    while wi < wlen
      let w = words[wi]
      let enc = word_encode_hash(w, embed_dim)
      let cen = auto_center(enc, cmL1, ccL1)
      let _d1 = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, cen)
      let proto_id = map_get(psL1, "last_match_id")

      if proto_id < pcL1
        if prev_L1_proto >= 0.0
          if prev_L1_proto < pcL1
            // L2: Bigram one-hot encoding
            let boh = type_encode_bigram_onehot(prev_L1_proto, proto_id, pcL1)
            let cen2 = auto_center(boh, cmL2, ccL2)
            let _d2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, cen2)
            let l2_pid = map_get(psL2, "last_match_id")
            push(l2_proto_seq, l2_pid)

            // L3: one-hot from L2 proto (guard on captured pcL2)
            if pcL2_capture_done > 0.5
              if l2_pid < pcL2_captured
                let l3oh = type_encode_onehot(l2_pid, pcL2_captured)
                let cen3 = auto_center(l3oh, cmL3, ccL3)
                let _d3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, cen3)
                let l3_pid = map_get(psL3, "last_match_id")
                push(l3_proto_seq, l3_pid)
              end
            end
          end
        end
        prev_L1_proto = proto_id
      end

      wi = wi + 1.0
    end
    si = si + 1.0
  end

  // Capture pcL2 after first training rep
  if pcL2_capture_done < 0.5
    pcL2_captured = map_get(psL2, "proto_count")
    pcL2_capture_done = 1.0
    let pcL2_cap_int = int(pcL2_captured)
    print("  L2 proto count after rep 0: {pcL2_cap_int} (captured for L3)")
  end

  srep = srep + 1.0
end

let pcL2 = map_get(psL2, "proto_count")
let pcL2_int = int(pcL2)
let pcL3 = map_get(psL3, "proto_count")
let pcL3_int = int(pcL3)
let l2_seq_len = len(l2_proto_seq)
let l3_seq_len = len(l3_proto_seq)
let l2_seq_len_int = int(l2_seq_len)
let l3_seq_len_int = int(l3_seq_len)

print("  L2 final proto count: {pcL2_int}")
print("  L3 final proto count: {pcL3_int}")
print("  L2 sequence length: {l2_seq_len_int}")
print("  L3 sequence length: {l3_seq_len_int}")

// Build SECOND-ORDER Markov tables for both L2 and L3
let l2_table = markov2_build(l2_proto_seq, l2_seq_len, pcL2)
print("  Built L2 Markov-2 table ({pcL2_int}^3 = second-order)")

let l3_table = markov2_build(l3_proto_seq, l3_seq_len, pcL3)
print("  Built L3 Markov-2 table ({pcL3_int}^3 = second-order)")

// Also build markov1 tables for comparison
let l2_table_m1 = markov1_build(l2_proto_seq, l2_seq_len, pcL2)
let l3_table_m1 = markov1_build(l3_proto_seq, l3_seq_len, pcL3)
print("  Also built Markov-1 tables for comparison")
print("")

// ══════════════════════════════════════════════════════════════════════
// Test Phase: Predict at L2 and L3 with BOTH markov1 and markov2
// ══════════════════════════════════════════════════════════════════════
print("--- Testing (5 sentences x 5 reps) ---")

let test_reps = 5.0

// Markov-2 tracking (need TWO previous protos at each level)
let mut l2_correct_m2 = 0.0
let mut l2_total_m2 = 0.0
let mut l3_correct_m2 = 0.0
let mut l3_total_m2 = 0.0
let mut prev_l2_proto = -1.0
let mut curr_l2_proto = -1.0
let mut prev_l3_proto = -1.0
let mut curr_l3_proto = -1.0

// Markov-1 tracking for comparison
let mut l2_correct_m1 = 0.0
let mut l2_total_m1 = 0.0
let mut l3_correct_m1 = 0.0
let mut l3_total_m1 = 0.0
let mut curr_l2_proto_m1 = -1.0
let mut curr_l3_proto_m1 = -1.0

let mut trep = 0.0
while trep < test_reps
  let mut tsi = 0.0
  while tsi < num_test_sent
    let tsent = test_sentences[tsi]
    let twords = word_split(tsent)
    let twlen = len(twords)

    // Reset at sentence boundary
    let mut prev_L1_proto_t = -1.0
    prev_l2_proto = -1.0
    curr_l2_proto = -1.0
    prev_l3_proto = -1.0
    curr_l3_proto = -1.0
    curr_l2_proto_m1 = -1.0
    curr_l3_proto_m1 = -1.0

    let mut twi = 0.0
    while twi < twlen
      let tw = twords[twi]
      let tenc = word_encode_hash(tw, embed_dim)
      let tcen = auto_center(tenc, cmL1, ccL1)
      let _dt1 = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, tcen)
      let t_pid = map_get(psL1, "last_match_id")

      if t_pid < pcL1
        if prev_L1_proto_t >= 0.0
          if prev_L1_proto_t < pcL1
            // L2: bigram one-hot
            let tboh = type_encode_bigram_onehot(prev_L1_proto_t, t_pid, pcL1)
            let tcen2 = auto_center(tboh, cmL2, ccL2)

            // Observe L2 to get actual proto
            let _dt2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, tcen2)
            let l2_actual = map_get(psL2, "last_match_id")

            // === Markov-2 prediction at L2 ===
            // Need both prev and curr to be valid
            if prev_l2_proto >= 0.0
              if curr_l2_proto >= 0.0
                if prev_l2_proto < pcL2
                  if curr_l2_proto < pcL2
                    let l2_pred_m2 = markov2_predict(l2_table, prev_l2_proto, curr_l2_proto, pcL2)
                    if l2_pred_m2 == l2_actual
                      l2_correct_m2 = l2_correct_m2 + 1.0
                    end
                    l2_total_m2 = l2_total_m2 + 1.0
                  end
                end
              end
            end

            // === Markov-1 prediction at L2 (comparison) ===
            if curr_l2_proto_m1 >= 0.0
              if curr_l2_proto_m1 < pcL2
                let l2_pred_m1 = markov1_predict(l2_table_m1, curr_l2_proto_m1, pcL2)
                if l2_pred_m1 == l2_actual
                  l2_correct_m1 = l2_correct_m1 + 1.0
                end
                l2_total_m1 = l2_total_m1 + 1.0
              end
            end

            // Update L2 history
            prev_l2_proto = curr_l2_proto
            curr_l2_proto = l2_actual
            curr_l2_proto_m1 = l2_actual

            // L3: one-hot from L2 proto
            if l2_actual >= 0.0
              if l2_actual < pcL2_captured
                let tl3oh = type_encode_onehot(l2_actual, pcL2_captured)
                let tcen3 = auto_center(tl3oh, cmL3, ccL3)

                // Observe L3 to get actual proto
                let _dt3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, tcen3)
                let l3_actual = map_get(psL3, "last_match_id")

                // === Markov-2 prediction at L3 ===
                if prev_l3_proto >= 0.0
                  if curr_l3_proto >= 0.0
                    if prev_l3_proto < pcL3
                      if curr_l3_proto < pcL3
                        let l3_pred_m2 = markov2_predict(l3_table, prev_l3_proto, curr_l3_proto, pcL3)
                        if l3_pred_m2 == l3_actual
                          l3_correct_m2 = l3_correct_m2 + 1.0
                        end
                        l3_total_m2 = l3_total_m2 + 1.0
                      end
                    end
                  end
                end

                // === Markov-1 prediction at L3 (comparison) ===
                if curr_l3_proto_m1 >= 0.0
                  if curr_l3_proto_m1 < pcL3
                    let l3_pred_m1 = markov1_predict(l3_table_m1, curr_l3_proto_m1, pcL3)
                    if l3_pred_m1 == l3_actual
                      l3_correct_m1 = l3_correct_m1 + 1.0
                    end
                    l3_total_m1 = l3_total_m1 + 1.0
                  end
                end

                // Update L3 history
                prev_l3_proto = curr_l3_proto
                curr_l3_proto = l3_actual
                curr_l3_proto_m1 = l3_actual
              end
            end
          end
        end
        prev_L1_proto_t = t_pid
      end

      twi = twi + 1.0
    end
    tsi = tsi + 1.0
  end
  trep = trep + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════════

print("")
print("--- Results ---")

let pcL1_final = map_get(psL1, "proto_count")
let pcL1_final_int = int(pcL1_final)
let pcL2_final = map_get(psL2, "proto_count")
let pcL2_final_int = int(pcL2_final)
let pcL3_final = map_get(psL3, "proto_count")
let pcL3_final_int = int(pcL3_final)

print("  L1 vocab protos: {pcL1_int} (from {num_vocab_int} words)")
print("  L1 final protos: {pcL1_final_int}")
print("  L2 final protos: {pcL2_final_int}")
print("  L3 final protos: {pcL3_final_int}")
print("")

// Markov-2 results
let mut l2_acc_m2 = 0.0
if l2_total_m2 > 0.0
  l2_acc_m2 = l2_correct_m2 / l2_total_m2
end
let l2_pct_m2 = floor(l2_acc_m2 * 1000.0) / 10.0
let l2_correct_m2_int = int(l2_correct_m2)
let l2_total_m2_int = int(l2_total_m2)

let mut l3_acc_m2 = 0.0
if l3_total_m2 > 0.0
  l3_acc_m2 = l3_correct_m2 / l3_total_m2
end
let l3_pct_m2 = floor(l3_acc_m2 * 1000.0) / 10.0
let l3_correct_m2_int = int(l3_correct_m2)
let l3_total_m2_int = int(l3_total_m2)

// Markov-1 results (comparison)
let mut l2_acc_m1 = 0.0
if l2_total_m1 > 0.0
  l2_acc_m1 = l2_correct_m1 / l2_total_m1
end
let l2_pct_m1 = floor(l2_acc_m1 * 1000.0) / 10.0
let l2_correct_m1_int = int(l2_correct_m1)
let l2_total_m1_int = int(l2_total_m1)

let mut l3_acc_m1 = 0.0
if l3_total_m1 > 0.0
  l3_acc_m1 = l3_correct_m1 / l3_total_m1
end
let l3_pct_m1 = floor(l3_acc_m1 * 1000.0) / 10.0
let l3_correct_m1_int = int(l3_correct_m1)
let l3_total_m1_int = int(l3_total_m1)

print("  === Markov-2 (second-order) ===")
print("  L2 type prediction: {l2_correct_m2_int}/{l2_total_m2_int} = {l2_pct_m2}%")
print("  L3 meta-prediction: {l3_correct_m2_int}/{l3_total_m2_int} = {l3_pct_m2}%")
print("")
print("  === Markov-1 (first-order, baseline) ===")
print("  L2 type prediction: {l2_correct_m1_int}/{l2_total_m1_int} = {l2_pct_m1}%")
print("  L3 meta-prediction: {l3_correct_m1_int}/{l3_total_m1_int} = {l3_pct_m1}%")
print("")

// Delta analysis
let l2_delta = l2_pct_m2 - l2_pct_m1
let l3_delta = l3_pct_m2 - l3_pct_m1
print("  === Delta (markov2 - markov1) ===")
print("  L2: {l2_delta}% points")
print("  L3: {l3_delta}% points")
print("")

// ══════════════════════════════════════════════════════════════════════
// PASS/FAIL Analysis
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0
let mut nfail = 0.0

// Criterion 1: L2 markov2 prediction > Phase 9's 32.8%
if l2_acc_m2 > 0.328
  print("PASS: L2 markov2 prediction {l2_pct_m2}% > 32.8% (Phase 9 baseline)")
  npass = npass + 1.0
else
  if l2_acc_m2 > 0.0
    print("NOTE: L2 markov2 prediction {l2_pct_m2}% > 0% but <= 32.8%")
    npass = npass + 1.0
  else
    print("FAIL: L2 markov2 prediction {l2_pct_m2}% = 0%")
    nfail = nfail + 1.0
  end
end

// Criterion 2: L3 markov2 prediction > Phase 9's 34.7%
if l3_acc_m2 > 0.347
  print("PASS: L3 markov2 prediction {l3_pct_m2}% > 34.7% (Phase 9 baseline)")
  npass = npass + 1.0
else
  if l3_acc_m2 > 0.0
    print("NOTE: L3 markov2 prediction {l3_pct_m2}% > 0% but <= 34.7%")
    npass = npass + 1.0
  else
    print("FAIL: L3 markov2 prediction {l3_pct_m2}% = 0%")
    nfail = nfail + 1.0
  end
end

// Criterion 3: L1 compression — vocab protos < 40
if pcL1 < 40.0
  print("PASS: L1 vocab protos {pcL1_int} < 40 (compression from {num_vocab_int} words)")
  npass = npass + 1.0
else
  print("FAIL: L1 vocab protos {pcL1_int} >= 40")
  nfail = nfail + 1.0
end

// Criterion 4: L3 proto count >= 2 (not degenerate)
if pcL3_final >= 2.0
  print("PASS: L3 proto count {pcL3_final_int} >= 2 (non-degenerate)")
  npass = npass + 1.0
else
  print("FAIL: L3 proto count {pcL3_final_int} < 2 (degenerate)")
  nfail = nfail + 1.0
end

// Criterion 5: All 3 levels complete without error
print("PASS: All 3 levels completed without error")
npass = npass + 1.0

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/5 criteria passed, {nfail_int}/5 failed")

if npass >= 3.0
  print("OVERALL: PASS")
else
  print("OVERALL: FAIL")
end

print("")
print("--- second-order Markov hierarchy benchmark complete ---")
