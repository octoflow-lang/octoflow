// OctoBrain Phase 19: Dot-Product Attention over Context Window
// Architecture:
//   L1 prototype matching at dim=16 (threshold=0.6)
//   GPU batch classification via gpu_matmul
//   Reverse vocabulary + similarity matrix for soft proto activation
//   Strategy B: Context-blended neural generation (from Phase 17)
//   Epoch learning: drift high-surprise protos, reclassify, rebuild (from Phase 18)
//   NEW: Dot-product attention over context window — content-aware weighting
//        replaces fixed positional decay with hybrid Q·K attention + recency
//
// Attention mechanism:
//   Query = proto embedding of current state (peL1[curr_proto_b])
//   Key[i] = proto embedding of context position i
//   weight[i] = 0.5 * softmax(Q · K[i] / temp) + 0.5 * positional_decay[i]
//   No learned projections — Q and K are raw proto embeddings.
//   Epoch drift provides indirect learning of attention patterns.
//
// PASS/FAIL criteria:
//   1. Final perplexity (ch12) < 50
//   2. Vocab coverage > 80%
//   3. Final coherence > 10%
//   4. Final diversity > 30 unique per 200
//   5. Coherence improvement: epoch 3 >= epoch 1
//   6. Attention selectivity: final attn entropy < positional entropy
//   7. Attention consistency: ALL epoch attn entropies < positional entropy
//   8. Pipeline completes without error
//
// Run: octoflow run "OctoBrain/examples/bench_alice_attention.flow" --allow-ffi --allow-read

use "../lib/text_word"
use "../lib/proto"
use "../lib/sequence"

print("=== OctoBrain Phase 19: Dot-Product Attention over Context Window ===")
print("    Hybrid Content + Positional Weighting")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 1: Load Corpus + Train/Test Split
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 1: Load Corpus + Train/Test Split ---")
let t_load_start = time()

let corpus_path = "OctoBrain/data/alice.txt"
let lines = read_lines(corpus_path)
let total_lines = len(lines)

let mut all_words = []
let mut word_chapters = []
let mut chapter_word_starts = []
let mut vocab_map = map()
let mut vocab = []
let mut next_vocab_id = 0.0
let mut current_chapter = 0.0
let mut num_chapters = 0.0

let mut li = 0.0
while li < total_lines
  let line = lines[li]
  if starts_with(line, "CHAPTER")
    current_chapter = current_chapter + 1.0
    num_chapters = num_chapters + 1.0
    push(chapter_word_starts, len(all_words))
  elif len(line) > 0.0
    if current_chapter > 0.0
      let words = word_clean_split_fast(line)
      let wlen = len(words)
      let mut wi = 0.0
      while wi < wlen
        let w = words[wi]
        if len(w) > 0.0
          push(all_words, w)
          push(word_chapters, current_chapter)
          if map_has(vocab_map, w) == 0.0
            map_set(vocab_map, w, next_vocab_id)
            push(vocab, w)
            next_vocab_id = next_vocab_id + 1.0
          end
        end
        wi = wi + 1.0
      end
    end
  end
  li = li + 1.0
end

push(chapter_word_starts, len(all_words))

let total_words = len(all_words)
let num_vocab = len(vocab)
let embed_dim = 16.0

let train_end_idx = chapter_word_starts[11]
let test_start_idx = chapter_word_starts[11]
let test_end_idx = chapter_word_starts[12]
let train_word_count = train_end_idx
let test_word_count = test_end_idx - test_start_idx

let t_load_end = time()
let load_ms = int((t_load_end - t_load_start) * 1000.0)
let total_words_int = int(total_words)
let num_vocab_int = int(num_vocab)
let train_int = int(train_word_count)
let test_int = int(test_word_count)
print("  Corpus: {total_words_int} words, {num_vocab_int} unique")
print("  Train: chapters 1-11 ({train_int} words)")
print("  Test:  chapter 12 ({test_int} words)")
print("  Time: {load_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 2: L1 Prototype Formation
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 2: L1 Prototype Formation ---")
let t_proto_start = time()

let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

let threshold_r1 = compute_threshold(embed_dim)

let mut normed_buf = []
let mut di = 0.0
while di < embed_dim
  push(normed_buf, 0.0)
  di = di + 1.0
end

let mut vi = 0.0
while vi < num_vocab
  let vw = vocab[vi]
  let venc = word_encode_hash(vw, embed_dim)

  let cc = ccL1[0]
  if cc < 0.5
    let mut d = 0.0
    while d < embed_dim
      push(cmL1, venc[d])
      d = d + 1.0
    end
    ccL1[0] = 1.0
  else
    let mut d = 0.0
    while d < embed_dim
      cmL1[d] = 0.99 * cmL1[d] + 0.01 * venc[d]
      d = d + 1.0
    end
    ccL1[0] = cc + 1.0

    let mut norm_sq = 0.0
    d = 0.0
    while d < embed_dim
      let c = venc[d] - cmL1[d]
      norm_sq = norm_sq + c * c
      d = d + 1.0
    end

    if norm_sq > 0.000001
      let inv_norm = 1.0 / sqrt(norm_sq)
      d = 0.0
      while d < embed_dim
        normed_buf[d] = (venc[d] - cmL1[d]) * inv_norm
        d = d + 1.0
      end

      let pcL1_now = map_get(psL1, "proto_count")
      let mut best_id = -1.0
      let mut best_sim = -2.0
      let mut p = 0.0
      while p < pcL1_now
        let base = p * embed_dim
        let mut dot = 0.0
        d = 0.0
        while d < embed_dim
          dot = dot + peL1[base + d] * normed_buf[d]
          d = d + 1.0
        end
        if dot > best_sim
          best_sim = dot
          best_id = p
        end
        p = p + 1.0
      end

      if pcL1_now == 0.0 || best_sim < threshold_r1
        d = 0.0
        while d < embed_dim
          push(peL1, normed_buf[d])
          d = d + 1.0
        end
        push(pmL1, 1.0)
        map_set(psL1, "proto_count", pcL1_now + 1.0)
      else
        let base = best_id * embed_dim
        let mut dnorm_sq = 0.0
        d = 0.0
        while d < embed_dim
          let dv = 0.9 * peL1[base + d] + 0.1 * normed_buf[d]
          dnorm_sq = dnorm_sq + dv * dv
          d = d + 1.0
        end
        let dinv = 1.0 / sqrt(dnorm_sq)
        d = 0.0
        while d < embed_dim
          peL1[base + d] = (0.9 * peL1[base + d] + 0.1 * normed_buf[d]) * dinv
          d = d + 1.0
        end
        pmL1[best_id] = pmL1[best_id] + 1.0
      end
    end
  end

  vi = vi + 1.0
end

// Batch-encode all vocab for reps 2+3
let mut vocab_batch_flat = []
vi = 0.0
while vi < num_vocab
  let vw = vocab[vi]
  let venc = word_encode_hash(vw, embed_dim)
  let mut norm_sq = 0.0
  let mut d = 0.0
  while d < embed_dim
    let c = venc[d] - cmL1[d]
    norm_sq = norm_sq + c * c
    d = d + 1.0
  end
  let mut inv_norm = 0.0
  if norm_sq > 0.0000001
    inv_norm = 1.0 / sqrt(norm_sq)
  end
  d = 0.0
  while d < embed_dim
    let c = venc[d] - cmL1[d]
    push(vocab_batch_flat, c * inv_norm)
    d = d + 1.0
  end
  vi = vi + 1.0
end

// Reps 2+3: gpu_matmul batch scoring
let batch_threshold = compute_threshold(embed_dim)
let mut brep = 0.0
while brep < 2.0
  let pcL1_now = map_get(psL1, "proto_count")
  let mut peL1_T = []
  let mut td = 0.0
  while td < embed_dim
    let mut tp = 0.0
    while tp < pcL1_now
      push(peL1_T, peL1[tp * embed_dim + td])
      tp = tp + 1.0
    end
    td = td + 1.0
  end
  let sims = gpu_matmul(vocab_batch_flat, peL1_T, num_vocab, pcL1_now, embed_dim)
  let mut bvi = 0.0
  while bvi < num_vocab
    let row_base = bvi * pcL1_now
    let mut best_id = 0.0
    let mut best_sim = sims[row_base]
    let mut p = 1.0
    while p < pcL1_now
      let s = sims[row_base + p]
      if s > best_sim
        best_sim = s
        best_id = p
      end
      p = p + 1.0
    end
    if best_sim >= batch_threshold
      let base = best_id * embed_dim
      let vbase = bvi * embed_dim
      let mut dnorm = 0.0
      let mut d = 0.0
      while d < embed_dim
        let dv = 0.9 * peL1[base + d] + 0.1 * vocab_batch_flat[vbase + d]
        dnorm = dnorm + dv * dv
        d = d + 1.0
      end
      let dinv = 1.0 / sqrt(dnorm)
      d = 0.0
      while d < embed_dim
        peL1[base + d] = (0.9 * peL1[base + d] + 0.1 * vocab_batch_flat[vbase + d]) * dinv
        d = d + 1.0
      end
      pmL1[best_id] = pmL1[best_id] + 1.0
    else
      let vbase = bvi * embed_dim
      let mut d = 0.0
      while d < embed_dim
        push(peL1, vocab_batch_flat[vbase + d])
        d = d + 1.0
      end
      push(pmL1, 1.0)
      map_set(psL1, "proto_count", map_get(psL1, "proto_count") + 1.0)
    end
    bvi = bvi + 1.0
  end
  brep = brep + 1.0
end

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)
let compression_pct = (1.0 - pcL1 / num_vocab) * 100.0
let compression_r = floor(compression_pct * 10.0) / 10.0

// Batch classify all vocab → vocab_proto_ids
let mut vocab_flat = []
let mut evi = 0.0
while evi < num_vocab
  let vw = vocab[evi]
  let enc = word_encode_hash(vw, embed_dim)
  let mut norm_sq = 0.0
  let mut d = 0.0
  while d < embed_dim
    let c = enc[d] - cmL1[d]
    norm_sq = norm_sq + c * c
    d = d + 1.0
  end
  let mut inv_norm = 0.0
  if norm_sq > 0.0000001
    inv_norm = 1.0 / sqrt(norm_sq)
  end
  d = 0.0
  while d < embed_dim
    let c = enc[d] - cmL1[d]
    push(vocab_flat, c * inv_norm)
    d = d + 1.0
  end
  evi = evi + 1.0
end

let mut peL1_T_cls = []
let mut td_cls = 0.0
while td_cls < embed_dim
  let mut tp_cls = 0.0
  while tp_cls < pcL1
    push(peL1_T_cls, peL1[tp_cls * embed_dim + td_cls])
    tp_cls = tp_cls + 1.0
  end
  td_cls = td_cls + 1.0
end

let sims_cls = gpu_matmul(vocab_flat, peL1_T_cls, num_vocab, pcL1, embed_dim)

let mut vocab_proto_ids = []
let mut qi = 0.0
while qi < num_vocab
  let row_base = qi * pcL1
  let mut best_id = 0.0
  let mut best_sim = sims_cls[row_base]
  let mut p3 = 1.0
  while p3 < pcL1
    let s3 = sims_cls[row_base + p3]
    if s3 > best_sim
      best_sim = s3
      best_id = p3
    end
    p3 = p3 + 1.0
  end
  push(vocab_proto_ids, best_id)
  qi = qi + 1.0
end

// Build per-occurrence proto_ids via vocab lookup
let mut proto_ids = []
let mut lki = 0.0
while lki < total_words
  let w = all_words[lki]
  let vid_lk = map_get(vocab_map, w)
  push(proto_ids, vocab_proto_ids[vid_lk])
  lki = lki + 1.0
end

let t_proto_end = time()
let proto_ms = int((t_proto_end - t_proto_start) * 1000.0)
print("  L1 protos: {pcL1_int} (from {num_vocab_int} unique words)")
print("  Compression: {compression_r}%")
print("  Time: {proto_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 3: Build Initial Generative Model
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 3: Build Initial Generative Model ---")
let t_model_start = time()

// 3a: Word frequencies in training set
let mut word_freq = []
let mut wfi = 0.0
while wfi < num_vocab
  push(word_freq, 0.0)
  wfi = wfi + 1.0
end
let mut ti = 0.0
while ti < train_end_idx
  let w = all_words[ti]
  let vid_wf = map_get(vocab_map, w)
  word_freq[vid_wf] = word_freq[vid_wf] + 1.0
  ti = ti + 1.0
end

// 3b: Reverse vocabulary — proto → word list + frequencies
let mut rv_words = []
let mut rv_freqs = []
let mut rv_offsets_final = []
let mut rp = 0.0
while rp < pcL1
  push(rv_offsets_final, len(rv_words))
  let mut rv_vi = 0.0
  while rv_vi < num_vocab
    if vocab_proto_ids[rv_vi] == rp
      if word_freq[rv_vi] > 0.0
        push(rv_words, vocab[rv_vi])
        push(rv_freqs, word_freq[rv_vi])
      end
    end
    rv_vi = rv_vi + 1.0
  end
  rp = rp + 1.0
end
push(rv_offsets_final, len(rv_words))

let rv_total_int = int(len(rv_words))
print("  Reverse vocab: {rv_total_int} word-proto entries")

// 3c: Training proto sequence
let mut train_proto_seq = []
let mut tpi = 0.0
while tpi < train_end_idx
  push(train_proto_seq, proto_ids[tpi])
  tpi = tpi + 1.0
end
let train_seq_len = len(train_proto_seq)

// 3d: 1st-order Markov table → mutable cur_table (persists across epochs)
let init_table = markov1_build(train_proto_seq, train_seq_len, pcL1)
let mut cur_table = []
let mut cti = 0.0
while cti < pcL1 * pcL1
  push(cur_table, init_table[cti])
  cti = cti + 1.0
end

// Row sums for normalization
let mut row_sums = []
let mut rsi = 0.0
while rsi < pcL1
  let mut rsum = 0.0
  let mut rsj = 0.0
  while rsj < pcL1
    rsum = rsum + cur_table[rsi * pcL1 + rsj]
    rsj = rsj + 1.0
  end
  push(row_sums, rsum)
  rsi = rsi + 1.0
end
print("  Markov table: {pcL1_int} x {pcL1_int}")

// 3e: Bigram set from training corpus (for coherence evaluation)
let mut bigram_set = map()
let mut bi = 1.0
while bi < train_end_idx
  let bg = all_words[bi - 1.0] + " " + all_words[bi]
  map_set(bigram_set, bg, 1.0)
  bi = bi + 1.0
end

// 3f: Chapter seed protos — first 5 proto IDs of chapter 1
let mut seed_protos = []
let mut si = 0.0
while si < 5.0
  push(seed_protos, proto_ids[si])
  si = si + 1.0
end

let t_model_end = time()
let model_ms = int((t_model_end - t_model_start) * 1000.0)
print("  Time: {model_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// Helper Functions
// ══════════════════════════════════════════════════════════════════════

// ── Helper: sample_word_soft ──
fn sample_word_soft(activation, act_len, rv_w, rv_f, rv_off, temp_word)
  let mut cand_words = []
  let mut cand_weights = []

  let mut pi_s = 0.0
  while pi_s < act_len
    let act = activation[pi_s]
    if act > 0.01
      let s_start = rv_off[pi_s]
      let s_end = rv_off[pi_s + 1.0]
      let mut fi = s_start
      while fi < s_end
        let w = act * log(rv_f[fi] + 1.0)
        push(cand_words, rv_w[fi])
        push(cand_weights, w)
        fi = fi + 1.0
      end
    end
    pi_s = pi_s + 1.0
  end

  if len(cand_words) < 1.0
    return "the"
  end

  let num_cand = len(cand_words)
  let mut max_w = -100.0
  let mut mwi = 0.0
  while mwi < num_cand
    if cand_weights[mwi] > max_w
      max_w = cand_weights[mwi]
    end
    mwi = mwi + 1.0
  end

  let mut temp_sum = 0.0
  let mut temp_vals = []
  let mut twi = 0.0
  while twi < num_cand
    let e = exp((cand_weights[twi] - max_w) / temp_word)
    push(temp_vals, e)
    temp_sum = temp_sum + e
    twi = twi + 1.0
  end

  let r = random() * temp_sum
  let mut cum = 0.0
  let mut ci_s = 0.0
  while ci_s < num_cand
    cum = cum + temp_vals[ci_s]
    if cum >= r
      return cand_words[ci_s]
    end
    ci_s = ci_s + 1.0
  end
  return cand_words[num_cand - 1.0]
end

// ══════════════════════════════════════════════════════════════════════
// PHASE 4: Attention Epoch Loop
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 4: Attention Epoch Loop ---")
let t_epoch_start = time()

let num_epochs = 4.0
let gen_count = 200.0

// Pre-allocate per-proto surprise accumulators
let mut proto_surprise_sum = []
let mut proto_surprise_count = []
let mut psa_init = 0.0
while psa_init < pcL1
  push(proto_surprise_sum, 0.0)
  push(proto_surprise_count, 0.0)
  psa_init = psa_init + 1.0
end

// Pre-allocate Strategy B working arrays
let mut blended = []
let mut modulated = []
let mut exp_mod = []
let mut activation = []
let mut mod_copy = []
let mut pa_init = 0.0
while pa_init < pcL1
  push(blended, 0.0)
  push(modulated, 0.0)
  push(exp_mod, 0.0)
  push(activation, 0.0)
  push(mod_copy, 0.0)
  pa_init = pa_init + 1.0
end

// Pre-allocate context window
let context_max = 20.0
let mut context_window = []
let mut cwi = 0.0
while cwi < context_max
  push(context_window, 0.0)
  cwi = cwi + 1.0
end

// Pre-allocate generation transition counts (pcL1 x pcL1)
let table_size = pcL1 * pcL1
let mut gen_trans = []
let mut gti_init = 0.0
while gti_init < table_size
  push(gen_trans, 0.0)
  gti_init = gti_init + 1.0
end

// Pre-allocate attention arrays
let mut attn_raw = []
let mut attn_weights = []
let mut awi = 0.0
while awi < context_max
  push(attn_raw, 0.0)
  push(attn_weights, 0.0)
  awi = awi + 1.0
end

// Epoch metrics arrays
let mut epoch_avg_surprise = []
let mut epoch_coherence = []
let mut epoch_diversity = []
let mut epoch_drift_count = []
let mut epoch_ppl = []
let mut epoch_attn_entropy = []

// ── Epoch Loop ──
let mut epoch = 0.0
while epoch < num_epochs
  let epoch_int = int(epoch)
  print("")
  print("  ── Epoch {epoch_int} ──")
  let t_ep_start = time()

  // ────────────────────────────────────────────────────────────────
  // Step 0: Drift + Rebuild (skip epoch 0)
  // ────────────────────────────────────────────────────────────────
  let mut drift_count = 0.0
  if epoch > 0.5
    let t_drift_start = time()

    let mut surp_mean_sum = 0.0
    let mut surp_mean_n = 0.0
    let mut sm_i = 0.0
    while sm_i < pcL1
      if proto_surprise_count[sm_i] > 1.5
        surp_mean_sum = surp_mean_sum + proto_surprise_sum[sm_i] / proto_surprise_count[sm_i]
        surp_mean_n = surp_mean_n + 1.0
      end
      sm_i = sm_i + 1.0
    end
    let mut surp_threshold = 0.99
    if surp_mean_n > 0.5
      surp_threshold = surp_mean_sum / surp_mean_n
    end

    let alpha_high = 0.12 / epoch
    let alpha_low = 0.03 / epoch

    let mut dp = 0.0
    while dp < pcL1
      if proto_surprise_count[dp] > 1.5
        let avg_surp = proto_surprise_sum[dp] / proto_surprise_count[dp]

        let mut alpha = alpha_low
        if avg_surp > surp_threshold
          alpha = alpha_high
        end

        let mut cd = 0.0
        while cd < embed_dim
          normed_buf[cd] = 0.0
          cd = cd + 1.0
        end
        let mut centroid_weight = 0.0

        let mut cv = 0.0
        while cv < num_vocab
          if vocab_proto_ids[cv] == dp
            let wf = word_freq[cv]
            if wf > 0.0
              let vbase = cv * embed_dim
              cd = 0.0
              while cd < embed_dim
                normed_buf[cd] = normed_buf[cd] + wf * vocab_flat[vbase + cd]
                cd = cd + 1.0
              end
              centroid_weight = centroid_weight + wf
            end
          end
          cv = cv + 1.0
        end

        if centroid_weight > 0.5
          let mut cnorm_sq = 0.0
          cd = 0.0
          while cd < embed_dim
            normed_buf[cd] = normed_buf[cd] / centroid_weight
            cnorm_sq = cnorm_sq + normed_buf[cd] * normed_buf[cd]
            cd = cd + 1.0
          end

          if cnorm_sq > 0.000001
            let cinv = 1.0 / sqrt(cnorm_sq)
            cd = 0.0
            while cd < embed_dim
              normed_buf[cd] = normed_buf[cd] * cinv
              cd = cd + 1.0
            end

            let pbase = dp * embed_dim
            let mut dnorm_sq = 0.0
            cd = 0.0
            while cd < embed_dim
              let dv = (1.0 - alpha) * peL1[pbase + cd] + alpha * normed_buf[cd]
              peL1[pbase + cd] = dv
              dnorm_sq = dnorm_sq + dv * dv
              cd = cd + 1.0
            end

            if dnorm_sq > 0.000001
              let dinv = 1.0 / sqrt(dnorm_sq)
              cd = 0.0
              while cd < embed_dim
                peL1[pbase + cd] = peL1[pbase + cd] * dinv
                cd = cd + 1.0
              end
            end

            drift_count = drift_count + 1.0
          end
        end
      end
      dp = dp + 1.0
    end

    let drift_int = int(drift_count)
    print("    Drift: {drift_int} protos updated")

    // Reclassify: transpose peL1 → gpu_matmul → argmax → update vocab_proto_ids
    let mut peL1_T_e = []
    let mut td_e = 0.0
    while td_e < embed_dim
      let mut tp_e = 0.0
      while tp_e < pcL1
        push(peL1_T_e, peL1[tp_e * embed_dim + td_e])
        tp_e = tp_e + 1.0
      end
      td_e = td_e + 1.0
    end
    let sims_e = gpu_matmul(vocab_flat, peL1_T_e, num_vocab, pcL1, embed_dim)

    let mut rci = 0.0
    while rci < num_vocab
      let row_base = rci * pcL1
      let mut best_id = 0.0
      let mut best_sim = sims_e[row_base]
      let mut rcp = 1.0
      while rcp < pcL1
        let s = sims_e[row_base + rcp]
        if s > best_sim
          best_sim = s
          best_id = rcp
        end
        rcp = rcp + 1.0
      end
      vocab_proto_ids[rci] = best_id
      rci = rci + 1.0
    end

    let mut rpi = 0.0
    while rpi < total_words
      let w = all_words[rpi]
      let vid_rp = map_get(vocab_map, w)
      proto_ids[rpi] = vocab_proto_ids[vid_rp]
      rpi = rpi + 1.0
    end

    let mut rtsi = 0.0
    while rtsi < train_end_idx
      train_proto_seq[rtsi] = proto_ids[rtsi]
      rtsi = rtsi + 1.0
    end

    let new_table = markov1_build(train_proto_seq, train_seq_len, pcL1)

    let reinforce = 15.0 / epoch
    let mut rei = 0.0
    while rei < table_size
      cur_table[rei] = new_table[rei] + reinforce * gen_trans[rei]
      rei = rei + 1.0
    end

    let mut rrs = 0.0
    while rrs < pcL1
      let mut rsum = 0.0
      let mut rrj = 0.0
      while rrj < pcL1
        rsum = rsum + cur_table[rrs * pcL1 + rrj]
        rrj = rrj + 1.0
      end
      row_sums[rrs] = rsum
      rrs = rrs + 1.0
    end

    let t_drift_end = time()
    let drift_ms = int((t_drift_end - t_drift_start) * 1000.0)
    print("    Rebuild: {drift_ms} ms")
  end

  push(epoch_drift_count, drift_count)

  // ────────────────────────────────────────────────────────────────
  // Build reverse vocab for this epoch (fresh arrays per epoch)
  // ────────────────────────────────────────────────────────────────
  let mut rv_words_e = []
  let mut rv_freqs_e = []
  let mut rv_offsets_e = []
  let mut rvp = 0.0
  while rvp < pcL1
    push(rv_offsets_e, len(rv_words_e))
    let mut rv_ev = 0.0
    while rv_ev < num_vocab
      if vocab_proto_ids[rv_ev] == rvp
        if word_freq[rv_ev] > 0.0
          push(rv_words_e, vocab[rv_ev])
          push(rv_freqs_e, word_freq[rv_ev])
        end
      end
      rv_ev = rv_ev + 1.0
    end
    rvp = rvp + 1.0
  end
  push(rv_offsets_e, len(rv_words_e))

  // ────────────────────────────────────────────────────────────────
  // Step 1: Generate 200 words (Strategy B + Attention) + collect surprise
  // ────────────────────────────────────────────────────────────────

  // Zero-reset surprise accumulators and gen_trans
  let mut zs = 0.0
  while zs < pcL1
    proto_surprise_sum[zs] = 0.0
    proto_surprise_count[zs] = 0.0
    zs = zs + 1.0
  end
  let mut zt = 0.0
  while zt < table_size
    gen_trans[zt] = 0.0
    zt = zt + 1.0
  end

  // Reset context window with seed protos
  let mut ctx_count = 0.0
  let mut cwri = 0.0
  while cwri < context_max
    if cwri < 5.0
      context_window[cwri] = seed_protos[cwri]
      ctx_count = ctx_count + 1.0
    else
      context_window[cwri] = 0.0
    end
    cwri = cwri + 1.0
  end

  let mut curr_proto_b = seed_protos[4]
  let mut context_strength = 1.0
  let decay_rate = 0.15
  let attn_temp = 0.5
  let top_k = 2.0
  let mut total_surprise = 0.0
  let mut attn_entropy_sum = 0.0
  let mut attn_entropy_count = 0.0

  let mut gen_words = []

  let mut gb = 0.0
  while gb < gen_count

    // ── Stage 1: CONTEXT PRIOR (Hybrid Dot-Product Attention) ──
    let mut bi_init = 0.0
    while bi_init < pcL1
      blended[bi_init] = 0.0
      bi_init = bi_init + 1.0
    end

    let ctx_len = ctx_count

    // 1a. Dot-product attention: Q·K for each context position
    let q_base = curr_proto_b * embed_dim
    let mut max_a = -100.0
    let mut ci_a = 0.0
    while ci_a < ctx_len
      let k_proto = context_window[ci_a]
      let k_base = k_proto * embed_dim
      let mut dot_val = 0.0
      let mut dd = 0.0
      while dd < embed_dim
        dot_val = dot_val + peL1[q_base + dd] * peL1[k_base + dd]
        dd = dd + 1.0
      end
      attn_raw[ci_a] = dot_val
      if dot_val > max_a
        max_a = dot_val
      end
      ci_a = ci_a + 1.0
    end

    // 1b. Softmax with temperature
    let mut attn_exp_sum = 0.0
    ci_a = 0.0
    while ci_a < ctx_len
      let ae = exp((attn_raw[ci_a] - max_a) / attn_temp)
      attn_weights[ci_a] = ae
      attn_exp_sum = attn_exp_sum + ae
      ci_a = ci_a + 1.0
    end

    // 1c. Normalize + track attention entropy
    let mut step_ent = 0.0
    ci_a = 0.0
    while ci_a < ctx_len
      attn_weights[ci_a] = attn_weights[ci_a] / attn_exp_sum
      let aw = attn_weights[ci_a]
      if aw > 0.0001
        step_ent = step_ent - aw * log(aw)
      end
      ci_a = ci_a + 1.0
    end
    attn_entropy_sum = attn_entropy_sum + step_ent
    attn_entropy_count = attn_entropy_count + 1.0

    // 1d. Positional weights (reuse attn_raw for storage)
    let mut pos_sum = 0.0
    ci_a = 0.0
    while ci_a < ctx_len
      let dist = ctx_len - 1.0 - ci_a
      let pw = exp(0.0 - decay_rate * dist)
      attn_raw[ci_a] = pw
      pos_sum = pos_sum + pw
      ci_a = ci_a + 1.0
    end
    ci_a = 0.0
    while ci_a < ctx_len
      attn_raw[ci_a] = attn_raw[ci_a] / pos_sum
      ci_a = ci_a + 1.0
    end

    // 1e. Hybrid blend: 0.5 * content_attention + 0.5 * positional
    let mut weight_sum = 0.0
    let mut ci_b = 0.0
    while ci_b < ctx_len
      let cw = 0.5 * attn_weights[ci_b] + 0.5 * attn_raw[ci_b]
      let ctx_proto = context_window[ci_b]
      let ctx_base = ctx_proto * pcL1
      let ctx_row_sum = row_sums[ctx_proto]
      if ctx_row_sum > 0.5
        let mut bj = 0.0
        while bj < pcL1
          blended[bj] = blended[bj] + cw * (cur_table[ctx_base + bj] / ctx_row_sum)
          bj = bj + 1.0
        end
      end
      weight_sum = weight_sum + cw
      ci_b = ci_b + 1.0
    end

    if weight_sum > 0.001
      let mut ni = 0.0
      while ni < pcL1
        blended[ni] = blended[ni] / weight_sum
        ni = ni + 1.0
      end
    end

    // ── Stage 2: TRANSITION DISTRIBUTION ──
    let trans_base = curr_proto_b * pcL1
    let trans_row_sum = row_sums[curr_proto_b]
    let mut mod_sum = 0.0
    let mut mi = 0.0
    while mi < pcL1
      let mut trans_prob = 0.0
      if trans_row_sum > 0.5
        trans_prob = cur_table[trans_base + mi] / trans_row_sum
      end
      let mod_val = trans_prob * (1.0 + context_strength * blended[mi])
      modulated[mi] = mod_val
      mod_sum = mod_sum + mod_val
      mi = mi + 1.0
    end

    // ── Stage 3: SAMPLE NEXT PROTO ──
    let mut chosen_proto = 0.0
    let mut sampling_sum = 0.0
    if mod_sum > 0.0001
      let mut max_mod = 0.0
      let mut mmi = 0.0
      while mmi < pcL1
        if modulated[mmi] > max_mod
          max_mod = modulated[mmi]
        end
        mmi = mmi + 1.0
      end
      mmi = 0.0
      while mmi < pcL1
        let e = exp((modulated[mmi] - max_mod) / 0.4)
        exp_mod[mmi] = e
        sampling_sum = sampling_sum + e
        mmi = mmi + 1.0
      end
      let r = random() * sampling_sum
      let mut cum = 0.0
      mmi = 0.0
      while mmi < pcL1
        cum = cum + exp_mod[mmi]
        if cum >= r
          chosen_proto = mmi
          mmi = pcL1
        end
        mmi = mmi + 1.0
      end
    else
      chosen_proto = markov1_sample(cur_table, curr_proto_b, pcL1, 0.4)
    end

    // ── Stage 4: SOFT ACTIVATION ──
    let mut act_sum = 0.0
    let mut ai = 0.0
    while ai < pcL1
      activation[ai] = 0.0
      mod_copy[ai] = modulated[ai]
      ai = ai + 1.0
    end

    let mut topk_count = 0.0
    let mut ki = 0.0
    while ki < top_k
      let mut best_val = -1.0
      let mut best_idx = 0.0
      let mut kj = 0.0
      while kj < pcL1
        if mod_copy[kj] > best_val
          best_val = mod_copy[kj]
          best_idx = kj
        end
        kj = kj + 1.0
      end
      if best_val > 0.0001
        let mut boost = best_val
        if best_idx == chosen_proto
          boost = best_val * 3.0
        end
        activation[best_idx] = boost
        act_sum = act_sum + boost
        mod_copy[best_idx] = -1.0
        topk_count = topk_count + 1.0
      end
      ki = ki + 1.0
    end

    if act_sum > 0.0001
      let mut tki = 0.0
      while tki < pcL1
        if activation[tki] > 0.0
          activation[tki] = activation[tki] / act_sum
        end
        tki = tki + 1.0
      end
    else
      activation[chosen_proto] = 1.0
    end

    // ── Stage 5: SAMPLE WORD ──
    let word = sample_word_soft(activation, pcL1, rv_words_e, rv_freqs_e, rv_offsets_e, 0.55)
    push(gen_words, word)

    // ── Stage 6: SURPRISE FEEDBACK + context update ──
    let mut chosen_prob = 0.0
    if sampling_sum > 0.0001
      chosen_prob = exp_mod[chosen_proto] / sampling_sum
    end

    // Information-theoretic surprisal for convergence metrics
    let mut surprisal = 7.0
    if chosen_prob > 0.001
      surprisal = 0.0 - log(chosen_prob)
    end
    total_surprise = total_surprise + surprisal

    // Raw surprise for drift + context strength feedback
    let raw_surprise = 1.0 - chosen_prob
    proto_surprise_sum[chosen_proto] = proto_surprise_sum[chosen_proto] + raw_surprise
    proto_surprise_count[chosen_proto] = proto_surprise_count[chosen_proto] + 1.0

    if raw_surprise > 0.7
      context_strength = context_strength * 0.7
      if context_strength < 0.3
        context_strength = 0.3
      end
    elif raw_surprise < 0.3
      context_strength = context_strength * 1.3
      if context_strength > 3.0
        context_strength = 3.0
      end
    end

    let gt_idx = curr_proto_b * pcL1 + chosen_proto
    gen_trans[gt_idx] = gen_trans[gt_idx] + 1.0

    // Update context window
    if ctx_count < context_max
      context_window[ctx_count] = chosen_proto
      ctx_count = ctx_count + 1.0
    else
      let mut shift_i = 0.0
      while shift_i < context_max - 1.0
        context_window[shift_i] = context_window[shift_i + 1.0]
        shift_i = shift_i + 1.0
      end
      context_window[context_max - 1.0] = chosen_proto
    end
    curr_proto_b = chosen_proto

    gb = gb + 1.0
  end

  // ────────────────────────────────────────────────────────────────
  // Step 2: Evaluate this epoch
  // ────────────────────────────────────────────────────────────────

  // Average surprise
  let avg_surprise = total_surprise / gen_count
  push(epoch_avg_surprise, avg_surprise)

  // Attention entropy for this epoch
  let mut epoch_ae = 0.0
  if attn_entropy_count > 0.5
    epoch_ae = attn_entropy_sum / attn_entropy_count
  end
  push(epoch_attn_entropy, epoch_ae)

  // Bigram coherence
  let mut e_bigram_match = 0.0
  let mut e_bigram_total = 0.0
  let mut ebi = 1.0
  while ebi < gen_count
    let bg_e = gen_words[ebi - 1.0] + " " + gen_words[ebi]
    e_bigram_total = e_bigram_total + 1.0
    if map_has(bigram_set, bg_e) > 0.0
      e_bigram_match = e_bigram_match + 1.0
    end
    ebi = ebi + 1.0
  end
  let mut e_coherence = 0.0
  if e_bigram_total > 0.0
    e_coherence = (e_bigram_match / e_bigram_total) * 100.0
  end
  push(epoch_coherence, e_coherence)

  // Diversity
  let mut e_unique_map = map()
  let mut e_unique_count = 0.0
  let mut dui = 0.0
  while dui < gen_count
    let dw = gen_words[dui]
    if map_has(e_unique_map, dw) == 0.0
      map_set(e_unique_map, dw, 1.0)
      e_unique_count = e_unique_count + 1.0
    end
    dui = dui + 1.0
  end
  push(epoch_diversity, e_unique_count)

  // Perplexity on ch12
  let mut log_prob_sum = 0.0
  let mut ppl_count = 0.0
  let mut tsi = test_start_idx + 1.0
  while tsi < test_end_idx
    let prev_p = proto_ids[tsi - 1.0]
    let curr_p = proto_ids[tsi]
    let rs = row_sums[prev_p]
    let count_val = cur_table[prev_p * pcL1 + curr_p]
    let prob = (count_val + 1.0) / (rs + pcL1)
    log_prob_sum = log_prob_sum + log(prob)
    ppl_count = ppl_count + 1.0
    tsi = tsi + 1.0
  end
  let mut e_ppl = 999.0
  if ppl_count > 0.0
    e_ppl = exp(0.0 - log_prob_sum / ppl_count)
  end
  push(epoch_ppl, e_ppl)

  // Print epoch summary
  let avg_r = floor(avg_surprise * 1000.0) / 1000.0
  let coh_r = floor(e_coherence * 10.0) / 10.0
  let div_int = int(e_unique_count)
  let ppl_r = floor(e_ppl * 100.0) / 100.0
  let drift_int = int(drift_count)
  let ae_r = floor(epoch_ae * 1000.0) / 1000.0
  print("    Surprise: {avg_r}  Coherence: {coh_r}%  Diversity: {div_int}  PPL: {ppl_r}  Drift: {drift_int}  AttnH: {ae_r}")

  // Print text preview
  let mut gen_text = ""
  let mut gti = 0.0
  while gti < gen_count
    if gti > 0.0
      gen_text = gen_text + " "
    end
    gen_text = gen_text + gen_words[gti]
    gti = gti + 1.0
  end
  let gen_preview = substr(gen_text, 0.0, 120.0)
  print("    Output: {gen_preview}...")

  let t_ep_end = time()
  let ep_ms = int((t_ep_end - t_ep_start) * 1000.0)
  print("    Time: {ep_ms} ms")

  epoch = epoch + 1.0
end

let t_epoch_end = time()
let epoch_total_ms = int((t_epoch_end - t_epoch_start) * 1000.0)
print("")
print("  Total epoch time: {epoch_total_ms} ms")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 5: Epoch Comparison + Attention Analysis
// ══════════════════════════════════════════════════════════════════════
print("--- Phase 5: Epoch Comparison + Attention Analysis ---")
print("  Epoch | Surprise | Coherence | Diversity | PPL     | Drifted | AttnEnt")
print("  ------|----------|-----------|-----------|---------|---------|--------")

let mut tbl_e = 0.0
while tbl_e < num_epochs
  let tbl_int = int(tbl_e)
  let s_r = floor(epoch_avg_surprise[tbl_e] * 1000.0) / 1000.0
  let c_r = floor(epoch_coherence[tbl_e] * 10.0) / 10.0
  let d_int = int(epoch_diversity[tbl_e])
  let p_r = floor(epoch_ppl[tbl_e] * 100.0) / 100.0
  let dr_int = int(epoch_drift_count[tbl_e])
  let a_r = floor(epoch_attn_entropy[tbl_e] * 1000.0) / 1000.0
  print("  {tbl_int}     | {s_r}    | {c_r}%     | {d_int}        | {p_r}   | {dr_int}      | {a_r}")
  tbl_e = tbl_e + 1.0
end
print("")

// Compute positional entropy reference (full 20-position window)
let mut pos_ref_sum = 0.0
let mut pri = 0.0
while pri < context_max
  let dist = context_max - 1.0 - pri
  let pw = exp(0.0 - 0.15 * dist)
  attn_raw[pri] = pw
  pos_ref_sum = pos_ref_sum + pw
  pri = pri + 1.0
end
let mut pos_ref_ent = 0.0
pri = 0.0
while pri < context_max
  let pw_n = attn_raw[pri] / pos_ref_sum
  if pw_n > 0.0001
    pos_ref_ent = pos_ref_ent - pw_n * log(pw_n)
  end
  pri = pri + 1.0
end

let pos_ent_r = floor(pos_ref_ent * 1000.0) / 1000.0
let last_e = num_epochs - 1.0
let last_attn_ent = epoch_attn_entropy[last_e]
let last_ae_r = floor(last_attn_ent * 1000.0) / 1000.0
print("  Positional entropy (reference, 20 positions): {pos_ent_r}")
print("  Final epoch attention entropy: {last_ae_r}")
if last_attn_ent < pos_ref_ent
  print("  -> Content-aware attention is MORE selective than positional decay")
else
  print("  -> Attention entropy not yet below positional (may need more epochs)")
end
print("")

// Vocab coverage (static — doesn't change with epochs)
let mut test_unique = map()
let mut test_found = 0.0
let mut test_total_unique = 0.0
let mut tui = test_start_idx
while tui < test_end_idx
  let tw = all_words[tui]
  if map_has(test_unique, tw) == 0.0
    map_set(test_unique, tw, 1.0)
    test_total_unique = test_total_unique + 1.0
    if map_has(vocab_map, tw) > 0.0
      let vid_t = map_get(vocab_map, tw)
      if word_freq[vid_t] > 0.0
        test_found = test_found + 1.0
      end
    end
  end
  tui = tui + 1.0
end
let mut vocab_coverage = 0.0
if test_total_unique > 0.0
  vocab_coverage = (test_found / test_total_unique) * 100.0
end
let coverage_r = floor(vocab_coverage * 10.0) / 10.0
let test_total_int = int(test_total_unique)
let test_found_int = int(test_found)
print("  Vocab coverage: {coverage_r}% ({test_found_int}/{test_total_int} unique ch12 words)")
print("")

// ══════════════════════════════════════════════════════════════════════
// PHASE 6: PASS/FAIL (8 criteria)
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL ===")
let mut npass = 0.0
let mut nfail = 0.0

let final_ppl = epoch_ppl[last_e]
let final_coherence = epoch_coherence[last_e]
let final_diversity = epoch_diversity[last_e]
let epoch1_coherence = epoch_coherence[1]

// Test 1: Final perplexity < 50
let final_ppl_r = floor(final_ppl * 100.0) / 100.0
if final_ppl < 50.0
  print("PASS [1/8]: Final perplexity {final_ppl_r} < 50")
  npass = npass + 1.0
else
  print("FAIL [1/8]: Final perplexity {final_ppl_r} >= 50")
  nfail = nfail + 1.0
end

// Test 2: Vocab coverage > 80%
if vocab_coverage > 80.0
  print("PASS [2/8]: Vocab coverage {coverage_r}% > 80%")
  npass = npass + 1.0
else
  print("FAIL [2/8]: Vocab coverage {coverage_r}% <= 80%")
  nfail = nfail + 1.0
end

// Test 3: Final coherence > 10%
let final_coh_r = floor(final_coherence * 10.0) / 10.0
if final_coherence > 10.0
  print("PASS [3/8]: Final coherence {final_coh_r}% > 10%")
  npass = npass + 1.0
else
  print("FAIL [3/8]: Final coherence {final_coh_r}% <= 10%")
  nfail = nfail + 1.0
end

// Test 4: Final diversity > 30 unique / 200
let final_div_int = int(final_diversity)
if final_diversity > 30.0
  print("PASS [4/8]: Final diversity {final_div_int} > 30")
  npass = npass + 1.0
else
  print("FAIL [4/8]: Final diversity {final_div_int} <= 30")
  nfail = nfail + 1.0
end

// Test 5: Coherence improvement — epoch 3 >= epoch 1
let coh_e1_r = floor(epoch1_coherence * 10.0) / 10.0
if final_coherence >= epoch1_coherence
  print("PASS [5/8]: Coherence improvement {final_coh_r}% >= {coh_e1_r}%")
  npass = npass + 1.0
else
  print("FAIL [5/8]: Coherence improvement {final_coh_r}% < {coh_e1_r}%")
  nfail = nfail + 1.0
end

// Test 6: Attention selectivity — final attn entropy < positional entropy
if last_attn_ent < pos_ref_ent
  print("PASS [6/8]: Attention selectivity {last_ae_r} < {pos_ent_r}")
  npass = npass + 1.0
else
  print("FAIL [6/8]: Attention selectivity {last_ae_r} >= {pos_ent_r}")
  nfail = nfail + 1.0
end

// Test 7: Attention consistency — ALL epochs attn entropy < positional entropy
let mut max_ae = 0.0
let mut mae_i = 0.0
while mae_i < num_epochs
  if epoch_attn_entropy[mae_i] > max_ae
    max_ae = epoch_attn_entropy[mae_i]
  end
  mae_i = mae_i + 1.0
end
let max_ae_r = floor(max_ae * 1000.0) / 1000.0
if max_ae < pos_ref_ent
  print("PASS [7/8]: Attention consistency {max_ae_r} < {pos_ent_r} (all epochs)")
  npass = npass + 1.0
else
  print("FAIL [7/8]: Attention consistency {max_ae_r} >= {pos_ent_r}")
  nfail = nfail + 1.0
end

// Test 8: Pipeline completed
print("PASS [8/8]: Pipeline completed")
npass = npass + 1.0

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/8 passed, {nfail_int}/8 failed")
if nfail < 0.5
  print("OVERALL: PASS ({npass_int}/8)")
else
  print("OVERALL: FAIL ({npass_int}/8)")
end

print("")
print("--- Phase 19 complete: Dot-Product Attention over Context Window ---")
