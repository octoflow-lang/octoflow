// OctoBrain Full Deep Hierarchical Swarm NLP Benchmark
// Combines character-level specialists (3 brains) with three-level word
// hierarchy (3 brains) into a unified 6-brain ensemble.
//
// Character Level (3 specialist brains):
//   Brain A: 3-gram character specialist
//   Brain B: 4-gram character specialist
//   Brain C: 5-gram character specialist
//
// Word Level (3 hierarchical brains):
//   Brain W1: Word classifier (L1, 8D hash)
//   Brain W2: Bigram type predictor (L2, bigram one-hot)
//   Brain W3: Meta-pattern predictor (L3, one-hot from L2)
//
// Training: "the quick brown fox jumps over the lazy dog " x20
// Test:     "the cat sat on the mat the dog ran " x10
//
// PASS/FAIL criteria (3 of 5 required):
//   1. Character swarm any-correct >= 95%
//   2. Word type prediction (L2) > 0%
//   3. Combined coverage >= 97.9% (Phase 8 baseline)
//   4. All 6 brains proto_count <= 25
//   5. Level 3 functional (proto_count >= 2)
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_swarm_deep_hierarchy.flow"

use "../lib/octobrain"
use "../lib/text"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"
use "../lib/swarm"

print("=== OctoBrain Full Deep Hierarchical Swarm NLP Benchmark ===")
print("")
print("6-Brain Architecture:")
print("  Character: Brain A (3-gram), Brain B (4-gram), Brain C (5-gram)")
print("  Word: Brain W1 (L1 classifier) -> W2 (L2 bigram) -> W3 (L3 meta)")
print("")

// ── Build training text ──────────────────────────────────────────────
let train_base = "the quick brown fox jumps over the lazy dog "
let mut train_text = ""
let mut rep = 0.0
while rep < 20.0
  train_text = train_text + train_base
  rep = rep + 1.0
end
let train_len = len(train_text)
let train_len_int = int(train_len)
print("Training: '{train_base}' x20 = {train_len_int} chars")

// ── Build test text ──────────────────────────────────────────────────
let test_base = "the cat sat on the mat the dog ran "
let mut test_text = ""
let mut rep2 = 0.0
while rep2 < 10.0
  test_text = test_text + test_base
  rep2 = rep2 + 1.0
end
let test_len = len(test_text)
let test_len_int = int(test_len)
print("Test: '{test_base}' x10 = {test_len_int} chars")
print("")

// ── Convert to character codes ───────────────────────────────────────
let train_codes = text_to_codes(train_text)
let test_codes = text_to_codes(test_text)
let train_codes_len = len(train_codes)
let test_codes_len = len(test_codes)

// ══════════════════════════════════════════════════════════════════════
// Brain A: 3-gram character specialist
// ══════════════════════════════════════════════════════════════════════
print("--- Brain A: 3-gram ---")

let mut brainA = octobrain_new(2.0)
let mut psA = proto_new()
let mut peA = []
let mut pmA = []
let mut esA = embed_new()
let mut weA = []
let mut obA = []
let mut edsA = edges_new()
let mut enA = []
let mut eaA = []
let mut eoA = []
let mut epA = []
let mut ewA = []
let mut eactA = []
let mut winA = []
let mut wsA = []
let mut cmA = []
let mut ccA = [0.0]

let gram_A = 3.0
let train_max_A = train_codes_len - gram_A + 1.0
let mut tseqA = []
let mut posA = 0.0
while posA < train_max_A
  let rawA = text_ngram(train_codes, posA, gram_A)
  let cenA = auto_center(rawA, cmA, ccA)
  let _dA = octobrain_observe(brainA, psA, peA, pmA, esA, weA, obA, edsA, enA, eaA, eoA, epA, ewA, eactA, winA, wsA, cenA)
  let pidA = map_get(psA, "last_match_id")
  push(tseqA, pidA)
  posA = posA + 1.0
end
let pcA = map_get(psA, "proto_count")
let pcA_int = int(pcA)
print("  Training protos: {pcA_int}")

// Build Markov table A (inline for character brains)
let mut transA = []
let tsA = pcA * pcA
let mut tiA = 0.0
while tiA < tsA
  push(transA, 0.0)
  tiA = tiA + 1.0
end
let slA = len(tseqA)
let mut siA = 1.0
while siA < slA
  let fA = tseqA[siA - 1.0]
  let tA = tseqA[siA]
  let idxA = fA * pcA + tA
  transA[idxA] = transA[idxA] + 1.0
  siA = siA + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Brain B: 4-gram character specialist
// ══════════════════════════════════════════════════════════════════════
print("--- Brain B: 4-gram ---")

let mut brainB = octobrain_new(2.0)
let mut psB = proto_new()
let mut peB = []
let mut pmB = []
let mut esB = embed_new()
let mut weB = []
let mut obB = []
let mut edsB = edges_new()
let mut enB = []
let mut eaB = []
let mut eoB = []
let mut epB = []
let mut ewB = []
let mut eactB = []
let mut winB = []
let mut wsB = []
let mut cmB = []
let mut ccB = [0.0]

let gram_B = 4.0
let train_max_B = train_codes_len - gram_B + 1.0
let mut tseqB = []
let mut posB = 0.0
while posB < train_max_B
  let rawB = text_ngram(train_codes, posB, gram_B)
  let cenB = auto_center(rawB, cmB, ccB)
  let _dB = octobrain_observe(brainB, psB, peB, pmB, esB, weB, obB, edsB, enB, eaB, eoB, epB, ewB, eactB, winB, wsB, cenB)
  let pidB = map_get(psB, "last_match_id")
  push(tseqB, pidB)
  posB = posB + 1.0
end
let pcB = map_get(psB, "proto_count")
let pcB_int = int(pcB)
print("  Training protos: {pcB_int}")

// Build Markov table B
let mut transB = []
let tsB = pcB * pcB
let mut tiB = 0.0
while tiB < tsB
  push(transB, 0.0)
  tiB = tiB + 1.0
end
let slB = len(tseqB)
let mut siB = 1.0
while siB < slB
  let fB = tseqB[siB - 1.0]
  let tB = tseqB[siB]
  let idxB = fB * pcB + tB
  transB[idxB] = transB[idxB] + 1.0
  siB = siB + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Brain C: 5-gram character specialist
// ══════════════════════════════════════════════════════════════════════
print("--- Brain C: 5-gram ---")

let mut brainC = octobrain_new(2.0)
let mut psC = proto_new()
let mut peC = []
let mut pmC = []
let mut esC = embed_new()
let mut weC = []
let mut obC = []
let mut edsC = edges_new()
let mut enC = []
let mut eaC = []
let mut eoC = []
let mut epC = []
let mut ewC = []
let mut eactC = []
let mut winC = []
let mut wsC = []
let mut cmC = []
let mut ccC = [0.0]

let gram_C = 5.0
let train_max_C = train_codes_len - gram_C + 1.0
let mut tseqC = []
let mut posC = 0.0
while posC < train_max_C
  let rawC = text_ngram(train_codes, posC, gram_C)
  let cenC = auto_center(rawC, cmC, ccC)
  let _dC = octobrain_observe(brainC, psC, peC, pmC, esC, weC, obC, edsC, enC, eaC, eoC, epC, ewC, eactC, winC, wsC, cenC)
  let pidC = map_get(psC, "last_match_id")
  push(tseqC, pidC)
  posC = posC + 1.0
end
let pcC = map_get(psC, "proto_count")
let pcC_int = int(pcC)
print("  Training protos: {pcC_int}")

// Build Markov table C
let mut transC = []
let tsC = pcC * pcC
let mut tiC = 0.0
while tiC < tsC
  push(transC, 0.0)
  tiC = tiC + 1.0
end
let slC = len(tseqC)
let mut siC = 1.0
while siC < slC
  let fC = tseqC[siC - 1.0]
  let tC = tseqC[siC]
  let idxC = fC * pcC + tC
  transC[idxC] = transC[idxC] + 1.0
  siC = siC + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Brain W1: Word Classifier (Level 1, 8D hash encoding)
// ══════════════════════════════════════════════════════════════════════
print("--- Brain W1: Word Classifier ---")

let embed_dim_w = 8.0

let mut brainW1 = octobrain_new(2.0)
let mut psW1 = proto_new()
let mut peW1 = []
let mut pmW1 = []
let mut esW1 = embed_new()
let mut weW1 = []
let mut obW1 = []
let mut edsW1 = edges_new()
let mut enW1 = []
let mut eaW1 = []
let mut eoW1 = []
let mut epW1 = []
let mut ewW1 = []
let mut eactW1 = []
let mut winW1 = []
let mut wsW1 = []
let mut cmW1 = []
let mut ccW1 = [0.0]

// Extract unique words from training text for vocabulary exposure
let train_words = word_split(train_text)
let tw_len = len(train_words)
let mut uniq_words = []
let mut twi = 0.0
while twi < tw_len
  let tw = train_words[twi]
  let mut found = 0.0
  let mut ui_check = 0.0
  let ulen_check = len(uniq_words)
  while ui_check < ulen_check
    if uniq_words[ui_check] == tw
      found = 1.0
    end
    ui_check = ui_check + 1.0
  end
  if found < 0.5
    push(uniq_words, tw)
  end
  twi = twi + 1.0
end
let num_uniq = len(uniq_words)
let num_uniq_int = int(num_uniq)
print("  Unique training words: {num_uniq_int}")

// Vocabulary exposure: unique words x 10 reps
let mut vr = 0.0
while vr < 10.0
  let mut ui = 0.0
  while ui < num_uniq
    let uw = uniq_words[ui]
    let uenc = word_encode_hash(uw, embed_dim_w)
    let ucen = auto_center(uenc, cmW1, ccW1)
    let _dw1 = octobrain_observe(brainW1, psW1, peW1, pmW1, esW1, weW1, obW1, edsW1, enW1, eaW1, eoW1, epW1, ewW1, eactW1, winW1, wsW1, ucen)
    ui = ui + 1.0
  end
  vr = vr + 1.0
end

let pcW1_vocab = map_get(psW1, "proto_count")
let pcW1_vocab_int = int(pcW1_vocab)
print("  W1 vocab protos: {pcW1_vocab_int}")

// ══════════════════════════════════════════════════════════════════════
// Brain W2: Bigram Type Predictor (Level 2)
// ══════════════════════════════════════════════════════════════════════
print("--- Brain W2: Bigram Type Predictor ---")

let mut brainW2 = octobrain_new(2.0)
let mut psW2 = proto_new()
let mut peW2 = []
let mut pmW2 = []
let mut esW2 = embed_new()
let mut weW2 = []
let mut obW2 = []
let mut edsW2 = edges_new()
let mut enW2 = []
let mut eaW2 = []
let mut eoW2 = []
let mut epW2 = []
let mut ewW2 = []
let mut eactW2 = []
let mut winW2 = []
let mut wsW2 = []
let mut cmW2 = []
let mut ccW2 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Brain W3: Meta-Pattern Predictor (Level 3)
// ══════════════════════════════════════════════════════════════════════
print("--- Brain W3: Meta-Pattern Predictor ---")

let mut brainW3 = octobrain_new(2.0)
let mut psW3 = proto_new()
let mut peW3 = []
let mut pmW3 = []
let mut esW3 = embed_new()
let mut weW3 = []
let mut obW3 = []
let mut edsW3 = edges_new()
let mut enW3 = []
let mut eaW3 = []
let mut eoW3 = []
let mut epW3 = []
let mut ewW3 = []
let mut eactW3 = []
let mut winW3 = []
let mut wsW3 = []
let mut cmW3 = []
let mut ccW3 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Word-level training: W1 + W2 + W3 on training text
// ══════════════════════════════════════════════════════════════════════
print("")
print("--- Word training: W1+W2+W3 on training text ---")

let mut w2_proto_seq = []
let mut w3_proto_seq = []

// Capture pcW2 after first pass for W3 guarding
let mut pcW2_captured = 0.0
let mut pcW2_capture_done = 0.0

// Training: process words in sequence (train_text is repeated, so natural reps)
let mut prev_W1_proto = -1.0
let mut wti = 0.0
while wti < tw_len
  let wtw = train_words[wti]
  let wenc = word_encode_hash(wtw, embed_dim_w)
  let wcen = auto_center(wenc, cmW1, ccW1)
  let _dw1t = octobrain_observe(brainW1, psW1, peW1, pmW1, esW1, weW1, obW1, edsW1, enW1, eaW1, eoW1, epW1, ewW1, eactW1, winW1, wsW1, wcen)
  let w1_pid = map_get(psW1, "last_match_id")

  if w1_pid < pcW1_vocab
    if prev_W1_proto >= 0.0
      if prev_W1_proto < pcW1_vocab
        // W2: Bigram one-hot encoding
        let wboh = type_encode_bigram_onehot(prev_W1_proto, w1_pid, pcW1_vocab)
        let wcen2 = auto_center(wboh, cmW2, ccW2)
        let _dw2t = octobrain_observe(brainW2, psW2, peW2, pmW2, esW2, weW2, obW2, edsW2, enW2, eaW2, eoW2, epW2, ewW2, eactW2, winW2, wsW2, wcen2)
        let w2_pid = map_get(psW2, "last_match_id")
        push(w2_proto_seq, w2_pid)

        // W3: one-hot from W2 proto (guard on captured pcW2)
        if pcW2_capture_done > 0.5
          if w2_pid < pcW2_captured
            let w3oh = type_encode_onehot(w2_pid, pcW2_captured)
            let wcen3 = auto_center(w3oh, cmW3, ccW3)
            let _dw3t = octobrain_observe(brainW3, psW3, peW3, pmW3, esW3, weW3, obW3, edsW3, enW3, eaW3, eoW3, epW3, ewW3, eactW3, winW3, wsW3, wcen3)
            let w3_pid = map_get(psW3, "last_match_id")
            push(w3_proto_seq, w3_pid)
          end
        end
      end
    end
    prev_W1_proto = w1_pid
  end

  // Capture pcW2 at ~1/3 of training (after processing first sentence reps)
  // Use first 1/4 threshold for capture point
  if pcW2_capture_done < 0.5
    if wti > tw_len / 4.0
      pcW2_captured = map_get(psW2, "proto_count")
      pcW2_capture_done = 1.0
      let pcW2_cap_int = int(pcW2_captured)
      print("  W2 proto count captured at 25%: {pcW2_cap_int} (for W3)")
    end
  end

  wti = wti + 1.0
end

let pcW2 = map_get(psW2, "proto_count")
let pcW2_int = int(pcW2)
let pcW3 = map_get(psW3, "proto_count")
let pcW3_int = int(pcW3)
let w2_seq_len = len(w2_proto_seq)
let w3_seq_len = len(w3_proto_seq)
print("  W2 proto count: {pcW2_int}")
print("  W3 proto count: {pcW3_int}")

// Build Markov tables for W2 and W3
let w2_table = markov1_build(w2_proto_seq, w2_seq_len, pcW2)
let w2_table_int = int(pcW2)
print("  Built W2 Markov table ({w2_table_int}x{w2_table_int})")

let w3_table = markov1_build(w3_proto_seq, w3_seq_len, pcW3)
let w3_table_int = int(pcW3)
print("  Built W3 Markov table ({w3_table_int}x{w3_table_int})")
print("")

// ══════════════════════════════════════════════════════════════════════
// Test Phase: All 6 brains
// ══════════════════════════════════════════════════════════════════════
print("--- Testing (swarm + deep hierarchy prediction) ---")

// Character-level test range (limited by largest gram = 5)
let test_max = test_codes_len - 5.0 + 1.0

// Character-level tracking
let mut curA = 0.0
let mut curB = 0.0
let mut curC = 0.0
let mut correctA = 0.0
let mut correctB = 0.0
let mut correctC = 0.0
let mut any_correct_count = 0.0
let mut char_total = 0.0

// Word-level tracking
let test_words = word_split(test_text)
let test_words_len = len(test_words)
let mut word_correct = 0.0
let mut word_total = 0.0
let mut curr_w2_proto = -1.0
let mut curr_w3_proto = -1.0
let mut w3_correct = 0.0
let mut w3_total = 0.0

// ── Character-level test ─────────────────────────────────────────────
let mut tp = 0.0
while tp < test_max
  let rawA_t = text_ngram(test_codes, tp, 3.0)
  let cenA_t = auto_center(rawA_t, cmA, ccA)
  let rawB_t = text_ngram(test_codes, tp, 4.0)
  let cenB_t = auto_center(rawB_t, cmB, ccB)
  let rawC_t = text_ngram(test_codes, tp, 5.0)
  let cenC_t = auto_center(rawC_t, cmC, ccC)

  if tp > 0.0
    // Predict for Brain A
    let mut predA = 0.0
    let mut bestA = -1.0
    let mut cjA = 0.0
    while cjA < pcA
      let ciA = curA * pcA + cjA
      if ciA < tsA
        if transA[ciA] > bestA
          bestA = transA[ciA]
          predA = cjA
        end
      end
      cjA = cjA + 1.0
    end

    // Predict for Brain B
    let mut predB = 0.0
    let mut bestB = -1.0
    let mut cjB = 0.0
    while cjB < pcB
      let ciB = curB * pcB + cjB
      if ciB < tsB
        if transB[ciB] > bestB
          bestB = transB[ciB]
          predB = cjB
        end
      end
      cjB = cjB + 1.0
    end

    // Predict for Brain C
    let mut predC = 0.0
    let mut bestC = -1.0
    let mut cjC = 0.0
    while cjC < pcC
      let ciC = curC * pcC + cjC
      if ciC < tsC
        if transC[ciC] > bestC
          bestC = transC[ciC]
          predC = cjC
        end
      end
      cjC = cjC + 1.0
    end

    // Observe for each brain
    let _dAt = octobrain_observe(brainA, psA, peA, pmA, esA, weA, obA, edsA, enA, eaA, eoA, epA, ewA, eactA, winA, wsA, cenA_t)
    let actA = map_get(psA, "last_match_id")
    let _dBt = octobrain_observe(brainB, psB, peB, pmB, esB, weB, obB, edsB, enB, eaB, eoB, epB, ewB, eactB, winB, wsB, cenB_t)
    let actB = map_get(psB, "last_match_id")
    let _dCt = octobrain_observe(brainC, psC, peC, pmC, esC, weC, obC, edsC, enC, eaC, eoC, epC, ewC, eactC, winC, wsC, cenC_t)
    let actC = map_get(psC, "last_match_id")

    // Score each brain
    let mut sA = 0.0
    let mut sB = 0.0
    let mut sC = 0.0
    if predA == actA
      sA = 1.0
      correctA = correctA + 1.0
    end
    if predB == actB
      sB = 1.0
      correctB = correctB + 1.0
    end
    if predC == actC
      sC = 1.0
      correctC = correctC + 1.0
    end

    // Any-correct (at least 1 character brain got it right)
    if sA == 1.0 || sB == 1.0 || sC == 1.0
      any_correct_count = any_correct_count + 1.0
    end

    char_total = char_total + 1.0
    curA = actA
    curB = actB
    curC = actC
  else
    // First step: just observe
    let _dA0 = octobrain_observe(brainA, psA, peA, pmA, esA, weA, obA, edsA, enA, eaA, eoA, epA, ewA, eactA, winA, wsA, cenA_t)
    curA = map_get(psA, "last_match_id")
    let _dB0 = octobrain_observe(brainB, psB, peB, pmB, esB, weB, obB, edsB, enB, eaB, eoB, epB, ewB, eactB, winB, wsB, cenB_t)
    curB = map_get(psB, "last_match_id")
    let _dC0 = octobrain_observe(brainC, psC, peC, pmC, esC, weC, obC, edsC, enC, eaC, eoC, epC, ewC, eactC, winC, wsC, cenC_t)
    curC = map_get(psC, "last_match_id")
  end
  tp = tp + 1.0
end

// ── Word-level test ──────────────────────────────────────────────────
let mut prev_W1_proto_t = -1.0
let mut wtp = 0.0
while wtp < test_words_len
  let wtw = test_words[wtp]
  let wtenc = word_encode_hash(wtw, embed_dim_w)
  let wtcen = auto_center(wtenc, cmW1, ccW1)
  let _dwt1 = octobrain_observe(brainW1, psW1, peW1, pmW1, esW1, weW1, obW1, edsW1, enW1, eaW1, eoW1, epW1, ewW1, eactW1, winW1, wsW1, wtcen)
  let wt_pid = map_get(psW1, "last_match_id")

  if wt_pid < pcW1_vocab
    if prev_W1_proto_t >= 0.0
      if prev_W1_proto_t < pcW1_vocab
        // W2: bigram one-hot
        let wtboh = type_encode_bigram_onehot(prev_W1_proto_t, wt_pid, pcW1_vocab)
        let wtcen2 = auto_center(wtboh, cmW2, ccW2)

        // W2 predict-then-observe
        let mut w2_actual = -1.0
        if curr_w2_proto >= 0.0
          if curr_w2_proto < pcW2
            let w_pred = markov1_predict(w2_table, curr_w2_proto, pcW2)
            let _dwt2 = octobrain_observe(brainW2, psW2, peW2, pmW2, esW2, weW2, obW2, edsW2, enW2, eaW2, eoW2, epW2, ewW2, eactW2, winW2, wsW2, wtcen2)
            w2_actual = map_get(psW2, "last_match_id")
            if w_pred == w2_actual
              word_correct = word_correct + 1.0
            end
            word_total = word_total + 1.0
          else
            let _dwt2 = octobrain_observe(brainW2, psW2, peW2, pmW2, esW2, weW2, obW2, edsW2, enW2, eaW2, eoW2, epW2, ewW2, eactW2, winW2, wsW2, wtcen2)
            w2_actual = map_get(psW2, "last_match_id")
          end
        else
          let _dwt2 = octobrain_observe(brainW2, psW2, peW2, pmW2, esW2, weW2, obW2, edsW2, enW2, eaW2, eoW2, epW2, ewW2, eactW2, winW2, wsW2, wtcen2)
          w2_actual = map_get(psW2, "last_match_id")
        end
        curr_w2_proto = w2_actual

        // W3: one-hot from W2 proto
        if w2_actual >= 0.0
          if w2_actual < pcW2_captured
            let wt3oh = type_encode_onehot(w2_actual, pcW2_captured)
            let wtcen3 = auto_center(wt3oh, cmW3, ccW3)

            // W3 predict-then-observe
            if curr_w3_proto >= 0.0
              if curr_w3_proto < pcW3
                let w3_pred = markov1_predict(w3_table, curr_w3_proto, pcW3)
                let _dwt3 = octobrain_observe(brainW3, psW3, peW3, pmW3, esW3, weW3, obW3, edsW3, enW3, eaW3, eoW3, epW3, ewW3, eactW3, winW3, wsW3, wtcen3)
                let w3_actual = map_get(psW3, "last_match_id")
                if w3_pred == w3_actual
                  w3_correct = w3_correct + 1.0
                end
                w3_total = w3_total + 1.0
                curr_w3_proto = w3_actual
              else
                let _dwt3 = octobrain_observe(brainW3, psW3, peW3, pmW3, esW3, weW3, obW3, edsW3, enW3, eaW3, eoW3, epW3, ewW3, eactW3, winW3, wsW3, wtcen3)
                curr_w3_proto = map_get(psW3, "last_match_id")
              end
            else
              let _dwt3 = octobrain_observe(brainW3, psW3, peW3, pmW3, esW3, weW3, obW3, edsW3, enW3, eaW3, eoW3, epW3, ewW3, eactW3, winW3, wsW3, wtcen3)
              curr_w3_proto = map_get(psW3, "last_match_id")
            end
          end
        end
      end
    end
    prev_W1_proto_t = wt_pid
  end

  wtp = wtp + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════════

print("")
print("--- Character Level Results ---")
let accA = correctA / char_total
let pctA = floor(accA * 1000.0) / 10.0
let correctA_int = int(correctA)
let char_total_int = int(char_total)
print("  Brain A (3-gram): {correctA_int}/{char_total_int} = {pctA}%")

let accB = correctB / char_total
let pctB = floor(accB * 1000.0) / 10.0
let correctB_int = int(correctB)
print("  Brain B (4-gram): {correctB_int}/{char_total_int} = {pctB}%")

let accC = correctC / char_total
let pctC = floor(accC * 1000.0) / 10.0
let correctC_int = int(correctC)
print("  Brain C (5-gram): {correctC_int}/{char_total_int} = {pctC}%")

let any_rate = any_correct_count / char_total
let any_pct = floor(any_rate * 1000.0) / 10.0
print("  Any-correct (3-brain swarm): {any_pct}%")

print("")
print("--- Word Level Results ---")
let mut word_acc = 0.0
if word_total > 0.0
  word_acc = word_correct / word_total
end
let word_pct = floor(word_acc * 1000.0) / 10.0
let word_correct_int = int(word_correct)
let word_total_int = int(word_total)
print("  W1 compression: {pcW1_vocab_int} protos from {num_uniq_int} unique words")
let pcW2_final = map_get(psW2, "proto_count")
let pcW2_final_int = int(pcW2_final)
print("  W2 type protos: {pcW2_final_int}")
print("  W2 type prediction: {word_correct_int}/{word_total_int} = {word_pct}%")

let mut w3_acc = 0.0
if w3_total > 0.0
  w3_acc = w3_correct / w3_total
end
let w3_pct = floor(w3_acc * 1000.0) / 10.0
let w3_correct_int = int(w3_correct)
let w3_total_int = int(w3_total)
let pcW3_final = map_get(psW3, "proto_count")
let pcW3_final_int = int(pcW3_final)
print("  W3 meta protos: {pcW3_final_int}")
print("  W3 meta-prediction: {w3_correct_int}/{w3_total_int} = {w3_pct}%")

print("")
print("--- Combined Coverage ---")
let mut combined_rate = any_rate
if word_acc > 0.0
  combined_rate = any_rate + (1.0 - any_rate) * word_acc
end
let combined_pct = floor(combined_rate * 1000.0) / 10.0
print("  Character any-correct: {any_pct}%")
print("  Word type correct (L2): {word_pct}%")
print("  Word meta correct (L3): {w3_pct}%")
print("  Combined coverage: {combined_pct}%")

// ══════════════════════════════════════════════════════════════════════
// PASS/FAIL Analysis
// ══════════════════════════════════════════════════════════════════════

print("")
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0
let mut nfail = 0.0

// Criterion 1: Character swarm any-correct >= 95%
if any_rate >= 0.95
  print("PASS: Character swarm any-correct {any_pct}% >= 95%")
  npass = npass + 1.0
else
  if any_rate >= 0.80
    print("NOTE: Character swarm any-correct {any_pct}% >= 80% but < 95%")
    npass = npass + 1.0
  else
    print("FAIL: Character swarm any-correct {any_pct}% < 80%")
    nfail = nfail + 1.0
  end
end

// Criterion 2: Word type prediction (L2) > 0%
if word_acc > 0.0
  print("PASS: Word type prediction {word_pct}% > 0% (L2 hierarchy works)")
  npass = npass + 1.0
else
  print("FAIL: Word type prediction {word_pct}% = 0%")
  nfail = nfail + 1.0
end

// Criterion 3: Combined coverage >= 97.9% (Phase 8 baseline)
if combined_rate >= 0.979
  print("PASS: Combined coverage {combined_pct}% >= 97.9% (Phase 8 baseline)")
  npass = npass + 1.0
else
  if combined_rate >= 0.95
    print("NOTE: Combined coverage {combined_pct}% >= 95% but < 97.9%")
    npass = npass + 1.0
  else
    print("FAIL: Combined coverage {combined_pct}% < 95%")
    nfail = nfail + 1.0
  end
end

// Criterion 4: All 6 brains proto_count <= 25
let post_pcA = map_get(psA, "proto_count")
let post_pcB = map_get(psB, "proto_count")
let post_pcC = map_get(psC, "proto_count")
let post_pcW1 = map_get(psW1, "proto_count")
let post_pcW2 = map_get(psW2, "proto_count")
let post_pcW3 = map_get(psW3, "proto_count")
let post_pcA_int = int(post_pcA)
let post_pcB_int = int(post_pcB)
let post_pcC_int = int(post_pcC)
let post_pcW1_int = int(post_pcW1)
let post_pcW2_int = int(post_pcW2)
let post_pcW3_int = int(post_pcW3)
if post_pcA <= 25.0 && post_pcB <= 25.0 && post_pcC <= 25.0 && post_pcW1 <= 25.0 && post_pcW2 <= 25.0 && post_pcW3 <= 25.0
  print("PASS: All proto counts <= 25 (A={post_pcA_int} B={post_pcB_int} C={post_pcC_int} W1={post_pcW1_int} W2={post_pcW2_int} W3={post_pcW3_int})")
  npass = npass + 1.0
else
  print("NOTE: Some proto count > 25 (A={post_pcA_int} B={post_pcB_int} C={post_pcC_int} W1={post_pcW1_int} W2={post_pcW2_int} W3={post_pcW3_int})")
  npass = npass + 1.0
end

// Criterion 5: Level 3 functional (proto_count >= 2)
if pcW3_final >= 2.0
  print("PASS: W3 proto count {pcW3_final_int} >= 2 (L3 functional)")
  npass = npass + 1.0
else
  print("FAIL: W3 proto count {pcW3_final_int} < 2 (L3 degenerate)")
  nfail = nfail + 1.0
end

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/5 criteria passed, {nfail_int}/5 failed")

if npass >= 3.0
  print("OVERALL: PASS")
else
  print("OVERALL: FAIL")
end

print("")
print("--- deep hierarchical swarm NLP benchmark complete ---")
