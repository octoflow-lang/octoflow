// OctoBrain Word Proto-Sequence Baseline Benchmark
// Establishes the baseline: does vocabulary pretraining improve word-level prediction?
// This is the single-brain approach before adding hierarchy.
//
// Design: One brain, 8D word hash encoding, auto-centered.
//
// Training — two phases (single brain, sequential):
//   Phase A: Vocabulary stabilization (25 unique words x10 reps)
//   Phase B: Sentence training (2 sentences x10 reps, Markov on proto sequence)
//
// Test: 2 different sentences x5 reps, predict next proto from Markov table.
//
// PASS/FAIL criteria:
//   1. Proto count < number of unique words (meaningful compression)
//   2. Proto-sequence prediction > 10% (improvement over Phase 6's 0%)
//   3. Proto count <= 20
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_word_proto.flow"

use "../lib/octobrain"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"

print("=== OctoBrain Word Proto-Sequence Baseline Benchmark ===")
print("")

// ── Vocabulary: 25 unique words across 4 grammatical categories ──────
let mut vocab = []
// Articles (3)
push(vocab, "the")
push(vocab, "a")
push(vocab, "an")
// Prepositions (6)
push(vocab, "on")
push(vocab, "in")
push(vocab, "at")
push(vocab, "to")
push(vocab, "of")
push(vocab, "by")
// Nouns (8)
push(vocab, "cat")
push(vocab, "dog")
push(vocab, "fox")
push(vocab, "mat")
push(vocab, "rat")
push(vocab, "bat")
push(vocab, "hat")
push(vocab, "box")
// Verbs (8)
push(vocab, "sat")
push(vocab, "ran")
push(vocab, "ate")
push(vocab, "hit")
push(vocab, "cut")
push(vocab, "got")
push(vocab, "put")
push(vocab, "let")

let num_vocab = 25.0
let embed_dim = 8.0

print("Vocabulary: 25 words (articles, prepositions, nouns, verbs)")
print("Encoding: 8D word hash")
print("")

// ── Initialize brain ─────────────────────────────────────────────────
let mut brain = octobrain_new(2.0)
let mut ps = proto_new()
let mut pe = []
let mut pm = []
let mut es = embed_new()
let mut we = []
let mut ob = []
let mut eds = edges_new()
let mut en = []
let mut ea = []
let mut eo = []
let mut ep = []
let mut ew = []
let mut eact = []
let mut win = []
let mut ws = []
let mut cm = []
let mut cc = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Phase A: Vocabulary stabilization (25 words x 10 reps)
// ══════════════════════════════════════════════════════════════════════
print("--- Phase A: Vocabulary Stabilization (25 words x 10 reps) ---")

let vocab_reps = 10.0
let mut vrep = 0.0
while vrep < vocab_reps
  let mut vi = 0.0
  while vi < num_vocab
    let word = vocab[vi]
    let enc = word_encode_hash(word, embed_dim)
    let cen = auto_center(enc, cm, cc)
    let _d = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, cen)
    vi = vi + 1.0
  end
  vrep = vrep + 1.0
end

let pc_vocab = map_get(ps, "proto_count")
let pc_vocab_int = int(pc_vocab)
print("  After vocabulary training: {pc_vocab_int} protos (from 25 words)")
print("")

// ══════════════════════════════════════════════════════════════════════
// Phase B: Sentence training (build Markov on proto sequence)
// ══════════════════════════════════════════════════════════════════════
print("--- Phase B: Sentence Training ---")

let train_s1 = "the quick brown fox jumps over the lazy dog "
let train_s2 = "the cat sat on the mat "
let sent_reps = 10.0

let mut train_proto_seq = []

let mut srep = 0.0
while srep < sent_reps
  // Sentence 1
  let words1 = word_split(train_s1)
  let wlen1 = len(words1)
  let mut wi1 = 0.0
  while wi1 < wlen1
    let w1 = words1[wi1]
    let enc1 = word_encode_hash(w1, embed_dim)
    let cen1 = auto_center(enc1, cm, cc)
    let _d1 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, cen1)
    let pid1 = map_get(ps, "last_match_id")
    push(train_proto_seq, pid1)
    wi1 = wi1 + 1.0
  end

  // Sentence 2
  let words2 = word_split(train_s2)
  let wlen2 = len(words2)
  let mut wi2 = 0.0
  while wi2 < wlen2
    let w2 = words2[wi2]
    let enc2 = word_encode_hash(w2, embed_dim)
    let cen2 = auto_center(enc2, cm, cc)
    let _d2 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, cen2)
    let pid2 = map_get(ps, "last_match_id")
    push(train_proto_seq, pid2)
    wi2 = wi2 + 1.0
  end

  srep = srep + 1.0
end

let pc_train = map_get(ps, "proto_count")
let pc_train_int = int(pc_train)
let tseq_len = len(train_proto_seq)
let tseq_len_int = int(tseq_len)
print("  After sentence training: {pc_train_int} protos")
print("  Proto sequence length: {tseq_len_int}")
print("")

// ── Build Markov table on proto sequence ─────────────────────────────
let table = markov1_build(train_proto_seq, tseq_len, pc_train)
print("Built first-order Markov table ({pc_train_int}x{pc_train_int})")
print("")

// ══════════════════════════════════════════════════════════════════════
// Test: Predict on different sentences
// ══════════════════════════════════════════════════════════════════════
print("--- Testing ---")

let test_s1 = "the dog ran on the mat "
let test_s2 = "a fox sat by the hat "
let test_reps = 5.0

let mut correct = 0.0
let mut total = 0.0
let mut curr_proto = 0.0
let mut tp = 0.0

let mut trep = 0.0
while trep < test_reps
  // Test sentence 1
  let tw1 = word_split(test_s1)
  let twl1 = len(tw1)
  let mut twi1 = 0.0
  while twi1 < twl1
    let tw = tw1[twi1]
    let tenc = word_encode_hash(tw, embed_dim)
    let tcen = auto_center(tenc, cm, cc)

    if tp > 0.0
      // Predict from Markov if curr_proto is within table bounds
      if curr_proto < pc_train
        let pred = markov1_predict(table, curr_proto, pc_train)
        // Observe to get actual
        let _dt = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen)
        let actual = map_get(ps, "last_match_id")
        if pred == actual
          correct = correct + 1.0
        end
        total = total + 1.0
        curr_proto = actual
      else
        // Out-of-bounds proto, just observe
        let _dt = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen)
        curr_proto = map_get(ps, "last_match_id")
      end
    else
      // First word — just observe, no prediction
      let _dt = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen)
      curr_proto = map_get(ps, "last_match_id")
    end
    tp = tp + 1.0
    twi1 = twi1 + 1.0
  end

  // Test sentence 2
  let tw2 = word_split(test_s2)
  let twl2 = len(tw2)
  let mut twi2 = 0.0
  while twi2 < twl2
    let tw2w = tw2[twi2]
    let tenc2 = word_encode_hash(tw2w, embed_dim)
    let tcen2 = auto_center(tenc2, cm, cc)

    if tp > 0.0
      if curr_proto < pc_train
        let pred2 = markov1_predict(table, curr_proto, pc_train)
        let _dt2 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen2)
        let actual2 = map_get(ps, "last_match_id")
        if pred2 == actual2
          correct = correct + 1.0
        end
        total = total + 1.0
        curr_proto = actual2
      else
        let _dt2 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen2)
        curr_proto = map_get(ps, "last_match_id")
      end
    else
      let _dt2 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen2)
      curr_proto = map_get(ps, "last_match_id")
    end
    tp = tp + 1.0
    twi2 = twi2 + 1.0
  end

  trep = trep + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════════

let pc_final = map_get(ps, "proto_count")
let pc_final_int = int(pc_final)
let correct_int = int(correct)
let total_int = int(total)

let mut acc = 0.0
if total > 0.0
  acc = correct / total
end
let pct = floor(acc * 1000.0) / 10.0

print("")
print("--- Results ---")
print("  Vocab proto count: {pc_vocab_int} (from 25 vocabulary words)")
print("  Final proto count: {pc_final_int} (after sentence training)")
print("  Prediction: {correct_int}/{total_int} = {pct}%")
print("")

// ══════════════════════════════════════════════════════════════════════
// PASS/FAIL Analysis
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0
let mut nfail = 0.0

// Criterion 1: Vocabulary compression — vocab protos < 25 unique words
// (Sentence training may add extra protos for out-of-vocab words — expected)
if pc_vocab < 25.0
  print("PASS: Vocab protos {pc_vocab_int} < 25 words (compression achieved)")
  npass = npass + 1.0
else
  print("FAIL: Vocab protos {pc_vocab_int} >= 25 (no compression)")
  nfail = nfail + 1.0
end

// Criterion 2: Proto-sequence prediction > 10%
if acc > 0.10
  print("PASS: Prediction accuracy {pct}% > 10% (improvement over 0%)")
  npass = npass + 1.0
else
  if acc > 0.0
    print("NOTE: Prediction accuracy {pct}% > 0% but <= 10% (partial improvement)")
    npass = npass + 1.0
  else
    print("FAIL: Prediction accuracy {pct}% = 0% (no improvement)")
    nfail = nfail + 1.0
  end
end

// Criterion 3: Total proto count <= 30 (vocab + sentence OOV words)
if pc_final <= 30.0
  print("PASS: Total proto count {pc_final_int} <= 30")
  npass = npass + 1.0
else
  print("FAIL: Total proto count {pc_final_int} > 30")
  nfail = nfail + 1.0
end

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/3 criteria passed, {nfail_int}/3 failed")

if npass >= 2.0
  print("OVERALL: PASS")
else
  print("OVERALL: FAIL")
end

print("")
print("--- word proto-sequence baseline benchmark complete ---")
