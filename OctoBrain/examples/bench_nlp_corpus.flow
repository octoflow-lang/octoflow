// OctoBrain Larger Corpus NLP Benchmark
// Validates that NLP findings hold on larger, more varied text.
// Scales from ~900 chars to ~3000 chars training, ~1500 chars test.
// Runs 4-gram encoding with both first-order and second-order Markov prediction.
//
// Training: 6 distinct sentences x5 reps (~3000 chars)
// Test:     3 different sentences x5 reps (~1500 chars)
//
// Single brain with 4-gram character encoding + auto_center.
// Builds BOTH first-order and second-order Markov tables from training,
// then evaluates both simultaneously on test data with bounds checking
// for new protos created during test.
//
// PASS/FAIL criteria:
//   1. First-order 4-gram accuracy >= 50%
//   2. Second-order >= first-order (or within 5% if both > 50%)
//   3. Proto count <= 40
//   4. New test protos <= 10
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_corpus.flow"

use "../lib/octobrain"
use "../lib/text"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"

print("=== OctoBrain Larger Corpus NLP Benchmark ===")
print("")

// ── Build training text (~3000 chars, 6 distinct sentences x5 reps) ──
let s1 = "the quick brown fox jumps over the lazy dog "
let s2 = "a stitch in time saves nine and an apple a day keeps the doctor away "
let s3 = "the rain in spain falls mainly on the plain "
let s4 = "to be or not to be that is the question "
let s5 = "she sells sea shells by the sea shore "
let s6 = "how much wood would a woodchuck chuck if a woodchuck could chuck wood "

let mut train_text = ""
let mut rep = 0.0
while rep < 5.0
  train_text = train_text + s1
  train_text = train_text + s2
  train_text = train_text + s3
  train_text = train_text + s4
  train_text = train_text + s5
  train_text = train_text + s6
  rep = rep + 1.0
end

let train_len = len(train_text)
let train_len_int = int(train_len)
print("Training: 6 sentences x5 reps = {train_len_int} chars")

// ── Build test text (~1500 chars, 3 different sentences x5 reps) ────
let t1 = "the cat sat on the mat and the dog ran in the rain "
let t2 = "a bird in the hand is worth two in the bush "
let t3 = "to err is human to forgive divine "

let mut test_text = ""
let mut rep2 = 0.0
while rep2 < 5.0
  test_text = test_text + t1
  test_text = test_text + t2
  test_text = test_text + t3
  rep2 = rep2 + 1.0
end

let test_len = len(test_text)
let test_len_int = int(test_len)
print("Test: 3 sentences x5 reps = {test_len_int} chars")
print("")

// ── Convert to codes ────────────────────────────────────────────
let train_codes = text_to_codes(train_text)
let test_codes = text_to_codes(test_text)
let train_codes_len = len(train_codes)
let test_codes_len = len(test_codes)

print("4-gram encoding, auto-centered")
print("")

// ── Initialize brain with all required arrays ───────────────────
let mut brain = octobrain_new(2.0)
let mut ps = proto_new()
let mut pe = []
let mut pm = []
let mut es = embed_new()
let mut we = []
let mut ob = []
let mut eds = edges_new()
let mut en = []
let mut ea = []
let mut eo = []
let mut ep = []
let mut ew = []
let mut eact = []
let mut win = []
let mut ws = []

// Auto-centering state
let mut cm = []
let mut cc = [0.0]

let gram_size = 4.0
let train_max = train_codes_len - gram_size + 1.0

// ══════════════════════════════════════════════════════════════════
// Training pass: feed all 4-grams, record proto sequence
// ══════════════════════════════════════════════════════════════════
let mut tseq = []
let mut pos = 0.0
while pos < train_max
  let raw = text_ngram(train_codes, pos, gram_size)
  let centered = auto_center(raw, cm, cc)
  let d = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, centered)
  let pid = map_get(ps, "last_match_id")
  push(tseq, pid)
  pos = pos + 1.0
end

let pc = map_get(ps, "proto_count")
let pc_int = int(pc)
print("Training protos: {pc_int}")

// ══════════════════════════════════════════════════════════════════
// Build BOTH Markov tables from training proto sequence
// ══════════════════════════════════════════════════════════════════
let sl = len(tseq)
let table1 = markov1_build(tseq, sl, pc)
let table2 = markov2_build(tseq, sl, pc)
print("Built first-order Markov table ({pc_int}x{pc_int})")
let pc_cubed = pc * pc * pc
let pc_cubed_int = int(pc_cubed)
print("Built second-order Markov table (size {pc_cubed_int})")
print("")

// ══════════════════════════════════════════════════════════════════
// Test pass: predict with both orders simultaneously
// Bounds checking: only predict if proto IDs are within training
// table dimensions (new protos created during test are skipped)
// ══════════════════════════════════════════════════════════════════
let pre_pc = map_get(ps, "proto_count")
let test_max = test_codes_len - gram_size + 1.0

let mut prev_proto = 0.0
let mut curr_proto = 0.0
let mut correct1 = 0.0
let mut correct2 = 0.0
let mut total1 = 0.0
let mut total2 = 0.0
let mut tp = 0.0

while tp < test_max
  let traw = text_ngram(test_codes, tp, gram_size)
  let tcen = auto_center(traw, cm, cc)

  if tp > 0.0
    // Check if curr_proto is within training table bounds
    let mut do_pred1 = 1.0
    if curr_proto >= pc
      do_pred1 = 0.0
    end

    // Observe to get actual proto (always needed)
    let dt = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen)
    let actual = map_get(ps, "last_match_id")

    // First-order prediction (available from tp >= 1)
    if do_pred1 > 0.5
      let pred1 = markov1_predict(table1, curr_proto, pc)
      if pred1 == actual
        correct1 = correct1 + 1.0
      end
      total1 = total1 + 1.0
    end

    // Second-order prediction (available from tp >= 2)
    if tp > 1.0
      let mut do_pred2 = 1.0
      if prev_proto >= pc || curr_proto >= pc
        do_pred2 = 0.0
      end
      if do_pred2 > 0.5
        let pred2 = markov2_predict(table2, prev_proto, curr_proto, pc)
        if pred2 == actual
          correct2 = correct2 + 1.0
        end
        total2 = total2 + 1.0
      end
    end

    // Update context
    prev_proto = curr_proto
    curr_proto = actual
  else
    // First observation — no prediction possible
    let dt0 = octobrain_observe(brain, ps, pe, pm, es, we, ob, eds, en, ea, eo, ep, ew, eact, win, ws, tcen)
    curr_proto = map_get(ps, "last_match_id")
  end

  tp = tp + 1.0
end

// ══════════════════════════════════════════════════════════════════
// Compute new test protos
// ══════════════════════════════════════════════════════════════════
let post_pc = map_get(ps, "proto_count")
let new_protos = post_pc - pre_pc
let new_protos_int = int(new_protos)

// ══════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════

let acc1 = correct1 / total1
let pct1 = floor(acc1 * 1000.0) / 10.0
let correct1_int = int(correct1)
let total1_int = int(total1)

let acc2 = correct2 / total2
let pct2 = floor(acc2 * 1000.0) / 10.0
let correct2_int = int(correct2)
let total2_int = int(total2)

print("--- First-Order Markov ---")
print("  Accuracy: {correct1_int}/{total1_int} = {pct1}%")
print("")
print("--- Second-Order Markov ---")
print("  Accuracy: {correct2_int}/{total2_int} = {pct2}%")
print("")
print("New test protos: {new_protos_int}")
print("")

// ══════════════════════════════════════════════════════════════════
// PASS/FAIL Analysis
// ══════════════════════════════════════════════════════════════════
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0

// Criterion 1: First-order 4-gram accuracy >= 50%
if acc1 >= 0.50
  print("PASS: First-order accuracy {pct1}% >= 50%")
  npass = npass + 1.0
else
  print("FAIL: First-order accuracy {pct1}% < 50%")
end

// Criterion 2: Second-order >= first-order (or within 5% if both > 50%)
if acc2 >= acc1
  print("PASS: Second-order ({pct2}%) >= first-order ({pct1}%)")
  npass = npass + 1.0
else
  if acc1 > 0.50 && acc2 > 0.50
    let diff = acc1 - acc2
    if diff <= 0.05
      let diff_pct = floor(diff * 1000.0) / 10.0
      print("PASS: Second-order ({pct2}%) within 5% of first-order ({pct1}%), diff={diff_pct}%")
      npass = npass + 1.0
    else
      let diff_pct2 = floor(diff * 1000.0) / 10.0
      print("FAIL: Second-order ({pct2}%) < first-order ({pct1}%) by {diff_pct2}%")
    end
  else
    print("FAIL: Second-order ({pct2}%) < first-order ({pct1}%)")
  end
end

// Criterion 3: Proto count <= 40
let total_pc = int(post_pc)
if post_pc <= 40.0
  print("PASS: Proto count = {total_pc} <= 40")
  npass = npass + 1.0
else
  print("FAIL: Proto count = {total_pc} > 40")
end

// Criterion 4: New test protos <= 10
if new_protos <= 10.0
  print("PASS: New test protos = {new_protos_int} <= 10")
  npass = npass + 1.0
else
  print("FAIL: New test protos = {new_protos_int} > 10")
end

let npass_int = int(npass)
print("")
print("Result: {npass_int}/4 criteria passed")

if npass >= 4.0
  print("OVERALL: PASS")
else
  if npass >= 3.0
    print("OVERALL: PARTIAL PASS")
  else
    print("OVERALL: FAIL")
  end
end

print("")
print("--- larger corpus NLP benchmark complete ---")
