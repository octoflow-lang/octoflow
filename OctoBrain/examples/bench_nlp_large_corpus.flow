// OctoBrain Phase 13: Large Corpus Hierarchy
// Scales training from 10 sentences to 30 sentences, 5 reps each.
// Tests whether the hierarchy maintains accuracy with denser Markov tables
// and whether markov2 finally outperforms markov1 with sufficient data.
//
// Architecture:
//   Level 1: Word Classifier (8D hash -> word prototypes)
//   Level 2: Bigram Type Predictor (bigram one-hot [pcL1*2])
//   Level 3: Meta-Pattern Predictor (one-hot [pcL2])
//   Prediction: markov1 AND markov2 at L2 and L3 (head-to-head)
//
// Corpus: 30 training sentences + 10 test sentences (all 50 vocab words used)
// Training: 30 sentences x 5 reps = 150 sentence passes
//
// PASS/FAIL criteria (3 of 5 required):
//   1. L2 markov1 prediction >= 30% (maintains Phase 9 level)
//   2. L2 markov2 prediction > L2 markov1 (data density payoff)
//   3. L1 compression: vocab protos < 40 (from 50 words)
//   4. L2 sequence length > 500 (enough data for markov2)
//   5. Pipeline completes without error
//
// Run: powershell.exe -NoProfile -ExecutionPolicy Bypass -File "C:\OctoFlow\run_test.ps1" run --bin octoflow -- run "C:\OctoFlow\OctoBrain\examples\bench_nlp_large_corpus.flow"

use "../lib/octobrain"
use "../lib/text_word"
use "../lib/preprocess"
use "../lib/proto"
use "../lib/sequence"
use "../lib/swarm"

print("=== OctoBrain Phase 13: Large Corpus Hierarchy ===")
print("")
print("Architecture:")
print("  Level 1: Word Classifier (8D hash)")
print("  Level 2: Bigram Type Predictor (bigram one-hot) + markov1 & markov2")
print("  Level 3: Meta-Pattern Predictor (one-hot) + markov1 & markov2")
print("  Corpus: 30 train + 10 test sentences, 5 reps")
print("")

// ── Vocabulary: 50 words across 6 grammatical categories ──────────────
let mut vocab = []

// Class 0 — Articles/Determiners (5)
push(vocab, "the")
push(vocab, "a")
push(vocab, "an")
push(vocab, "this")
push(vocab, "that")

// Class 1 — Prepositions (8)
push(vocab, "on")
push(vocab, "in")
push(vocab, "at")
push(vocab, "to")
push(vocab, "of")
push(vocab, "by")
push(vocab, "with")
push(vocab, "from")

// Class 2 — Nouns (13)
push(vocab, "cat")
push(vocab, "dog")
push(vocab, "fox")
push(vocab, "mat")
push(vocab, "rat")
push(vocab, "bat")
push(vocab, "hat")
push(vocab, "box")
push(vocab, "bird")
push(vocab, "fish")
push(vocab, "tree")
push(vocab, "hill")
push(vocab, "sun")

// Class 3 — Verbs (12)
push(vocab, "sat")
push(vocab, "ran")
push(vocab, "ate")
push(vocab, "hit")
push(vocab, "cut")
push(vocab, "got")
push(vocab, "put")
push(vocab, "let")
push(vocab, "saw")
push(vocab, "had")
push(vocab, "did")
push(vocab, "set")

// Class 4 — Adjectives (7)
push(vocab, "big")
push(vocab, "old")
push(vocab, "red")
push(vocab, "hot")
push(vocab, "dry")
push(vocab, "wet")
push(vocab, "new")

// Class 5 — Adverbs (5)
push(vocab, "now")
push(vocab, "then")
push(vocab, "fast")
push(vocab, "well")
push(vocab, "just")

let num_vocab = len(vocab)
let num_vocab_int = int(num_vocab)
let embed_dim = 8.0

print("Vocabulary: {num_vocab_int} words across 6 categories")

// ══════════════════════════════════════════════════════════════════════
// Training corpus: 30 sentences (3x Phase 9)
// Uses all 50 vocab words. Exercises all 6 grammatical categories.
// ══════════════════════════════════════════════════════════════════════
let mut train_sentences = []

// Block 1: Simple subject-verb-preposition patterns (10 sentences)
push(train_sentences, "the cat sat on the mat ")
push(train_sentences, "a dog ran to the hill ")
push(train_sentences, "the fox sat by the tree ")
push(train_sentences, "a rat ran in the box ")
push(train_sentences, "the bat sat on the hat ")
push(train_sentences, "a bird sat in the tree ")
push(train_sentences, "the fish sat by the sun ")
push(train_sentences, "a cat ran to the dog ")
push(train_sentences, "the rat sat on the hill ")
push(train_sentences, "a fox ran by the mat ")

// Block 2: Adjective-noun patterns with verbs (10 sentences)
push(train_sentences, "the big cat sat on a red mat ")
push(train_sentences, "a fast dog ran to the old tree ")
push(train_sentences, "the hot sun sat on the dry hill ")
push(train_sentences, "a wet bird sat in the new box ")
push(train_sentences, "the old fox ran by a big hat ")
push(train_sentences, "a red bat hit the dry mat ")
push(train_sentences, "the new fish saw the old cat ")
push(train_sentences, "a big rat got the wet hat ")
push(train_sentences, "the fast fox cut by the hot sun ")
push(train_sentences, "a dry bird had the red box ")

// Block 3: Complex patterns with adverbs and prepositions (10 sentences)
push(train_sentences, "just now the cat ran fast to the hill ")
push(train_sentences, "then a dog sat well by the old tree ")
push(train_sentences, "now the big fish saw a new bird in the sun ")
push(train_sentences, "just then the wet fox ran fast from the mat ")
push(train_sentences, "well now a red bat sat on this old hat ")
push(train_sentences, "the fast cat just ran to that big box ")
push(train_sentences, "then the old dog had a hot fish by the tree ")
push(train_sentences, "now a dry rat sat fast on the new hill ")
push(train_sentences, "just now this wet bird ran well from the sun ")
push(train_sentences, "then that big fox saw the red hat at the mat ")

let num_train_sent = len(train_sentences)
let num_train_int = int(num_train_sent)
print("Training sentences: {num_train_int}")

// ══════════════════════════════════════════════════════════════════════
// Test corpus: 10 sentences (2x Phase 9)
// Different word combinations, same grammatical patterns
// ══════════════════════════════════════════════════════════════════════
let mut test_sentences = []

push(test_sentences, "the dog sat on the tree ")
push(test_sentences, "a cat ran to the box ")
push(test_sentences, "the big bird sat by a red hill ")
push(test_sentences, "a fast rat ran in the old hat ")
push(test_sentences, "the wet fox saw the dry mat ")
push(test_sentences, "just now a new bat ran fast to the sun ")
push(test_sentences, "then the hot cat sat well on this big fish ")
push(test_sentences, "now that old dog had a wet bird from the tree ")
push(test_sentences, "the red fox just ran by the new hill ")
push(test_sentences, "a dry hat sat fast at the old box ")

let num_test_sent = len(test_sentences)
let num_test_int = int(num_test_sent)
print("Test sentences: {num_test_int}")
print("")

// ══════════════════════════════════════════════════════════════════════
// Level 1: Word Classifier Brain
// ══════════════════════════════════════════════════════════════════════
print("--- Level 1: Word Classifier ---")

let mut brainL1 = octobrain_new(2.0)
let mut psL1 = proto_new()
let mut peL1 = []
let mut pmL1 = []
let mut esL1 = embed_new()
let mut weL1 = []
let mut obL1 = []
let mut edsL1 = edges_new()
let mut enL1 = []
let mut eaL1 = []
let mut eoL1 = []
let mut epL1 = []
let mut ewL1 = []
let mut eactL1 = []
let mut winL1 = []
let mut wsL1 = []
let mut cmL1 = []
let mut ccL1 = [0.0]

// Vocabulary exposure (50 words x 10 reps)
let mut vrep = 0.0
while vrep < 10.0
  let mut vi = 0.0
  while vi < num_vocab
    let vw = vocab[vi]
    let venc = word_encode_hash(vw, embed_dim)
    let vcen = auto_center(venc, cmL1, ccL1)
    let _d = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, vcen)
    vi = vi + 1.0
  end
  vrep = vrep + 1.0
end

let pcL1 = map_get(psL1, "proto_count")
let pcL1_int = int(pcL1)
print("  L1 vocab protos: {pcL1_int} (from {num_vocab_int} words)")

// ══════════════════════════════════════════════════════════════════════
// Level 2 + Level 3: Initialize brains
// ══════════════════════════════════════════════════════════════════════
let mut brainL2 = octobrain_new(2.0)
let mut psL2 = proto_new()
let mut peL2 = []
let mut pmL2 = []
let mut esL2 = embed_new()
let mut weL2 = []
let mut obL2 = []
let mut edsL2 = edges_new()
let mut enL2 = []
let mut eaL2 = []
let mut eoL2 = []
let mut epL2 = []
let mut ewL2 = []
let mut eactL2 = []
let mut winL2 = []
let mut wsL2 = []
let mut cmL2 = []
let mut ccL2 = [0.0]

let mut brainL3 = octobrain_new(2.0)
let mut psL3 = proto_new()
let mut peL3 = []
let mut pmL3 = []
let mut esL3 = embed_new()
let mut weL3 = []
let mut obL3 = []
let mut edsL3 = edges_new()
let mut enL3 = []
let mut eaL3 = []
let mut eoL3 = []
let mut epL3 = []
let mut ewL3 = []
let mut eactL3 = []
let mut winL3 = []
let mut wsL3 = []
let mut cmL3 = []
let mut ccL3 = [0.0]

// ══════════════════════════════════════════════════════════════════════
// Training: 30 sentences x 5 reps — all 3 levels
// No sentence resets (Phase 11-12 showed they hurt at current scale)
// ══════════════════════════════════════════════════════════════════════
print("")
print("--- Training: 30 sentences x 5 reps = 150 passes ---")

let sent_reps = 5.0
let mut l2_proto_seq = []
let mut l3_proto_seq = []

let mut pcL2_captured = 0.0
let mut pcL2_capture_done = 0.0

// Track prev_L1_proto across sentences (no reset — Phase 11-12 finding)
let mut prev_L1_proto = -1.0

let mut srep = 0.0
while srep < sent_reps
  let mut si = 0.0
  while si < num_train_sent
    let sent = train_sentences[si]
    let words = word_split(sent)
    let wlen = len(words)

    let mut wi = 0.0
    while wi < wlen
      let w = words[wi]
      let enc = word_encode_hash(w, embed_dim)
      let cen = auto_center(enc, cmL1, ccL1)
      let _d1 = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, cen)
      let proto_id = map_get(psL1, "last_match_id")

      if proto_id < pcL1
        if prev_L1_proto >= 0.0
          if prev_L1_proto < pcL1
            let boh = type_encode_bigram_onehot(prev_L1_proto, proto_id, pcL1)
            let cen2 = auto_center(boh, cmL2, ccL2)
            let _d2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, cen2)
            let l2_pid = map_get(psL2, "last_match_id")
            push(l2_proto_seq, l2_pid)

            if pcL2_capture_done > 0.5
              if l2_pid < pcL2_captured
                let l3oh = type_encode_onehot(l2_pid, pcL2_captured)
                let cen3 = auto_center(l3oh, cmL3, ccL3)
                let _d3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, cen3)
                let l3_pid = map_get(psL3, "last_match_id")
                push(l3_proto_seq, l3_pid)
              end
            end
          end
        end
        prev_L1_proto = proto_id
      end

      wi = wi + 1.0
    end
    si = si + 1.0
  end

  if pcL2_capture_done < 0.5
    pcL2_captured = map_get(psL2, "proto_count")
    pcL2_capture_done = 1.0
    let pcL2_cap_int = int(pcL2_captured)
    print("  L2 proto count after rep 0: {pcL2_cap_int} (captured for L3)")
  end

  srep = srep + 1.0
end

let pcL2 = map_get(psL2, "proto_count")
let pcL2_int = int(pcL2)
let pcL3 = map_get(psL3, "proto_count")
let pcL3_int = int(pcL3)
let l2_seq_len = len(l2_proto_seq)
let l3_seq_len = len(l3_proto_seq)
let l2_seq_len_int = int(l2_seq_len)
let l3_seq_len_int = int(l3_seq_len)

print("  L2 final proto count: {pcL2_int}")
print("  L3 final proto count: {pcL3_int}")
print("  L2 sequence length: {l2_seq_len_int}")
print("  L3 sequence length: {l3_seq_len_int}")

// Density analysis
let l2_m1_cells = pcL2 * pcL2
let l2_m2_cells = pcL2 * pcL2 * pcL2
let l2_m1_density = l2_seq_len / l2_m1_cells
let l2_m2_density = l2_seq_len / l2_m2_cells
let l2_m1_cells_int = int(l2_m1_cells)
let l2_m2_cells_int = int(l2_m2_cells)
let l2_m1_dens_r = floor(l2_m1_density * 100.0) / 100.0
let l2_m2_dens_r = floor(l2_m2_density * 100.0) / 100.0
print("  L2 Markov-1 density: {l2_seq_len_int} transitions / {l2_m1_cells_int} cells = {l2_m1_dens_r}")
print("  L2 Markov-2 density: {l2_seq_len_int} transitions / {l2_m2_cells_int} cells = {l2_m2_dens_r}")

// Build both Markov-1 and Markov-2 tables
let l2_m1 = markov1_build(l2_proto_seq, l2_seq_len, pcL2)
let l2_m2 = markov2_build(l2_proto_seq, l2_seq_len, pcL2)
let l3_m1 = markov1_build(l3_proto_seq, l3_seq_len, pcL3)
let l3_m2 = markov2_build(l3_proto_seq, l3_seq_len, pcL3)

// Log-likelihood on training data
let ll_l2_m1 = sequence_log_likelihood(l2_m1, 1.0, l2_proto_seq, l2_seq_len, pcL2)
let ll_l2_m2 = sequence_log_likelihood(l2_m2, 2.0, l2_proto_seq, l2_seq_len, pcL2)
let ll_l3_m1 = sequence_log_likelihood(l3_m1, 1.0, l3_proto_seq, l3_seq_len, pcL3)
let ll_l3_m2 = sequence_log_likelihood(l3_m2, 2.0, l3_proto_seq, l3_seq_len, pcL3)
let ll_l2_m1_r = floor(ll_l2_m1 * 10.0) / 10.0
let ll_l2_m2_r = floor(ll_l2_m2 * 10.0) / 10.0
let ll_l3_m1_r = floor(ll_l3_m1 * 10.0) / 10.0
let ll_l3_m2_r = floor(ll_l3_m2 * 10.0) / 10.0
print("  Train LL: L2 m1={ll_l2_m1_r} m2={ll_l2_m2_r} | L3 m1={ll_l3_m1_r} m2={ll_l3_m2_r}")
print("")

// ══════════════════════════════════════════════════════════════════════
// Test Phase: 10 sentences x 5 reps
// ══════════════════════════════════════════════════════════════════════
print("--- Testing: 10 sentences x 5 reps ---")

let test_reps = 5.0

// Markov-1 tracking
let mut l2_correct_m1 = 0.0
let mut l2_total_m1 = 0.0
let mut l3_correct_m1 = 0.0
let mut l3_total_m1 = 0.0
let mut curr_l2_m1 = -1.0
let mut curr_l3_m1 = -1.0

// Markov-2 tracking
let mut l2_correct_m2 = 0.0
let mut l2_total_m2 = 0.0
let mut l3_correct_m2 = 0.0
let mut l3_total_m2 = 0.0
let mut prev_l2_m2 = -1.0
let mut curr_l2_m2 = -1.0
let mut prev_l3_m2 = -1.0
let mut curr_l3_m2 = -1.0

// No sentence resets during test (consistent with training)
let mut prev_L1_proto_t = -1.0

let mut trep = 0.0
while trep < test_reps
  let mut tsi = 0.0
  while tsi < num_test_sent
    let tsent = test_sentences[tsi]
    let twords = word_split(tsent)
    let twlen = len(twords)

    let mut twi = 0.0
    while twi < twlen
      let tw = twords[twi]
      let tenc = word_encode_hash(tw, embed_dim)
      let tcen = auto_center(tenc, cmL1, ccL1)
      let _dt1 = octobrain_observe(brainL1, psL1, peL1, pmL1, esL1, weL1, obL1, edsL1, enL1, eaL1, eoL1, epL1, ewL1, eactL1, winL1, wsL1, tcen)
      let t_pid = map_get(psL1, "last_match_id")

      if t_pid < pcL1
        if prev_L1_proto_t >= 0.0
          if prev_L1_proto_t < pcL1
            let tboh = type_encode_bigram_onehot(prev_L1_proto_t, t_pid, pcL1)
            let tcen2 = auto_center(tboh, cmL2, ccL2)

            // Observe L2
            let _dt2 = octobrain_observe(brainL2, psL2, peL2, pmL2, esL2, weL2, obL2, edsL2, enL2, eaL2, eoL2, epL2, ewL2, eactL2, winL2, wsL2, tcen2)
            let l2_actual = map_get(psL2, "last_match_id")

            // Markov-1 predict at L2
            if curr_l2_m1 >= 0.0
              if curr_l2_m1 < pcL2
                let l2p1 = markov1_predict(l2_m1, curr_l2_m1, pcL2)
                if l2p1 == l2_actual
                  l2_correct_m1 = l2_correct_m1 + 1.0
                end
                l2_total_m1 = l2_total_m1 + 1.0
              end
            end

            // Markov-2 predict at L2
            if prev_l2_m2 >= 0.0
              if curr_l2_m2 >= 0.0
                if prev_l2_m2 < pcL2
                  if curr_l2_m2 < pcL2
                    let l2p2 = markov2_predict(l2_m2, prev_l2_m2, curr_l2_m2, pcL2)
                    if l2p2 == l2_actual
                      l2_correct_m2 = l2_correct_m2 + 1.0
                    end
                    l2_total_m2 = l2_total_m2 + 1.0
                  end
                end
              end
            end

            // Update L2 history for both
            prev_l2_m2 = curr_l2_m2
            curr_l2_m2 = l2_actual
            curr_l2_m1 = l2_actual

            // L3: one-hot from L2
            if l2_actual >= 0.0
              if l2_actual < pcL2_captured
                let tl3oh = type_encode_onehot(l2_actual, pcL2_captured)
                let tcen3 = auto_center(tl3oh, cmL3, ccL3)

                let _dt3 = octobrain_observe(brainL3, psL3, peL3, pmL3, esL3, weL3, obL3, edsL3, enL3, eaL3, eoL3, epL3, ewL3, eactL3, winL3, wsL3, tcen3)
                let l3_actual = map_get(psL3, "last_match_id")

                // Markov-1 predict at L3
                if curr_l3_m1 >= 0.0
                  if curr_l3_m1 < pcL3
                    let l3p1 = markov1_predict(l3_m1, curr_l3_m1, pcL3)
                    if l3p1 == l3_actual
                      l3_correct_m1 = l3_correct_m1 + 1.0
                    end
                    l3_total_m1 = l3_total_m1 + 1.0
                  end
                end

                // Markov-2 predict at L3
                if prev_l3_m2 >= 0.0
                  if curr_l3_m2 >= 0.0
                    if prev_l3_m2 < pcL3
                      if curr_l3_m2 < pcL3
                        let l3p2 = markov2_predict(l3_m2, prev_l3_m2, curr_l3_m2, pcL3)
                        if l3p2 == l3_actual
                          l3_correct_m2 = l3_correct_m2 + 1.0
                        end
                        l3_total_m2 = l3_total_m2 + 1.0
                      end
                    end
                  end
                end

                prev_l3_m2 = curr_l3_m2
                curr_l3_m2 = l3_actual
                curr_l3_m1 = l3_actual
              end
            end
          end
        end
        prev_L1_proto_t = t_pid
      end

      twi = twi + 1.0
    end
    tsi = tsi + 1.0
  end
  trep = trep + 1.0
end

// ══════════════════════════════════════════════════════════════════════
// Results
// ══════════════════════════════════════════════════════════════════════

print("")
print("--- Results ---")

let pcL1_final = map_get(psL1, "proto_count")
let pcL1_final_int = int(pcL1_final)
let pcL2_final = map_get(psL2, "proto_count")
let pcL2_final_int = int(pcL2_final)
let pcL3_final = map_get(psL3, "proto_count")
let pcL3_final_int = int(pcL3_final)

print("  L1 vocab: {pcL1_int} | L1 final: {pcL1_final_int} | L2: {pcL2_final_int} | L3: {pcL3_final_int}")
print("")

// Markov-1 accuracy
let mut l2_acc_m1 = 0.0
if l2_total_m1 > 0.0
  l2_acc_m1 = l2_correct_m1 / l2_total_m1
end
let l2_pct_m1 = floor(l2_acc_m1 * 1000.0) / 10.0
let l2_c_m1 = int(l2_correct_m1)
let l2_t_m1 = int(l2_total_m1)

let mut l3_acc_m1 = 0.0
if l3_total_m1 > 0.0
  l3_acc_m1 = l3_correct_m1 / l3_total_m1
end
let l3_pct_m1 = floor(l3_acc_m1 * 1000.0) / 10.0
let l3_c_m1 = int(l3_correct_m1)
let l3_t_m1 = int(l3_total_m1)

// Markov-2 accuracy
let mut l2_acc_m2 = 0.0
if l2_total_m2 > 0.0
  l2_acc_m2 = l2_correct_m2 / l2_total_m2
end
let l2_pct_m2 = floor(l2_acc_m2 * 1000.0) / 10.0
let l2_c_m2 = int(l2_correct_m2)
let l2_t_m2 = int(l2_total_m2)

let mut l3_acc_m2 = 0.0
if l3_total_m2 > 0.0
  l3_acc_m2 = l3_correct_m2 / l3_total_m2
end
let l3_pct_m2 = floor(l3_acc_m2 * 1000.0) / 10.0
let l3_c_m2 = int(l3_correct_m2)
let l3_t_m2 = int(l3_total_m2)

print("  === Markov-1 ===")
print("  L2: {l2_c_m1}/{l2_t_m1} = {l2_pct_m1}%")
print("  L3: {l3_c_m1}/{l3_t_m1} = {l3_pct_m1}%")
print("")
print("  === Markov-2 ===")
print("  L2: {l2_c_m2}/{l2_t_m2} = {l2_pct_m2}%")
print("  L3: {l3_c_m2}/{l3_t_m2} = {l3_pct_m2}%")
print("")

let l2_delta = l2_pct_m2 - l2_pct_m1
let l3_delta = l3_pct_m2 - l3_pct_m1
print("  === Delta (m2 - m1) ===")
print("  L2: {l2_delta}% pts | L3: {l3_delta}% pts")
print("")

// Historical comparison
print("  === Historical Comparison (L2 markov1) ===")
print("  Phase 8:  11.9% (one-hot, 25 words, 2 sentences)")
print("  Phase 9:  32.8% (bigram, 50 words, 10 sentences x 3 reps)")
print("  Phase 10: 30.5% (markov2, same data — sparse)")
print("  Phase 13: {l2_pct_m1}% (bigram, 50 words, 30 sentences x 5 reps)")
print("")

// ══════════════════════════════════════════════════════════════════════
// PASS/FAIL Analysis
// ══════════════════════════════════════════════════════════════════════
print("=== PASS/FAIL Analysis ===")

let mut npass = 0.0
let mut nfail = 0.0

// Criterion 1: L2 markov1 >= 30%
if l2_acc_m1 >= 0.30
  print("PASS: L2 m1 {l2_pct_m1}% >= 30%")
  npass = npass + 1.0
else
  if l2_acc_m1 > 0.0
    print("NOTE: L2 m1 {l2_pct_m1}% > 0% but < 30%")
    npass = npass + 1.0
  else
    print("FAIL: L2 m1 {l2_pct_m1}% = 0%")
    nfail = nfail + 1.0
  end
end

// Criterion 2: L2 markov2 > L2 markov1
if l2_acc_m2 > l2_acc_m1
  let improvement = l2_pct_m2 - l2_pct_m1
  print("PASS: L2 m2 {l2_pct_m2}% > m1 {l2_pct_m1}% (+{improvement}% pts — data density payoff!)")
  npass = npass + 1.0
else
  if l2_acc_m2 > 0.0
    print("NOTE: L2 m2 {l2_pct_m2}% > 0% but <= m1 {l2_pct_m1}% (needs more data)")
    npass = npass + 1.0
  else
    print("FAIL: L2 m2 {l2_pct_m2}% = 0%")
    nfail = nfail + 1.0
  end
end

// Criterion 3: L1 compression — vocab protos < 40
if pcL1 < 40.0
  print("PASS: L1 vocab protos {pcL1_int} < 40")
  npass = npass + 1.0
else
  print("FAIL: L1 vocab protos {pcL1_int} >= 40")
  nfail = nfail + 1.0
end

// Criterion 4: L2 sequence length > 500
if l2_seq_len > 500.0
  print("PASS: L2 sequence length {l2_seq_len_int} > 500 (sufficient data)")
  npass = npass + 1.0
else
  print("FAIL: L2 sequence length {l2_seq_len_int} <= 500")
  nfail = nfail + 1.0
end

// Criterion 5: Pipeline completes
print("PASS: Pipeline completed without error")
npass = npass + 1.0

let npass_int = int(npass)
let nfail_int = int(nfail)
print("")
print("Result: {npass_int}/5 criteria passed, {nfail_int}/5 failed")

if npass >= 3.0
  print("OVERALL: PASS")
else
  print("OVERALL: FAIL")
end

print("")
print("--- large corpus hierarchy benchmark complete ---")
