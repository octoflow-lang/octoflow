// OctoBrain Word-Level Text Processing Module
// Tokenizes text into words and encodes as numeric vectors.
// Captures phrase-level structure that character N-grams miss.

use "vecmath"

// ── word_split ──────────────────────────────────────────────────
// Split text by spaces into array of word strings.
// "the cat sat" → ["the", "cat", "sat"]
// Consecutive spaces produce empty strings (filtered out).
fn word_split(text)
  let n = len(text)
  let mut words = []
  let mut current = ""
  let mut i = 0.0
  while i < n
    let ch = char_at(text, i)
    if ch == " "
      if len(current) > 0.0
        push(words, current)
        current = ""
      end
    else
      current = current + ch
    end
    i = i + 1.0
  end
  // Don't forget the last word if text doesn't end with space
  if len(current) > 0.0
    push(words, current)
  end
  return words
end

// ── word_vocab_build ────────────────────────────────────────────
// Build vocabulary from array of words. Returns a map: word → ID (float).
// Each unique word gets an incrementing ID starting from 0.
// Also sets "vocab_size" key in the returned map.
fn word_vocab_build(words)
  let n = len(words)
  let mut vocab = map()
  let mut next_id = 0.0
  let mut i = 0.0
  while i < n
    let word = words[i]
    if map_has(vocab, word) == 0.0
      map_set(vocab, word, next_id)
      next_id = next_id + 1.0
    end
    i = i + 1.0
  end
  map_set(vocab, "vocab_size", next_id)
  return vocab
end

// ── word_encode_hash ────────────────────────────────────────────
// Encode a word as a fixed-dimension numeric vector using character hash.
// For each character in word: hash to dimension index, increment that slot.
// Returns array of length embed_dim.
// Hash function: dim_index = floor(ord(char) * 7.13) mod embed_dim
fn word_encode_hash(word, embed_dim)
  let mut vec = []
  let mut di = 0.0
  while di < embed_dim
    push(vec, 0.0)
    di = di + 1.0
  end

  let n = len(word)
  let mut i = 0.0
  while i < n
    let ch = char_at(word, i)
    let code = ord(ch)
    let raw_idx = code * 7.13
    let dim_idx = raw_idx - floor(raw_idx / embed_dim) * embed_dim
    let idx = floor(dim_idx)
    vec[idx] = vec[idx] + 1.0
    i = i + 1.0
  end

  return vec
end

// ── word_clean ──────────────────────────────────────────────────
// Clean a sentence for NLP: lowercase + strip common punctuation.
// "The cat, sat on a MAT." → "the cat sat on a mat"
fn word_clean(text)
  let mut t = to_lower(text)
  t = replace(t, ".", "")
  t = replace(t, ",", "")
  t = replace(t, "!", "")
  t = replace(t, "?", "")
  t = replace(t, ";", "")
  t = replace(t, ":", "")
  t = replace(t, "'", "")
  t = replace(t, "(", "")
  t = replace(t, ")", "")
  t = replace(t, "-", " ")
  return t
end

// ── word_clean_split ────────────────────────────────────────────
// Clean + split in one call. Returns array of lowercase words.
fn word_clean_split(text)
  let cleaned = word_clean(text)
  return word_split(cleaned)
end

// ── word_clean_split_fast ────────────────────────────────────────
// Clean + split using Rust-native split() builtin. O(n) per line.
// Drop-in replacement for word_clean_split — same output, ~40x faster.
fn word_clean_split_fast(text)
  let cleaned = word_clean(text)
  let parts = split(cleaned, " ")
  let mut words = []
  let n = len(parts)
  let mut i = 0.0
  while i < n
    if len(parts[i]) > 0.0
      push(words, parts[i])
    end
    i = i + 1.0
  end
  return words
end

// ── word_bigram_vector ──────────────────────────────────────────
// Create a feature vector from two consecutive words (word bigram).
// Concatenates the hash encodings of both words.
// Returns array of length embed_dim * 2.
fn word_bigram_vector(word1, word2, embed_dim)
  let v1 = word_encode_hash(word1, embed_dim)
  let v2 = word_encode_hash(word2, embed_dim)
  let mut result = []
  let mut i = 0.0
  while i < embed_dim
    push(result, v1[i])
    i = i + 1.0
  end
  i = 0.0
  while i < embed_dim
    push(result, v2[i])
    i = i + 1.0
  end
  return result
end
