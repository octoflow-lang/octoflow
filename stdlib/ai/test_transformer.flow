// test_transformer.flow — Transformer Layer Tests
// Run from C:\FlowGPU: octoflow run stdlib/ai/test_transformer.flow --allow-read --allow-write --allow-exec --allow-ffi

use "transformer"

let mut counters = [0.0, 0.0]

fn check(counters, label, got, expected)
  if got == expected
    counters[0] = counters[0] + 1.0
    return 1.0
  end
  counters[1] = counters[1] + 1.0
  print("  FAIL: {label} — got {got}, expected {expected}")
  return 0.0
end

fn check_near(counters, label, got, expected, tol)
  let d = abs(got - expected)
  if d < tol
    counters[0] = counters[0] + 1.0
    return 1.0
  end
  counters[1] = counters[1] + 1.0
  print("  FAIL: {label} — got {got}, expected {expected}, diff={d}")
  return 0.0
end

print("=== TRANSFORMER LAYER TESTS ===")
print(" ")

// ── Test 1: RMSNorm CPU ─────────────────────────────────────────
print("--- RMSNorm ---")

let mut hidden = []
push(hidden, 1.0)
push(hidden, 2.0)
push(hidden, 3.0)
push(hidden, 4.0)

let mut weight = []
push(weight, 1.0)
push(weight, 1.0)
push(weight, 1.0)
push(weight, 1.0)

let normed = rmsnorm_cpu(hidden, weight)

// RMS of [1,2,3,4]: sqrt((1+4+9+16)/4) = sqrt(7.5) ≈ 2.738
// Normalized: [1/2.738, 2/2.738, 3/2.738, 4/2.738]
let _c = check(counters, "rmsnorm len", len(normed), 4.0)
let _c = check_near(counters, "rmsnorm[0]", normed[0], 0.365, 0.01)
let _c = check_near(counters, "rmsnorm[3]", normed[3], 1.460, 0.01)

// ── Test 2: SiLU activation ─────────────────────────────────────
print("--- SiLU ---")

let s0 = silu_cpu(0.0)
let _c = check_near(counters, "silu(0)", s0, 0.0, 0.01)

let s1 = silu_cpu(1.0)
// SiLU(1) ≈ 0.731 (1 * sigmoid(1))
let _c = check_near(counters, "silu(1)", s1, 0.731, 0.1)

let s_neg = silu_cpu(-1.0)
// SiLU(-1) ≈ -0.269
let _c = check_near(counters, "silu(-1)", s_neg, -0.269, 0.1)

// ── Test 3: Transformer layer structure ────────────────────────
print("--- Transformer layer MVP ---")

let mut model = map()
map_set(model, "n_embd", 4.0)
map_set(model, "n_head", 2.0)
map_set(model, "n_layer", 1.0)

let mut h_in = []
push(h_in, 1.0)
push(h_in, 2.0)
push(h_in, 3.0)
push(h_in, 4.0)

let h_out = transformer_layer_mvp(h_in, 0.0, model)

// MVP does: RMSNorm → identity attention → residual → RMSNorm → identity FFN → residual
// With identity ops, output should be close to input (with some RMSNorm effect)
let _c = check(counters, "layer output len", len(h_out), 4.0)

// ── SUMMARY ─────────────────────────────────────────────────────
let pass = counters[0]
let fail = counters[1]
let total = pass + fail
print(" ")
print("=== TRANSFORMER TEST SUMMARY ===")
print("  pass: {pass}/{total}")
print("  fail: {fail}")
if fail == 0.0
  print("  ALL PASS (CPU placeholders)")
else
  print("  FAILURES DETECTED")
end

print(" ")
print("NOTE: MVP uses CPU placeholders for RMSNorm/SiLU/Attention/FFN.")
print("GPU kernels will replace these in full implementation.")
