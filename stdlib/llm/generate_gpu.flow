// stdlib/llm/generate_gpu.flow — GPU-Accelerated Text Generation
//
// Uses gguf_infer_layer (compiled Rust + GPU matvec) for transformer layers.
// GPU dispatches matrix-vector products; Rust handles norm/attention/activation.
// KV cache managed in Rust thread-local (fast, zero .flow overhead).
//
// Architecture:
//   gguf_matvec → gpu_cached_matvec → dispatch_resident_pc(matvec.spv)
//   Weights cached in GPU_BUFFER_CACHE (VRAM) — upload once, reuse across tokens.
//   Intermediate ops (rmsnorm, rope, silu, attention) in compiled Rust.
//
// Usage:
//   use "generate_gpu"
//   let _r = run_generate_gpu("path/to/model.gguf", "Hello world")
//
// Or standalone:
//   octoflow run stdlib/llm/run_generate_gpu.flow --allow-read --allow-ffi

use "gguf"
use "ops"
use "sampling"
use "chat"

fn run_generate_gpu(model_path, prompt)
  // ── Configuration ──────────────────────────────────────────────
  let max_tokens = 50.0
  let max_seq = 64.0
  let strategy = "top_p"
  let temperature = 0.8
  let top_k = 40.0
  let top_p = 0.9

  print("=== OctoFlow GPU Generation ===")
  print("Loading model metadata...")

  let model = gguf_load_from_file(model_path)

  let n_embd = map_get(model, "n_embd")
  let n_head = map_get(model, "n_head")
  let mut n_kv_head = map_get(model, "n_kv_head")
  let n_ff = map_get(model, "n_ff")
  let n_layer = map_get(model, "n_layer")
  let vocab_size = map_get(model, "vocab_size")
  let eps = gguf_meta_default(model, "attention.layer_norm_rms_epsilon", 0.00001)

  if n_kv_head == 0.0
    n_kv_head = n_head
  end

  let head_dim = n_embd / n_head

  let arch = map_get(model, "arch")
  print("  {arch} -- {n_layer} layers, {n_embd} dim, {vocab_size} vocab")
  print("  Inference: Rust + GPU matvec (dispatch chain)")

  // ── Load vocabulary ────────────────────────────────────────────
  print("Loading vocabulary...")
  let vocab = gguf_load_vocab(model_path)

  // ── Build adaptive chat template ──────────────────────────────
  print("Building chat template...")
  let chat_tokens = resolve_chat_tokens(model_path, model, vocab)
  let prompt_ids = build_chat_tokens(model_path, model, vocab, prompt)
  let stop_tokens = get_stop_tokens(model, chat_tokens)

  let n_prompt = len(prompt_ids)
  print("  Prompt: {prompt}")
  print("  Template tokens: {n_prompt}")

  // ── Load persistent weights ────────────────────────────────────
  print("Loading output norm...")
  let out_norm_w = gguf_load_tensor(model_path, model, "output_norm.weight")

  // KV cache: managed by gguf_infer_layer in Rust thread-local
  print("  KV cache: Rust-managed (max_seq={max_seq})")

  // ── Main inference loop ────────────────────────────────────────
  let total_steps = n_prompt + max_tokens
  print(" ")
  print("--- Prefilling {n_prompt} tokens + generating {max_tokens} tokens ---")

  let mut current_token = prompt_ids[0]
  let mut generated_text = " "
  let mut seq_pos = 0.0
  let mut gen_count = 0.0
  let mut done = 0.0

  while seq_pos < total_steps
    if done == 1.0
      seq_pos = total_steps
    end
    if seq_pos >= max_seq
      print("  (max sequence length reached)")
      seq_pos = total_steps
    end
    if seq_pos < total_steps

    // 1. Load embedding for current token
    let tok_emb = gguf_load_tensor(model_path, model, "token_embd.weight", current_token)
    let mut hidden = []
    let mut ei = 0.0
    while ei < n_embd
      push(hidden, tok_emb[int(ei)])
      ei = ei + 1.0
    end

    // 2. Run transformer layers (compiled Rust + GPU matvec)
    let mut layer_idx = 0.0
    while layer_idx < n_layer
      let layer_out = gguf_infer_layer(model_path, model, hidden, layer_idx, seq_pos, max_seq)

      // Copy layer output back to hidden
      let mut hi = 0.0
      while hi < n_embd
        hidden[int(hi)] = layer_out[int(hi)]
        hi = hi + 1.0
      end

      // Evict TENSOR_CACHE only (system RAM). GPU_BUFFER_CACHE stays across tokens.
      // Layers that fit in VRAM: instant on subsequent tokens (weights stay resident).
      // Layers that overflow VRAM: re-dequant from disk each token.
      if layer_idx >= 1.0
        let evict_idx = layer_idx - 1.0
        let _ev = gguf_evict_layer_ram(model_path, model, evict_idx)
      end

      layer_idx = layer_idx + 1.0
    end

    // 3. Post-transformer: prefill vs generation
    let prefill_end = n_prompt - 1.0
    if seq_pos < prefill_end
      let npos = seq_pos + 1.0
      current_token = prompt_ids[int(npos)]
      let ptext = vocab[int(current_token)]
      print("  [prefill {npos}/{n_prompt}] token={current_token} text={ptext}")
    else
      // Compute logits: output norm + lm_head projection
      let normed_out = rmsnorm_cpu(hidden, out_norm_w, n_embd, eps)
      let logits = gguf_matvec(model_path, model, "token_embd.weight", normed_out)

      let mut sampled_idx = 0.0
      if strategy == "greedy"
        sampled_idx = sample_greedy(logits)
      elif strategy == "top_k"
        sampled_idx = sample_top_k(logits, top_k, temperature)
      elif strategy == "top_p"
        sampled_idx = sample_top_p(logits, top_p, temperature)
      elif strategy == "min_p"
        sampled_idx = sample_min_p(logits, 0.05, temperature)
      else
        sampled_idx = sample_temperature(logits, temperature)
      end
      let sampled_logit = logits[int(sampled_idx)]

      let token_text = vocab[int(sampled_idx)]
      generated_text = generated_text + token_text
      gen_count = gen_count + 1.0
      print("  [gen {gen_count}] pos={seq_pos} token={sampled_idx} logit={sampled_logit} text={token_text}")

      if is_stop_token(stop_tokens, sampled_idx) == 1.0
        print("  (stop token -- stopping)")
        done = 1.0
      end
      if gen_count >= max_tokens
        done = 1.0
      end

      current_token = sampled_idx
    end

    end
    seq_pos = seq_pos + 1.0
  end

  print("---")
  print("Prompt: {prompt}")
  print("Raw: {generated_text}")
  let decoded = bpe_decode(generated_text)
  print(" ")
  print("=== Output ===")
  print("{decoded}")
  return 0.0
end
