// stdlib/llm/transformer.flow — GPU Transformer Layer
//
// RMSNorm -> Attention -> Residual -> RMSNorm -> FFN -> Residual
//
// transformer_layer(hidden, attn_norm_w, wq, wk, wv, wo, ffn_norm_w,
//                   w_gate, w_up, w_down, pos, n_head, n_kv_head,
//                   n_embd, n_ff, eps)
//
// All weight arrays passed explicitly (no map — OctoFlow maps can't store arrays).
// eps parameter for adaptive RMSNorm (read from GGUF metadata).
// n_kv_head for GQA support (n_kv_head < n_head).

use "ops"
use "../gpu/runtime"
use "kernels/attention"
use "kernels/ffn"

fn transformer_layer(hidden, attn_norm_w, wq, wk, wv, wo, ffn_norm_w, w_gate, w_up, w_down, pos, n_head, n_kv_head, n_embd, n_ff, eps)
  // 1. Pre-attention RMSNorm (CPU — adaptive eps)
  let normed_attn = rmsnorm_cpu(hidden, attn_norm_w, n_embd, eps)

  // 2. Self-Attention (GPU — adaptive GQA)
  let attn_out = gpu_attention(normed_attn, wq, wk, wv, wo, pos, n_head, n_kv_head, n_embd)

  // 3. Residual connection
  let mut h_post_attn = []
  let mut j = 0.0
  while j < n_embd
    push(h_post_attn, hidden[int(j)] + attn_out[int(j)])
    j = j + 1.0
  end

  // 4. Pre-FFN RMSNorm (CPU — adaptive eps)
  let normed_ffn = rmsnorm_cpu(h_post_attn, ffn_norm_w, n_embd, eps)

  // 5. FFN (GPU — SwiGLU)
  let ffn_out = gpu_ffn(normed_ffn, w_gate, w_up, w_down, n_embd, n_ff)

  // 6. Residual connection
  let mut h_final = []
  let mut k = 0.0
  while k < n_embd
    push(h_final, h_post_attn[int(k)] + ffn_out[int(k)])
    k = k + 1.0
  end

  return h_final
end
