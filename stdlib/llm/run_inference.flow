// Run standalone inference
// Usage: octoflow run stdlib/llm/run_inference.flow --allow-read --allow-ffi

use "inference"

let model_path = "G:/ollama_models/blobs/sha256-29d8c98fa6b098e200069bfb88b9508dc3e85586d20cba59f8dda9a808165104"
let _r = run_inference(model_path)
