// stdlib/ai/tokenizer.flow — BPE Tokenizer for LLM Inference
//
// Byte Pair Encoding (BPE) tokenizer used by modern LLMs.
// Supports Qwen, Llama, Mistral tokenizer formats.
//
// Functions:
//   tokenizer_load(vocab_file) → vocab_map
//     Load vocabulary from JSON file (tokenizer.json format)
//
//   tokenizer_encode(text, vocab) → token_ids[]
//     Encode text to token IDs using greedy longest match
//
//   tokenizer_decode(token_ids, vocab) → text
//     Decode token IDs back to text
//
// Requirements: --allow-read
// Usage:
//   use "stdlib/ai/tokenizer"
//   let vocab = tokenizer_load("tokenizer.json")
//   let tokens = tokenizer_encode("Hello world", vocab)
//   let decoded = tokenizer_decode(tokens, vocab)

// ── Tokenizer Loading ───────────────────────────────────────────────

fn tokenizer_load(vocab_file)
  // Load tokenizer.json (HuggingFace format)
  // For MVP: simplified vocab-only loader
  // Full tokenizer.json has added_tokens, normalizer, pre_tokenizer, etc.

  let vocab_json = read_file(vocab_file)
  let vocab_data = json_parse(vocab_json)

  // Extract vocab from model.vocab or vocab key
  let vocab_map = map_get(vocab_data, "model")
  if vocab_map == 0.0
    vocab_map = vocab_data
  end

  return vocab_map
end

// ── Simplified Vocab Builder (for testing without tokenizer.json) ──

fn tokenizer_build_simple()
  // Build minimal vocab for testing: common tokens + special tokens
  let mut vocab = map()

  // Special tokens
  map_set(vocab, "<unk>", 0.0)
  map_set(vocab, "<s>", 1.0)
  map_set(vocab, "</s>", 2.0)

  // Common single-char tokens (ASCII 32-126)
  let mut i = 32.0
  while i < 127.0
    let c = chr(i)
    map_set(vocab, c, i)
    i = i + 1.0
  end

  // Common words
  map_set(vocab, "the", 200.0)
  map_set(vocab, "and", 201.0)
  map_set(vocab, "is", 202.0)
  map_set(vocab, "of", 203.0)
  map_set(vocab, "to", 204.0)
  map_set(vocab, "Hello", 250.0)
  map_set(vocab, "world", 251.0)

  return vocab
end

// ── Encoding: Text → Token IDs ─────────────────────────────────────

fn tokenizer_encode(text, vocab)
  let n = len(text)
  let mut tokens = []
  let mut pos = 0.0

  while pos < n
    // Greedy longest match
    let mut match_len = 0.0
    let mut match_id = 0.0

    // Try substrings from current position, longest first
    let max_len = n - pos
    if max_len > 32.0
      max_len = 32.0
    end

    let mut try_len = max_len
    while try_len > 0.0
      let candidate = substr(text, pos, try_len)
      if map_has(vocab, candidate)
        match_len = try_len
        match_id = map_get(vocab, candidate)
        try_len = 0.0
      else
        try_len = try_len - 1.0
      end
    end

    if match_len == 0.0
      // No match found - use single char or <unk>
      let c = char_at(text, pos)
      if map_has(vocab, c)
        match_id = map_get(vocab, c)
      else
        match_id = 0.0
      end
      match_len = 1.0
    end

    push(tokens, match_id)
    pos = pos + match_len
  end

  return tokens
end

// ── Decoding: Token IDs → Text ─────────────────────────────────────

fn tokenizer_decode(token_ids, vocab)
  // Build reverse vocab: id → token_str
  let mut id_to_token = map()

  // Extract all keys from vocab
  let keys_str = map_keys(vocab)
  let keys = split(keys_str, ",")

  let mut i = 0.0
  while i < len(keys)
    let token_str = keys[int(i)]
    // Skip empty strings from split
    if len(token_str) > 0.0
      let token_id = map_get(vocab, token_str)
      let id_str = str(token_id)
      map_set(id_to_token, id_str, token_str)
    end
    i = i + 1.0
  end

  // Decode each token ID
  let mut result = ""
  let mut j = 0.0
  while j < len(token_ids)
    let tid = token_ids[int(j)]
    let tid_str = str(tid)
    if map_has(id_to_token, tid_str)
      let token_text = map_get(id_to_token, tid_str)
      result = result + token_text
    else
      result = result + "<unk>"
    end
    j = j + 1.0
  end

  return result
end

// ── Load vocab from GGUF model ──────────────────────────────────────

fn tokenizer_load_from_gguf(model)
  // Extract tokenizer vocab from GGUF metadata
  // GGUF stores vocab as array of strings in metadata
  // Key: "tokenizer.ggml.tokens" (array of strings)

  let mut vocab = map()

  // For now, return simple vocab
  // Full GGUF tokenizer extraction needs array-of-strings support
  // which requires enhanced metadata parser

  print("WARNING: tokenizer_load_from_gguf not fully implemented")
  print("Using simplified vocab for testing")

  return tokenizer_build_simple()
end
