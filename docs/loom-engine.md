# The Loom Engine

> Weaving threads of parallel computation.

```
        ğŸ™
      â•±â•±â•²â•²â•²â•²
     â•±â•±  â•²â•²â•²â•²        OctoFlow's Loom Engine
    â”‚â”‚ â—‰â—‰ â”‚â”‚â”‚â”‚       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â•²â•²    â•±â•±â•±â•±       GPU compute runtime
     â•²â•²  â•±â•±â•±â•±        for the rest of us
      â•°â•±â•²â•±â•²â•¯
     â•±â”‚â”‚â”‚â”‚â”‚â•²
    â•± â”‚â•±â•²â”‚â•² â•²         Eight arms.
   â•±  â•±  â•²  â•²        Thousands of threads.
  â•±  â•± â—‡â—‡ â•²  â•²       One loom.
```

---

## What is Loom?

Loom is OctoFlow's GPU compute engine. It weaves thousands of parallel threads
into coordinated computation â€” sieving primes, multiplying matrices, reducing
datasets, training models â€” all from a high-level language, on any GPU.

No CUDA. No driver lock-in. Just write `.flow`, and the loom weaves.

```flow
use stdlib.loom.sieve
let count = gpu_prime_count(1000000000.0)
print("Primes below one billion: {count:.0}")
// â†’ 50,847,534 (exact, verified)
```

Three lines. 50 million primes. Under 3 seconds on a mid-range GPU.

---

## How It Works

### The Weaving Metaphor

A textile loom takes many threads and weaves them into fabric following a pattern.
OctoFlow's Loom does the same:

| Textile Loom | OctoFlow Loom |
|---|---|
| Threads | GPU threads (thousands) |
| Pattern | Dispatch chain (pre-recorded kernel sequence) |
| Shuttle | Push constants (data passed to each kernel) |
| Fabric | Computed result |
| Loom frame | Compute unit (GPU memory + command buffer) |

The key insight: **the entire pattern is set up before weaving begins.** In Loom,
you record all your kernel dispatches into a chain, compile it once, then launch.
The GPU executes the entire chain with zero CPU interruption.

This is why Loom can execute 95,000+ kernel dispatches in a single submission â€”
the pattern is pre-woven, the GPU just runs it.

### Three Tiers

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tier 1: Patterns                                   â”‚
â”‚  gpu_sum(data)  Â·  gpu_sort(data)  Â·  gpu_sieve(N)  â”‚
â”‚  One function call. The loom handles everything.     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tier 2: Expert                                     â”‚
â”‚  loom_boot â†’ loom_dispatch â†’ loom_build â†’ loom_run  â”‚
â”‚  Custom dispatch chains. You control the pattern.    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tier 3: IR                                         â”‚
â”‚  ir_begin â†’ ir_fmul â†’ ir_barrier â†’ ir_finalize      â”‚
â”‚  Write custom SPIR-V kernels in .flow code.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Most users stay at Tier 1. Power users drop to Tier 2 for custom pipelines.
Kernel authors use Tier 3 to emit new SPIR-V compute shaders â€” from OctoFlow
itself, not from C++ or GLSL.

---

## Quick Start

### Tier 1: One-Call Patterns

```flow
// Parallel reduction
use stdlib.loom.reduce
let total = gpu_sum(data)
let biggest = gpu_max(data)

// Parallel sieve
use stdlib.loom.sieve
let primes = gpu_prime_count(10000000000.0)   // Ï€(10^10), exact

// Parallel map
use stdlib.loom.map
let doubled = gpu_map(data, "mul", 2.0)
let roots = gpu_map(data, "sqrt")

// Matrix multiply
use stdlib.loom.matmul
let C = gpu_matmul(A, B, rows_a, cols_b, cols_a)
```

### Tier 2: Custom Dispatch Chains

```flow
// Boot a compute unit
let unit = loom_boot(1.0, 8194, 4096)

// Upload data
loom_write(unit, 0.0, my_primes)

// Record kernel dispatches (the "pattern")
loom_dispatch(unit, "stdlib/loom/kernels/sieve_init.spv", [seg, words, n_lo, n_hi], 32.0)
loom_dispatch(unit, "stdlib/loom/kernels/sieve_mark.spv", [seg, words, 0, 53], 32.0)
loom_dispatch(unit, "stdlib/loom/kernels/sieve_count.spv", [words, count_off], 32.0)

// Compile and launch
let prog = loom_build(unit)
loom_launch(prog)           // async â€” returns immediately

// Poll for completion
while loom_poll(prog) < 0.5
end

// Read results
let result = loom_read(unit, 0.0, 0.0, 8194)
```

### Tier 3: Custom SPIR-V Kernels via IR

```flow
use stdlib.loom.ir

// Build a GPU kernel that doubles every element
let mut prog = ir_begin()
let entry = ir_entry(prog)
let body = ir_block(prog)

let gid = ir_global_id(entry)
let val = ir_buf_load(body, 0.0, gid)
let doubled = ir_fmul(body, val, ir_const(body, 2.0))
ir_buf_store(body, 0.0, gid, doubled)

let kernel = ir_finalize(prog)

// Dispatch the custom kernel
let unit = loom_boot(1.0, 1024, 0)
loom_dispatch_jit(unit, kernel, [], 4.0)
let p = loom_build(unit)
loom_run(p)
let result = loom_read(unit, 0.0, 0.0, 1024)
```

You just wrote a GPU compute shader in OctoFlow. No GLSL, no HLSL, no CUDA.
The IR builder emits valid SPIR-V binary that runs on any Vulkan GPU.

---

## API Reference

### Core Functions

| Function | Purpose |
|---|---|
| `loom_boot(bind, reg_size, globals)` | Create a compute unit with register and globals buffers |
| `loom_write(unit, offset, data)` | Upload an array to the unit's globals buffer |
| `loom_dispatch(unit, kernel, params, wg)` | Record a kernel dispatch into the unit's chain |
| `loom_dispatch_jit(unit, ir, params, wg)` | Record a JIT-compiled kernel dispatch |
| `loom_build(unit)` | Compile the dispatch chain into a Vulkan command buffer |
| `loom_run(prog)` | Execute synchronously (blocks until complete) |
| `loom_launch(prog)` | Execute asynchronously (returns immediately) |
| `loom_poll(prog)` | Check if async execution has completed (1.0 = done) |
| `loom_read(unit, bind, off, len)` | Read results back from GPU memory |

### Pattern Functions

| Function | Pattern | Description |
|---|---|---|
| `gpu_sum(data)` | Reduce | Sum all elements |
| `gpu_min(data)` | Reduce | Find minimum element |
| `gpu_max(data)` | Reduce | Find maximum element |
| `gpu_map(data, op, ...)` | Map | Apply operation to every element |
| `gpu_sort(data)` | Sort | Parallel radix sort |
| `gpu_scan(data)` | Scan | Prefix sum (inclusive) |
| `gpu_prime_count(N)` | Sieve | Count primes below N (exact) |
| `gpu_matmul(A, B, m, n, k)` | MatMul | Matrix multiply: A is mÃ—k, B is kÃ—n, result is mÃ—n |

### IR Builder Functions (Tier 3)

| Function | SPIR-V | Purpose |
|---|---|---|
| `ir_begin()` | â€” | Start a new kernel program |
| `ir_entry(prog)` | OpFunction | Create entry point |
| `ir_block(prog)` | OpLabel | Create a basic block |
| `ir_global_id(block)` | BuiltIn GlobalInvocationId | Get thread ID |
| `ir_const(block, val)` | OpConstant | Float constant |
| `ir_const_u(block, val)` | OpConstant | Uint32 constant |
| `ir_buf_load(block, bind, idx)` | OpAccessChain + OpLoad | Load from buffer |
| `ir_buf_store(block, bind, idx, val)` | OpAccessChain + OpStore | Store to buffer |
| `ir_fadd`, `ir_fsub`, `ir_fmul`, `ir_fdiv` | OpFAdd/Sub/Mul/Div | Float arithmetic |
| `ir_iadd`, `ir_isub`, `ir_imul` | OpIAdd/Sub/Mul | Integer arithmetic |
| `ir_shl`, `ir_shr`, `ir_not` | OpShift/OpNot | Bitwise operations |
| `ir_bitcount(block, val)` | OpBitCount | Hardware popcount |
| `ir_buf_atomic_and(block, bind, idx, mask)` | OpAtomicAnd | Atomic bit-clear |
| `ir_barrier(block)` | OpControlBarrier | Workgroup sync |
| `ir_shared_load`, `ir_shared_store` | Workgroup memory | Shared memory ops |
| `ir_u32_to_u64`, `ir_u64_to_u32` | OpUConvert | 64-bit widening/narrowing |
| `ir_imul64`, `ir_iadd64`, `ir_udiv64` | 64-bit OpIMul/IAdd/UDiv | 64-bit arithmetic |
| `ir_finalize(prog)` | â€” | Emit SPIR-V binary |

---

## Architecture

### The Dispatch Chain Model

```
Record Phase                    Execute Phase
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
loom_boot()         â”€â”
loom_write()         â”‚
loom_dispatch() Ã—N   â”‚â”€â”€ chain â”€â”€â–¶  loom_build() â”€â”€â–¶ loom_launch()
loom_dispatch() Ã—N   â”‚                                    â”‚
loom_dispatch() Ã—N  â”€â”˜                                    â–¼
                                                    GPU executes
                                                    entire chain
                                                    (zero CPU trips)
                                                          â”‚
                                                          â–¼
                                                    loom_read()
```

All kernel dispatches are **pre-recorded** into a chain. The chain is compiled
into a single Vulkan command buffer and submitted once. The GPU executes every
dispatch back-to-back without returning to the CPU.

At scale, this matters enormously. OctoFlow's prime sieve at 10^10 runs
95,370 dispatches in a single submission. CUDA frameworks need a CPU round-trip
for each kernel launch (~5-20us each). Loom's chains eliminate that overhead
entirely.

### Async VM Swarm

```
       â”Œâ”€â”€ Unit #0  â”€â”€â”€ chain of 5,960 dispatches â”€â”€â–¶ GPU
       â”œâ”€â”€ Unit #1  â”€â”€â”€ chain of 5,960 dispatches â”€â”€â–¶ GPU
       â”œâ”€â”€ Unit #2  â”€â”€â”€ chain of 5,960 dispatches â”€â”€â–¶ GPU
CPU â”€â”€â”€â”¤   ...                                        â•²
       â”œâ”€â”€ Unit #14 â”€â”€â”€ chain of 5,960 dispatches â”€â”€â–¶  â–¶ parallel
       â””â”€â”€ Unit #15 â”€â”€â”€ chain of 5,960 dispatches â”€â”€â–¶ GPU
```

Multiple compute units run simultaneously via `loom_launch()`. Each unit has
its own register buffer and dispatch chain. The CPU boots all units, records
all chains, then launches everything â€” no coordination overhead during execution.

### Memory Layout (per unit)

```
Registers (B0):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0 .. NUM_WORDS-1]          Bitmap / work buffer â”‚
â”‚ [NUM_WORDS]                 Count scratch        â”‚
â”‚ [NUM_WORDS+1]               Accumulator          â”‚
â”‚ [NUM_WORDS+2 .. +NP]        Carry-forward state  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Globals (read-only, shared input):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [0 .. num_primes-1]         Input data (primes)  â”‚
â”‚ [num_primes .. pad]         Padding to 256       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Registers are read-write (GPU workspace). Globals are uploaded once and read
by all dispatches. Push constants (up to 5 floats) carry per-dispatch parameters
like segment index and array bounds.

---

## The Loom Engine

> **The Loom Engine IS Main Loom + Support Loom. Neither exists without the other.**

Every Loom application has two roles â€” even if they share a single VM:

```
                        THE LOOM ENGINE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                           â”‚
â”‚  SUPPORT LOOM (I/O Bridge)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  CPU â†â†’ GPU bidirectional                           â”‚  â”‚
â”‚  â”‚  â€¢ Boot / shutdown lifecycle                        â”‚  â”‚
â”‚  â”‚  â€¢ State: write snapshots, read for undo/redo       â”‚  â”‚
â”‚  â”‚  â€¢ Double buffer: wait(prev) â†’ present â†’ launch     â”‚  â”‚
â”‚  â”‚  â€¢ Upload: font atlas, weights, primes, config      â”‚  â”‚
â”‚  â”‚  â€¢ Download: results, framebuffer, metrics           â”‚  â”‚
â”‚  â”‚  â€¢ Persistence: file I/O, checkpoints               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                           â”‚ services                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚         â–¼                 â–¼                 â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ MAIN LOOM  â”‚  â”‚ MAIN LOOM  â”‚  â”‚ MAIN LOOM  â”‚          â”‚
â”‚  â”‚ (Render)   â”‚  â”‚ (Compute)  â”‚  â”‚ (Physics)  â”‚          â”‚
â”‚  â”‚            â”‚  â”‚            â”‚  â”‚            â”‚          â”‚
â”‚  â”‚ GPU only   â”‚  â”‚ GPU only   â”‚  â”‚ GPU only   â”‚          â”‚
â”‚  â”‚ 1-way recv â”‚  â”‚ 1-way recv â”‚  â”‚ 1-way recv â”‚          â”‚
â”‚  â”‚ dispatches â”‚  â”‚ dispatches â”‚  â”‚ dispatches â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                           â”‚
â”‚  Rules:                                                   â”‚
â”‚  â€¢ N Main Looms â†’ 1 Support Loom (many-to-one)           â”‚
â”‚  â€¢ Each Main polls exactly 1 Support (no I/O race)       â”‚
â”‚  â€¢ Main never initiates I/O â€” Support orchestrates all   â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Why Main Loom Cannot Exist Alone

Every Main Loom lifecycle requires I/O at four phases. Without Support, there is no Loom:

| Phase | What Happens | I/O Required |
|-------|-------------|--------------|
| **Boot** | Allocate VRAM, create buffers | `loom_boot()` â€” CPU â†’ GPU allocation |
| **Init** | Upload shaders, data, weights | `loom_dispatch()`, `loom_write()`, `loom_set_heap()` |
| **Compute** | Dispatch kernels, async execute | `loom_dispatch()` â†’ `loom_build()` â†’ `loom_launch()` |
| **Results** | Read back, present, poll | `loom_wait()`, `loom_present()`, `loom_read()` |
| **Shutdown** | Free VRAM, destroy buffers | `loom_shutdown()` |

Boot, Init, Results, Shutdown are all I/O â€” Support Loom territory. Only Compute is pure GPU.

### Main Loom (GPU Compute)

- **Purpose:** Pure GPU computation. Shaders, kernels, rendering, matmul.
- **Communication:** 1-way receive only. Accepts dispatches via `loom_dispatch()`.
- **Never does:** `loom_write()`, `loom_read()`, `loom_present()`, `loom_wait()`, file I/O.
- **Multiplicity:** N per application (render, physics, AI, etc.).
- **Constraint:** Each Main polls exactly ONE Support Loom. No cross-Main reads.

### Support Loom (I/O Bridge)

- **Purpose:** All CPUâ†”GPU data movement. Lifecycle management. State persistence.
- **Communication:** Bidirectional. CPU writes in, reads out. Orchestrates Main's presentation.
- **Owns:** Boot/shutdown, double buffer cycle, state snapshots, font atlas, weight cache.
- **Multiplicity:** 1 per application (serializes I/O, avoids races).
- **Services:** All Main Looms read through Support's orchestration.

### Three Ways to Build a Loom Engine

#### Implicit Support (CPU as Support)

The simplest form. CPU orchestrates all I/O directly. No dedicated Support VM.

```flow
// Prime Sieve â€” 16 Main Looms, CPU as implicit Support
let mut vms = []
let mut i = 0.0
while i < 16.0
  let vm = loom_boot(1.0, 8194.0, 4096.0)     // Support: boot
  loom_write(vm, 0.0, primes)                   // Support: upload
  loom_dispatch(vm, "sieve.spv", params, wg)    // Main: dispatch
  let prog = loom_build(vm)                     // Main: compile
  loom_launch(prog)                             // Main: async fire
  push(vms, vm)
  i = i + 1.0
end
// ... poll all, read results (Support orchestrates) ...
```

**When to use:** One-shot compute. No persistent GPU state. No real-time I/O.

#### Conflated (Single VM, Both Roles)

One VM does both compute and I/O. The OctoUI pattern for simple apps.

```flow
// Counter â€” single VM, conflated Main + Support
let vm = loom_boot(1.0, 1.0, 360000.0)          // Support: boot
// ... warm-up dispatches ...
while running == 1.0
  loom_wait(prev)                                // Support: sync
  loom_present(vm, total)                        // Support: present
  loom_set_heap(vm, atlas)                       // Support: upload
  loom_dispatch(vm, "rect.spv", params, wg)      // Main: dispatch
  loom_dispatch(vm, "text.spv", params, wg)      // Main: dispatch
  let prog = loom_build(vm)                      // Main: compile
  loom_launch(prog)                              // Main: async fire
end
```

**When to use:** Simple apps. No persistent state beyond framebuffer.

#### Explicit Support VM (Separated Roles)

Two VMs: Main does GPU compute, Support does state I/O. The clean architecture.

```flow
// Two-Loom Counter â€” Main + explicit Support
let support_vm = loom_boot(1.0, 0.0, 256.0)     // Support: lightweight
let main_vm = loom_boot(1.0, 1.0, 360000.0)     // Main: GPU compute

while running == 1.0
  loom_write(support_vm, offset, snapshot)       // Support: state
  let val = loom_read(support_vm, cursor)        // Support: undo
  loom_wait(prev)                                // Support: sync Main
  loom_present(main_vm, total)                   // Support: present Main
  loom_dispatch(main_vm, "shader.spv", p, wg)   // Main: dispatch
  loom_launch(loom_build(main_vm))               // Main: fire
end
```

**When to use:** Apps with persistent GPU state (undo/redo, streaming, training).

### The Double Buffer Cycle

The core efficiency of the Loom Engine. Support owns this cycle:

```
Frame N:                          Frame N+1:
  GPU working (async)               GPU working (async)
  â†“                                 â†“
  loom_wait(N)   â† instant         loom_wait(N+1) â† instant
  loom_present() â†’ window          loom_present() â†’ window
  ... CPU collects next frame ...  ... CPU collects next frame ...
  loom_dispatch Ã— N â†’ build        loom_dispatch Ã— N â†’ build
  loom_launch(N+1) â†’ fire&forget   loom_launch(N+2) â†’ fire&forget
```

GPU and CPU work in parallel. The `loom_wait` at frame start is usually instant because the GPU had an entire frame's worth of CPU time to finish.

### Communication Rules

| Direction | Mechanism | Who Initiates | Example |
|-----------|-----------|---------------|---------|
| CPU â†’ Support | `loom_write(vm, offset, data)` | App logic | Write state snapshot |
| Support â†’ CPU | `loom_read(vm, offset, count)` | App logic | Read undo state |
| CPU â†’ Main | `loom_dispatch(vm, kernel, params, n)` | App logic | Submit shader work |
| Main â†’ window | `loom_present(vm, total)` | Support orchestrates | Present framebuffer |
| Main sync | `loom_wait(prog_id)` | Support orchestrates | Wait for GPU completion |
| Main poll | `loom_poll(prog_id)` | Support orchestrates | Non-blocking check |

### Use Cases

| Application | Main Loom(s) | Support Loom | Support Type |
|-------------|-------------|--------------|--------------|
| **Counter app** | Render | CPU (conflated) | Implicit |
| **Two-Loom Counter** | Render | State VM (undo/redo) | Explicit |
| **Prime Sieve** | 16 Compute VMs | CPU (boot/read) | Implicit |
| **Game Engine** | Render + Physics + AI | State VM (ECS, save/load) | Explicit |
| **LLM Inference** | Compute (matmul) | Weight VM (layer cache, KV) | Explicit |
| **Data Streaming** | Transform VMs | I/O VM (file, network) | Explicit |
| **Training** | Forward + Backward | Optimizer VM (gradients, checkpoint) | Explicit |

Rule of thumb: Use explicit Support VM when you need **persistent GPU-resident state** across frames or iterations.

---

## Multi-Threading: Support Loom as Thread Coordinator

The Loom Engine unlocks two things:

1. **Unhinged parallel GPU compute** â€” N Main Looms dispatch freely. Homeostasis is the safety valve.
2. **Multi-threading as default** â€” Support Loom manages CPU threads transparently. No thread code in `.flow`.

### The Problem

Today, OctoUI dispatches 60-70 GPU operations per frame. Each dispatch reads the same `.spv` files from disk. `loom_present()` blocks 10-50ms downloading framebuffers. Homeostasis `sleep()` blocks per-dispatch. All on a single CPU thread.

The GPU is parallel. The CPU is not. The CPU is the bottleneck.

### The Solution

Support Loom becomes the threading coordinator. It manages a thread pool so CPU can keep up with parallel GPU compute â€” without exposing threads to `.flow` code.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Main Thread (event loop, app logic â€” single-threaded)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ui_poll_events() â†’ state updates â†’ submit work       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                             â”‚ submits                       â”‚
â”‚                             â–¼                               â”‚
â”‚  SUPPORT LOOM THREAD POOL                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚  I/O Thread â”‚ â”‚  Worker 1  â”‚ â”‚  Worker 2  â”‚              â”‚
â”‚  â”‚  loom_write â”‚ â”‚  build     â”‚ â”‚  build     â”‚              â”‚
â”‚  â”‚  loom_read  â”‚ â”‚  chain VM1 â”‚ â”‚  chain VM2 â”‚              â”‚
â”‚  â”‚  loom_wait  â”‚ â”‚  dispatch  â”‚ â”‚  dispatch  â”‚              â”‚
â”‚  â”‚  loom_pres. â”‚ â”‚  Ã— N       â”‚ â”‚  Ã— N       â”‚              â”‚
â”‚  â”‚  file I/O   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                             â”‚
â”‚         â”‚              â”‚              â”‚                      â”‚
â”‚         â–¼              â–¼              â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ Support VM â”‚ â”‚  Main VM 1 â”‚ â”‚  Main VM 2 â”‚              â”‚
â”‚  â”‚ (state)    â”‚ â”‚  (render)  â”‚ â”‚  (compute) â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         GPU                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Key properties:

- **App code stays single-threaded.** No threads in `.flow` â€” Support Loom handles it internally.
- **I/O thread serializes all CPUâ†”GPU transfers.** One thread owns `loom_write/read/wait/present` â€” no races.
- **Worker threads parallelize dispatch chain building.** Each Main Loom's `loom_dispatch Ã— N â†’ loom_build` runs concurrently.
- **Main thread stays responsive.** Event loop never blocks on GPU I/O.
- **Homeostasis remains automatic.** Runtime auto-paces dispatches via timing-based detection.

---

## vs. CUDA Streams

CUDA Streams are the industry benchmark for CPU-GPU overlap. NVIDIA's own benchmarks show 42-48% speedup from overlapping data transfers with kernel execution. OctoFlow's Loom Engine achieves the same overlap â€” but transparently.

### CUDA Streams: Manual Everything

```cuda
// CUDA: 3 concurrent GPU pipelines â€” ~40 lines minimum
cudaStream_t s1, s2, s3;
cudaStreamCreate(&s1);
cudaStreamCreate(&s2);
cudaStreamCreate(&s3);

// Pin host memory (required for async)
float *h_a, *h_b, *h_c;
cudaMallocHost(&h_a, size);
cudaMallocHost(&h_b, size);
cudaMallocHost(&h_c, size);

// Allocate device memory
float *d_a, *d_b, *d_c;
cudaMalloc(&d_a, size);
cudaMalloc(&d_b, size);
cudaMalloc(&d_c, size);

// Async transfer + compute per stream
cudaMemcpyAsync(d_a, h_a, size, cudaMemcpyHostToDevice, s1);
kernel_render<<<grid, block, 0, s1>>>(d_a);

cudaMemcpyAsync(d_b, h_b, size, cudaMemcpyHostToDevice, s2);
kernel_physics<<<grid, block, 0, s2>>>(d_b);

cudaMemcpyAsync(d_c, h_c, size, cudaMemcpyHostToDevice, s3);
kernel_particles<<<grid, block, 0, s3>>>(d_c);

// Synchronize
cudaStreamSynchronize(s1);
cudaStreamSynchronize(s2);
cudaStreamSynchronize(s3);

// Cleanup (6 frees, 3 stream destroys)
cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);
cudaFreeHost(h_a); cudaFreeHost(h_b); cudaFreeHost(h_c);
cudaStreamDestroy(s1); cudaStreamDestroy(s2); cudaStreamDestroy(s3);
```

### OctoFlow Loom Engine: Same Result

```flow
// OctoFlow: 3 concurrent GPU pipelines â€” 15 lines
let render  = loom_boot(1.0, 1.0, total)
let physics = loom_boot(1.0, 1.0, total)
let parts   = loom_boot(1.0, 1.0, total)

loom_dispatch(render,  "render.spv",  r_params, wg)
loom_dispatch(physics, "physics.spv", p_params, wg)
loom_dispatch(parts,   "parts.spv",   t_params, wg)

loom_launch(loom_build(render))
loom_launch(loom_build(physics))
loom_launch(loom_build(parts))

while loom_poll(render) < 0.5
end
// Done. No streams, no pinned memory, no cleanup.
```

### The Comparison

| | CUDA | Vulkan (raw) | OctoFlow |
|---|------|-------------|----------|
| Streams/queues | Manual `cudaStreamCreate` | Manual `VkQueue` + `VkCommandPool` per thread | **None** |
| Pinned memory | `cudaMallocHost` | `VK_MEMORY_PROPERTY_HOST_VISIBLE` | **Automatic** |
| Synchronization | `cudaStreamSynchronize`, events | Semaphores, fences, barriers | **Automatic** |
| Thread management | `pthread` / `std::thread` | Per-thread command pools | **Automatic** |
| Upload/download overlap | Manual async memcpy per stream | Manual staging + copy commands | **Automatic** |
| Memory cleanup | Manual `cudaFree` Ã— N | Manual `vkDestroyBuffer` Ã— N | **Automatic** |
| Vendor lock-in | NVIDIA only | Any GPU | **Any GPU** |
| Lines for 3 pipelines | ~40-50 | ~100-200 | **~15** |

### Benchmark Context

NVIDIA's published benchmarks for CUDA stream overlap:

| Technique | Speedup | Source |
|-----------|---------|--------|
| Async transfer + compute overlap | 42-44% | NVIDIA Tesla K20c / C2050 |
| 3+ parallel streams (PCIe) | 40-60% latency reduction | NVBench 2024 |
| Pinned memory + 4 queues | 95% PCIe peak (vs 65% serial) | CUDA Best Practices Guide |
| Stream-based kernel concurrency | up to 24% | Multi-stream evaluation study |

OctoFlow's Loom Engine targets the same overlap pattern â€” but through the runtime, not through developer code. The developer writes sequential `.flow`, the Support Loom threads it automatically.

---

## Proven at Scale

### Prime Sieve: Seven Generations

| Scale | Result | GPU Time | Total | Status |
|---|---|---|---|---|
| pi(10^7) | 664,579 | 10ms | 234ms | EXACT |
| pi(10^8) | 5,761,455 | 96ms | 507ms | EXACT |
| pi(10^9) | 50,847,536 | 792ms | 2,523ms | EXACT |
| pi(10^10) | 455,052,512 | 7,843ms | 22,400ms | EXACT |

Hardware: GTX 1660 SUPER (mid-range, 6GB). All results verified against
known prime counting function values.

### The Journey: v1 to v7

| Version | Key Innovation | pi(10^9) GPU | Speedup |
|---|---|---|---|
| v1 | f32 per-element, trial division | 65,000ms | baseline |
| v2 | Bit-packed uint32, hardware popcount | 1,350ms | 48x |
| v3 | L1-sized segments, shared memory, bucket sieve | 765ms | 1.8x |
| v4 | Carry-forward offsets, selective JIT | 764ms | ~1x |
| v5 | Runtime SPIR-V synthesis | 760ms | ~1x |
| v6 | uint64 addressing (breaks 4B wall) | 775ms | â€” |
| v7 | Sentinel carry-forward + uint64 | 792ms | (at 10^10+) |

From 65 seconds to under 1 second. 1000x less VRAM. Exact at 10 billion.

### GPU Patterns Used

| Pattern | Sieve Usage | Transferable To |
|---|---|---|
| Bit-packing (32 bools/word) | Prime bitmap | Bloom filters, image masks, graph adjacency |
| L1-sized segmentation (32KB) | Cache-hot sieve | Any streaming computation |
| Shared memory prime cache | Small prime marking | Cooperative loading for any shared data |
| Tree reduction (shared mem) | Parallel popcount | Sum, min, max, histogram |
| Atomic AND (bit-clear) | Composite marking | Lock-free set operations |
| Carry-forward state | Resume across segments | Iterative solvers, streaming aggregation |
| uint64 arithmetic | Address beyond 4B | Large-scale indexing, cryptographic ops |
| Async swarm (16 units) | Parallel segments | Any embarrassingly parallel workload |

---

## Standard Library

### Domain-Organized Modules

```
stdlib/loom/
â”œâ”€â”€ core/                     Engine runtime
â”‚   â”œâ”€â”€ boot.flow             Unit lifecycle
â”‚   â”œâ”€â”€ dispatch.flow         Kernel dispatch + chain recording
â”‚   â””â”€â”€ monitor.flow          Profiling + diagnostics
â”‚
â”œâ”€â”€ ir.flow                   Kernel authoring (Tier 3) â€” re-exports compiler IR
â”‚
â”œâ”€â”€ patterns/                 One-call GPU compute (Tier 1)
â”‚   â”œâ”€â”€ reduce.flow           gpu_sum, gpu_min, gpu_max
â”‚   â”œâ”€â”€ map.flow              gpu_map (element-wise transforms)
â”‚   â”œâ”€â”€ scan.flow             gpu_scan (prefix sum)
â”‚   â”œâ”€â”€ sort.flow             gpu_sort (radix sort)
â”‚   â”œâ”€â”€ sieve.flow            gpu_prime_count (parallel sieve)
â”‚   â””â”€â”€ matmul.flow           gpu_matmul (tiled matrix multiply)
â”‚
â”œâ”€â”€ math/                     Numerical computation
â”‚   â”œâ”€â”€ linalg.flow           Linear algebra (dot, cross, normalize)
â”‚   â”œâ”€â”€ stats.flow            Statistical operations on GPU
â”‚   â”œâ”€â”€ signal.flow           Signal processing (FFT, convolution)
â”‚   â””â”€â”€ advanced.flow         Special functions (gamma, bessel)
â”‚
â”œâ”€â”€ nn/                       Neural network primitives
â”‚   â”œâ”€â”€ attention.flow        Multi-head attention
â”‚   â”œâ”€â”€ ffn.flow              Feed-forward layers
â”‚   â”œâ”€â”€ rmsnorm.flow          RMS normalization
â”‚   â”œâ”€â”€ rope.flow             Rotary position embedding
â”‚   â”œâ”€â”€ silu.flow             SiLU activation
â”‚   â”œâ”€â”€ softmax.flow          Softmax
â”‚   â”œâ”€â”€ matmul_tiled.flow     Tiled GEMM
â”‚   â””â”€â”€ dequant.flow          Quantization (Q4_K, Q6_K)
â”‚
â”œâ”€â”€ data/                     Data-parallel operations
â”‚   â”œâ”€â”€ array_ops.flow        GPU array operations
â”‚   â”œâ”€â”€ aggregate.flow        Group-by, histogram
â”‚   â”œâ”€â”€ composite.flow        Multi-step data pipelines
â”‚   â””â”€â”€ dlb_scan.flow         Load-balanced parallel scan
â”‚
â”œâ”€â”€ kernels/                  Pre-compiled SPIR-V binaries
â”‚   â”œâ”€â”€ math/                 abs, add, sqrt, sin, cos, ...
â”‚   â”œâ”€â”€ reduce/               reduce_sum, reduce_min, reduce_max
â”‚   â”œâ”€â”€ sieve/                sieve_init, sieve_mark, sieve_count, ...
â”‚   â”œâ”€â”€ nn/                   matvec, rmsnorm, rope, silu, softmax, ...
â”‚   â””â”€â”€ vm/                   vm_add, vm_scale, vm_relu, ...
â”‚
â”œâ”€â”€ emit/                     Kernel emitters (.flow â†’ .spv)
â”‚   â”œâ”€â”€ sieve/                Sieve kernel emitters (v1-v7)
â”‚   â”œâ”€â”€ nn/                   Neural net kernel emitters
â”‚   â””â”€â”€ ops/                  Math operation kernel emitters
â”‚
â””â”€â”€ tests/                    Test suite
    â”œâ”€â”€ test_bitwise_ir.flow
    â”œâ”€â”€ test_uint64_ir.flow
    â”œâ”€â”€ test_sieve.flow
    â”œâ”€â”€ test_reduce.flow
    â”œâ”€â”€ test_nn_kernels.flow
    â””â”€â”€ ...
```

### Design Principles

**Serve the dish, not the recipe.**

- `use stdlib.loom.sieve` gives you `gpu_prime_count(N)` â€” one call, exact result
- `use stdlib.loom.reduce` gives you `gpu_sum(data)` â€” not `boot + write + dispatch + build + launch + poll + read`
- `use stdlib.loom.nn.attention` gives you `gpu_attention(Q, K, V)` â€” not a 200-line dispatch chain

The patterns hide the machinery. The expert API exposes it when you need it.

**LLM-first naming.**

Every function name is guessable from its description:
- "sum this data on GPU" â†’ `gpu_sum(data)`
- "count primes below N" â†’ `gpu_prime_count(N)`
- "multiply matrices A and B" â†’ `gpu_matmul(A, B, m, n, k)`

An LLM generating OctoFlow code should never need to read documentation to
find the right function name. If it can describe what it wants, it can guess
the function.

**Domain grouping, not implementation grouping.**

Old: `stdlib/gpu/emit_sieve_mark_v3_large.flow` (organized by what it IS)
New: `stdlib/loom/emit/sieve/mark_v3_large.flow` (organized by what it DOES)

Old: `stdlib/gpu/emit_vm_rmsnorm.flow` (mixed with sieve code)
New: `stdlib/loom/emit/nn/rmsnorm.flow` (with other neural net emitters)

Users find things by domain (sieve, neural nets, reduction), not by
implementation detail (emit, vm, v3).

---

## Under the Hood

### SPIR-V: The Fabric

Every Loom kernel compiles to SPIR-V â€” the standard intermediate language for
Vulkan compute shaders. SPIR-V is:
- **Vendor-neutral**: Runs on NVIDIA, AMD, Intel, Qualcomm, ARM Mali
- **Binary**: No runtime compilation step (unlike GLSL)
- **Validatable**: `spirv-val` checks correctness before GPU touches it

OctoFlow emits SPIR-V directly â€” no GLSL, no HLSL, no intermediate language.
The IR builder (`stdlib/loom/ir/ir.flow`) is itself written in OctoFlow: the
language writes its own GPU kernels.

### Vulkan: The Frame

Loom sits on Vulkan Compute via the `ash` crate (thin Rust bindings):
- **Command buffers**: Pre-recorded dispatch chains compiled to GPU-native commands
- **Descriptor sets**: Buffer bindings (registers, globals) set once per unit
- **Push constants**: Small per-dispatch parameters (up to 5 Ã— 32-bit values)
- **Memory barriers**: Automatic between dispatches (no manual sync)

### f32 Precision Engineering

OctoFlow uses `Value::Float(f32)` for all values. f32 has a 24-bit mantissa â€”
exact integers only to 2^24 = 16,777,216. Loom's proven solutions:

| Challenge | Solution |
|---|---|
| Large uint32 constants | Compute on GPU: `ir_not(c0)` = 0xFFFFFFFF |
| Push constants > 2^24 | Pass small inputs, GPU computes full value |
| N > 2^24 (addressing) | Split: `N_hi Ã— 2^24 + N_lo`, reconstruct in uint64 |
| Accumulation overflow | GPU accumulates in uint32, readback via `float_to_bits()` |
| NaN bit patterns | Never interpret raw uint32 as f32; use indirect computation |
| Boundary precision | Sentinel design: GPU-side is authoritative, CPU-side is advisory |

These patterns are hard-won through seven generations of GPU sieve development
and apply to any Loom program working with large integers or addresses.

---

## Icon: The Octopus Weaving

```
        â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
        â”‚                           â”‚
        â”‚      ğŸ™                   â”‚
        â”‚    Eight arms             â”‚
        â”‚    working the loom       â”‚
        â”‚                           â”‚
        â”‚    â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•ªâ•â•â•       â”‚
        â”‚    â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€       â”‚
        â”‚    â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•ªâ•â•â•       â”‚
        â”‚    â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€â”¼â”€â”€â”€       â”‚
        â”‚    â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•ªâ•â•â•       â”‚
        â”‚                           â”‚
        â”‚    Threads woven          â”‚
        â”‚    into fabric            â”‚
        â”‚                           â”‚
        â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯
```

**Visual concept**: An octopus sitting at a loom, its eight arms each pulling
a different thread through the weave. The warp threads (vertical) are data
streams. The weft threads (horizontal) are GPU operations. The fabric that
emerges is the computed result.

**Icon elements:**
- Octopus (brand identity) in profile view, working a loom
- Loom frame with visible thread grid (suggests parallel structure)
- Gradient from raw threads (left) to woven fabric (right) â€” input to output
- Eight arms visible, each engaged with different part of the weave

**Color palette:**
- Deep ocean blue (#1a3a5c) â€” background
- Warm amber (#f0a030) â€” thread/compute highlights
- Silver (#c0c0cc) â€” loom frame
- White (#ffffff) â€” fabric/output

**Tagline options:**
- "Weaving parallel computation"
- "Eight arms. Thousands of threads. One loom."
- "The GPU runtime for the rest of us"

---

## Getting Started

### Prerequisites

- OctoFlow compiler (latest)
- Vulkan-capable GPU with driver
- Vulkan SDK (for `spirv-val` validation, optional)

### Your First Loom Program

```flow
// hello_loom.flow â€” double every element on GPU

let data = [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]

// Boot a compute unit (1 buffer, 8 registers, no globals)
let unit = loom_boot(1.0, 8, 0)

// Dispatch the "double" kernel
loom_dispatch(unit, "stdlib/loom/kernels/math/double.spv", [], 1.0)

// Build, run, read
let prog = loom_build(unit)
loom_run(prog)
let result = loom_read(unit, 0.0, 0.0, 8)

print("Input:  {data}")
print("Output: {result}")
// â†’ [2.0, 4.0, 6.0, 8.0, 10.0, 12.0, 14.0, 16.0]
```

### Running It

```bash
octoflow run hello_loom.flow --allow-read
```

---

## LoomDB â€” GPU-Resident Data Layer

LoomDB captures GPU pipeline results and makes them searchable â€” without
touching disk. It runs in its own Loom, completely isolated from your main
compute pipeline.

### The Two-Loom Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MAIN LOOM (zero I/O)            â”‚
â”‚                                  â”‚
â”‚  Compute: loom_dispatch chains   â”‚
â”‚  Capture: loomdb_capture()       â”‚  GPU memory only
â”‚  Search:  loomdb_search()        â”‚  No file syscalls
â”‚           loomdb_gpu_search()    â”‚  No network calls
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ shared VRAM
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOOMDB LOOM (owns all I/O)      â”‚
â”‚                                  â”‚
â”‚  Persist: loomdb_flush()         â”‚  Writes .ldb + .vectors + .meta
â”‚  Restore: loomdb_restore_*()     â”‚  Loads from disk at startup
â”‚  Never blocks the Main Loom.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The Main Loom physically cannot do I/O â€” none of the capture/search functions
contain file operations. This isn't a convention; it's architectural enforcement.

### Quick Start

```flow
use "db/loomdb"

// Create: 384-dimensional embeddings, auto-flush at 10,000
let ldb = loomdb_create(384.0, 10000.0)
let vectors = loomdb_create_vectors()

// Capture results (GPU memory, zero I/O)
let _c1 = loomdb_capture(ldb, vectors, "emb_001", embedding_a, "batch=42")
let _c2 = loomdb_capture(ldb, vectors, "emb_002", embedding_b, "batch=42")
let _c3 = loomdb_capture(ldb, vectors, "emb_003", embedding_c, "batch=43")

// Search (GPU memory, zero I/O)
let _s = loomdb_search(ldb, vectors, query_vector, 5.0, "cosine")

// Read results
let count = loomdb_result_count(ldb)
for i in range(0, count)
  let id = loomdb_result_id(ldb, i)
  let score = loomdb_result_score(ldb, i)
  print("{id}: {score}")
end

// GPU-accelerated search (fast for 1000+ vectors)
let _g = loomdb_gpu_search(ldb, vectors, query_vector, 10.0)
```

### Persistence

```flow
// Check capacity
if loomdb_needs_flush(ldb) == 1.0
  let _f = loomdb_flush(ldb, vectors, "cache/embeddings")
end

// Restore on next startup
let ldb = loomdb_restore_meta("cache/embeddings")
let vectors = loomdb_restore_vectors("cache/embeddings")
// GPU-resident again, ready to search
```

### LoomDB API

| Function | I/O? | Description |
|----------|------|-------------|
| `loomdb_create(dims, cap)` | No | Create instance (dims = embedding size, cap = flush threshold) |
| `loomdb_create_vectors()` | No | Create empty vectors array |
| `loomdb_capture(ldb, vecs, id, emb, meta)` | No | Capture a vector to GPU memory |
| `loomdb_search(ldb, vecs, q, k, metric)` | No | CPU similarity search (cosine/dot/euclidean) |
| `loomdb_gpu_search(ldb, vecs, q, k)` | No | GPU-accelerated search via gpu_matmul |
| `loomdb_needs_flush(ldb)` | No | Check if capacity threshold reached |
| `loomdb_normalize(ldb, vecs)` | No | Pre-normalize for faster cosine search |
| `loomdb_result_count(ldb)` | No | Number of search results |
| `loomdb_result_id(ldb, i)` | No | Result ID at position i |
| `loomdb_result_score(ldb, i)` | No | Result score at position i |
| `loomdb_result_meta(ldb, i)` | No | Result metadata at position i |
| `loomdb_flush(ldb, vecs, path)` | **Yes** | Persist to .ldb + .vectors + .meta |
| `loomdb_restore_meta(path)` | **Yes** | Load metadata from disk |
| `loomdb_restore_vectors(path)` | **Yes** | Load vectors from disk |

---

## OctoDB â€” Structured Data Storage

OctoDB is OctoFlow's embedded database for structured data â€” tables, rows,
CRUD operations, and `.odb` file persistence. It also serves as LoomDB's
cold storage tier.

### Quick Start

```flow
use "db/core"
use "db/engine"

// Create a table
let db = db_create()
let users = db_table(db, "users", ["name", "age", "email"])

// Insert
let mut row = map()
row["name"] = "Alice"
row["age"] = 30.0
row["email"] = "alice@example.com"
let _i = db_insert(users, row)

// Query
let indices = db_where(users, "age", ">", 25.0)
let avg_age = db_aggregate(users, "age", "avg")

// Multi-condition
let results = db_select(users, ["age", "name"], [">", "contains"], [25.0, "Ali"])

// Persist
let _s = db_save(users, "data/users.odb")
let restored = db_load("data/users.odb")
```

### OctoDB API

| Function | Description |
|----------|-------------|
| `db_create()` | Create database |
| `db_table(db, name, columns)` | Create table |
| `db_insert(table, row)` | Insert row (returns index) |
| `db_select_row(table, idx)` | Get row by index |
| `db_select_column(table, col, n)` | Get first n values of a column |
| `db_where(table, col, op, val)` | Filter rows (==, !=, >, <, >=, <=, contains) |
| `db_select(table, cols, ops, vals)` | Multi-condition AND filter |
| `db_update(table, idx, row)` | Update row fields |
| `db_delete(table, idx)` | Soft delete |
| `db_count(table)` | Row count |
| `db_distinct(table, col)` | Unique values |
| `db_aggregate(table, col, op)` | sum, avg, min, max, count |
| `db_save(table, path)` / `db_load(path)` | Single-table persistence (.odb) |
| `db_import_csv(table, path)` | Import CSV into table |

For multi-table persistence: `db_save_all_start`, `db_save_all_add`, `db_load_all`
(see `stdlib/db/persist.flow`).

---

## The Two-Tier Pattern

OctoDB and LoomDB work together:

```
GPU Memory (LoomDB)          Disk (OctoDB)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ loomdb_capture()  â”‚        â”‚ .odb files   â”‚
â”‚ loomdb_search()   â”‚ flush  â”‚ .ldb files   â”‚
â”‚ loomdb_gpu_search â”‚ â”€â”€â”€â”€>  â”‚ .vectors     â”‚
â”‚                   â”‚        â”‚ .meta        â”‚
â”‚ Source of truth   â”‚ <â”€â”€â”€â”€  â”‚ Cold storage â”‚
â”‚ during runtime    â”‚restore â”‚ between runs â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use OctoDB** for structured data: user tables, config, logs, CSV imports.
**Use LoomDB** for GPU pipeline results: embeddings, features, similarity search.
**Use both** when you need GPU-speed search with disk persistence between sessions.

---

## Roadmap

| Phase | What | Status |
|---|---|---|
| Loom Core | boot, dispatch, build, run, launch, poll, read | **Done** |
| IR Builder | 60+ SPIR-V ops including uint64, atomics, shared memory | **Done** |
| Prime Sieve | v1-v7, exact to 10^10, 95K dispatches | **Done** |
| Neural Net Kernels | attention, ffn, rmsnorm, rope, silu, softmax, matvec | **Done** |
| API Rename | vm_* â†’ loom_* function aliases (15+ aliases) | **Done** |
| Homeostasis | Timing-based auto-pacing for GPU throttle detection | **Done** |
| Multi-VM Fix | Buffer pool OOM recovery, lightweight Loom | **Done** |
| LoomDB | GPU-resident data layer with I/O isolation | **Done** |
| OctoDB | Structured CRUD with .odb persistence | **Done** |
| Two-Tier DB | LoomDB + OctoDB integration | **Done** |
| Loom Engine Architecture | Main Loom + Support Loom, three implementation patterns | **Done** |
| Support Loom Threading | SPIR-V cache, async present, batch pacing, queue mutex | **In Progress** |
| Pattern Library | gpu_sum, gpu_sort one-call wrappers | Partial |
| Multi-Loom Showcase | Visual demo: 3 Main Looms + Support, benchmarked vs CUDA | Planned |
| Console Monitor | loom_profile_start/end, timing, VRAM stats | Planned |
| Multi-GPU Swarm | Network dispatch across machines | Future |
| Compiled Chains | Eliminate interpreter bottleneck for dispatch recording | Future |

---

## Why "Loom"?

- A **loom** weaves many threads into fabric â€” we weave GPU threads into results
- **Threads** are the fundamental unit of both textiles and GPU compute
- The **dispatch chain** is the pattern â€” pre-recorded, then woven in one pass
- The **octopus** works the loom with eight arms â€” our brand, our architecture
- Four characters. Zero ecosystem collision. Immediately evocative.

The loom weaves. The octopus works the loom. The fabric is your result.
