# OctoFlow — Annex M: Neural Networks Built in OctoFlow

**Parent Document:** OctoFlow Strategic Vision
**Status:** Architecture Specification
**Version:** 0.1
**Date:** February 17, 2026

---

## The Vision

**Build a complete neural network framework in pure OctoFlow** — not as external library, not as Python bindings, but as `.flow` code using OctoFlow's hypergraph primitives (Annex L).

This proves:
1. OctoFlow's primitives are sufficient for production ML
2. The hypergraph abstraction is genuinely expressive
3. GPU acceleration works for real AI/ML workloads
4. OctoFlow can compete with PyTorch/TensorFlow on their home turf

---

## Table of Contents

1. What Gets Built in Pure .flow
2. The Three Architectures
3. Feedforward Networks
4. Graph Neural Networks (GNNs)
5. Transformers
6. Automatic Differentiation
7. Training Loop
8. Loss Functions
9. Optimizers
10. Example: MNIST Digit Classification
11. Example: Node Classification on Citation Graph
12. Example: Text Classification with Transformer
13. Performance vs PyTorch
14. What This Proves

---

## 1. What Gets Built in Pure .flow

### The Stack

```
Layer 4: High-level APIs (Python's PyTorch equivalent)
  - model.fit(data, labels)
  - model.predict(data)
  - Sequential, ModuleList, pre-built architectures

Layer 3: Building Blocks (written in .flow)
  - Linear layer, Conv layer, Attention
  - ReLU, Softmax, BatchNorm
  - CrossEntropy, MSE loss
  - SGD, Adam optimizers

Layer 2: Tensor Operations (OctoFlow primitives + HyperGraphDB)
  - GEMM (dense matrix multiply)
  - SpMM (sparse matrix multiply for GNNs)
  - Elementwise ops (add, multiply, relu)
  - Scatter, gather (from Annex L)

Layer 1: GPU Kernels (SPIR-V, generated by OctoFlow compiler)
  - Already exist from Phases 0-40
  - Extended in Phases 47-51 for SpMM, GEMM, advanced reductions
```

**Layers 3-4 are written in .flow.** This is the proof that OctoFlow is expressive enough for production ML.

---

## 2. The Three Architectures

To prove completeness, we implement three fundamentally different neural network architectures:

1. **Feedforward (MLP)** — Dense matrix operations, no graph structure
2. **Graph Neural Network (GCN/GAT)** — Hypergraph message passing via incidence matrix
3. **Transformer** — Attention as hypergraph, sequence modeling

If all three work in pure .flow, OctoFlow can handle any neural network architecture.

---

## 3. Feedforward Networks

### Architecture

```flow
// Module: stdlib/ml/linear.flow
fn linear_layer(input: Tensor, weights: Tensor, bias: Tensor) -> Tensor {
    // input: [batch, in_features]
    // weights: [in_features, out_features]
    // bias: [out_features]
    return (input @ weights) + bias
}

fn relu(x: Tensor) -> Tensor {
    return max(x, 0.0)  // elementwise, already GPU-accelerated
}

// Module: stdlib/ml/feedforward.flow
struct FeedForward {
    W1: Tensor,
    b1: Tensor,
    W2: Tensor,
    b2: Tensor,
    W3: Tensor,
    b3: Tensor
}

fn forward(model: FeedForward, input: Tensor) -> Tensor {
    let h1 = relu(linear_layer(input, model.W1, model.b1))
    let h2 = relu(linear_layer(h1, model.W2, model.b2))
    let output = linear_layer(h2, model.W3, model.b3)  // no activation on output
    return output
}
```

### Training Loop

```flow
fn train_feedforward(
    train_data: Tensor,      // [num_samples, input_dim]
    train_labels: Tensor,    // [num_samples, num_classes]
    num_epochs: u32,
    learning_rate: f32
) -> FeedForward {
    let mut model = init_feedforward(input_dim, hidden_dim, num_classes)

    for epoch in range(num_epochs)
        // Forward pass
        let logits = forward(model, train_data)

        // Loss
        let loss = cross_entropy(logits, train_labels)

        // Backward pass (compute gradients)
        let grads = autograd(loss, model)

        // Update weights
        model.W1 = model.W1 - learning_rate * grads.W1
        model.W2 = model.W2 - learning_rate * grads.W2
        model.W3 = model.W3 - learning_rate * grads.W3
        model.b1 = model.b1 - learning_rate * grads.b1
        model.b2 = model.b2 - learning_rate * grads.b2
        model.b3 = model.b3 - learning_rate * grads.b3

        if epoch % 10 == 0
            print("Epoch {epoch}: Loss = {loss:.4}")
        end
    end

    return model
}
```

---

## 4. Graph Neural Networks (GNNs)

### GCN Layer (from Annex L)

```flow
// Module: stdlib/ml/gcn.flow
fn gcn_layer(
    features: Tensor,        // [num_vertices, feature_dim]
    graph: HyperGraph,
    weights: Tensor          // [feature_dim, output_dim]
) -> Tensor {
    let messages = scatter(features, graph.B)      // B^T @ features
    let aggregated = gather(messages, graph.B)     // B @ messages
    let transformed = aggregated @ weights         // linear projection
    return relu(transformed)
}
```

### Graph Attention (GAT)

```flow
// Module: stdlib/ml/gat.flow
fn graph_attention_layer(
    features: Tensor,
    graph: HyperGraph,
    W: Tensor,
    attention_W: Tensor
) -> Tensor {
    // Transform features
    let h_transformed = features @ W

    // Compute attention scores (for each edge)
    let edge_features = scatter(h_transformed, graph.B)
    let attention_scores = softmax(edge_features @ attention_W)

    // Weighted aggregation
    let weighted_messages = edge_features * attention_scores
    return gather(weighted_messages, graph.B)
}
```

### Training GNN

```flow
fn train_gcn(
    graph: HyperGraph,
    node_labels: [u32; num_vertices],
    train_mask: [bool; num_vertices]
) -> Model {
    let mut model = init_gcn(input_dim, hidden_dim, num_classes)

    for epoch in range(num_epochs)
        // Forward pass: 2 GCN layers
        let h1 = gcn_layer(graph.vertex_features, graph, model.W1)
        let logits = gcn_layer(h1, graph, model.W2)

        // Loss (only on training nodes)
        let loss = cross_entropy(logits[train_mask], node_labels[train_mask])

        // Backward pass
        let grads = autograd(loss, model)

        // Update
        model.W1 = model.W1 - learning_rate * grads.W1
        model.W2 = model.W2 - learning_rate * grads.W2

        // Validation
        let val_acc = accuracy(logits[val_mask], node_labels[val_mask])
        print("Epoch {epoch}: Loss={loss:.4}, Val Acc={val_acc:.2}")
    end

    return model
}
```

---

## 5. Transformers

### Multi-Head Attention

```flow
// Module: stdlib/ml/attention.flow
fn multi_head_attention(
    Q: Tensor,               // [batch, seq_len, d_model]
    K: Tensor,
    V: Tensor,
    num_heads: u32
) -> Tensor {
    let d_k = d_model / num_heads

    // Split into heads
    let Q_heads = split_heads(Q, num_heads)  // [batch, heads, seq_len, d_k]
    let K_heads = split_heads(K, num_heads)
    let V_heads = split_heads(V, num_heads)

    // Scaled dot-product attention per head
    let scores = (Q_heads @ K_heads.T) / sqrt(d_k)
    let attention_weights = softmax(scores, dim=-1)
    let attended = attention_weights @ V_heads

    // Concatenate heads
    return concat_heads(attended)
}
```

### Transformer Block

```flow
fn transformer_block(
    input: Tensor,
    W_Q: Tensor, W_K: Tensor, W_V: Tensor, W_O: Tensor,
    W_ff1: Tensor, W_ff2: Tensor
) -> Tensor {
    // Multi-head self-attention
    let Q = input @ W_Q
    let K = input @ W_K
    let V = input @ W_V
    let attended = multi_head_attention(Q, K, V, num_heads)
    let attn_out = attended @ W_O

    // Residual + layer norm
    let normed1 = layer_norm(input + attn_out)

    // Feedforward
    let ff = relu(normed1 @ W_ff1) @ W_ff2

    // Residual + layer norm
    return layer_norm(normed1 + ff)
}
```

---

## 6. Automatic Differentiation

### The Gradient Tape Pattern

```flow
// Module: stdlib/ml/autograd.flow

// Record operations during forward pass
struct GradTape {
    operations: [Operation],
    intermediate_values: Map<TensorID, Tensor>
}

fn forward_and_record(
    model: Model,
    input: Tensor,
    tape: GradTape
) -> Tensor {
    // Each operation is recorded for backward pass
    let h1 = record(tape, linear_layer(input, model.W1, model.b1))
    let a1 = record(tape, relu(h1))
    let h2 = record(tape, linear_layer(a1, model.W2, model.b2))
    return h2
}

fn backward(loss: Tensor, tape: GradTape) -> Gradients {
    let mut grads = map()
    let mut grad_output = ones_like(loss)

    // Traverse tape in reverse
    for op in reverse(tape.operations)
        match op.type
            "linear" =>
                // dL/dW = input^T @ grad_output
                // dL/dinput = grad_output @ W^T
                let grad_W = op.input.T @ grad_output
                let grad_b = sum(grad_output, dim=0)
                let grad_input = grad_output @ op.weights.T
                map_set(grads, op.weight_id, grad_W)
                map_set(grads, op.bias_id, grad_b)
                grad_output = grad_input

            "relu" =>
                // dL/dinput = grad_output * (input > 0)
                grad_output = grad_output * (op.input > 0.0)

            "scatter" =>
                // Transpose of scatter is gather
                grad_output = gather(grad_output, op.graph.B)

            "gather" =>
                // Transpose of gather is scatter
                grad_output = scatter(grad_output, op.graph.B)
        end
    end

    return grads
}
```

**This is autograd in 40 lines.** PyTorch's autograd is thousands of lines because it handles arbitrary Python objects. OctoFlow's autograd only handles tensors and hypergraph ops.

---

## 7. Training Loop

### Generic Training Loop (Works for Any Model)

```flow
// Module: stdlib/ml/training.flow
fn train(
    model: Model,
    train_data: Tensor,
    train_labels: Tensor,
    num_epochs: u32,
    learning_rate: f32
) -> Model {
    let mut tape = init_gradient_tape()

    for epoch in range(num_epochs)
        // Forward pass (with gradient tape)
        let logits = forward_and_record(model, train_data, tape)

        // Compute loss
        let loss = cross_entropy(logits, train_labels)

        // Backward pass
        let grads = backward(loss, tape)

        // Update weights (gradient descent)
        model = update_weights(model, grads, learning_rate)

        // Report
        if epoch % 10 == 0
            let acc = accuracy(logits, train_labels)
            print("Epoch {epoch}: Loss={loss:.4}, Acc={acc:.2}")
        end

        // Clear tape for next iteration
        tape = reset(tape)
    end

    return model
}
```

---

## 8. Loss Functions

### Cross-Entropy (Classification)

```flow
fn cross_entropy(logits: Tensor, labels: Tensor) -> f32 {
    // logits: [batch, num_classes]
    // labels: [batch, num_classes] (one-hot) or [batch] (class indices)

    let probs = softmax(logits, dim=-1)
    let log_probs = log(probs + 1e-8)  // numerical stability

    // If labels are indices, convert to one-hot
    let labels_one_hot = to_one_hot(labels, num_classes)

    let per_sample_loss = -sum(labels_one_hot * log_probs, dim=-1)
    return mean(per_sample_loss)
}
```

### Mean Squared Error (Regression)

```flow
fn mse_loss(predictions: Tensor, targets: Tensor) -> f32 {
    let diff = predictions - targets
    return mean(diff * diff)
}
```

---

## 9. Optimizers

### SGD (Stochastic Gradient Descent)

```flow
fn sgd_update(
    weights: Tensor,
    gradients: Tensor,
    learning_rate: f32
) -> Tensor {
    return weights - learning_rate * gradients
}
```

### Adam (Adaptive Moment Estimation)

```flow
struct AdamState {
    m: Map<TensorID, Tensor>,  // first moment (mean)
    v: Map<TensorID, Tensor>,  // second moment (variance)
    t: u32                     // timestep
}

fn adam_update(
    weights: Tensor,
    gradients: Tensor,
    state: AdamState,
    learning_rate: f32,
    beta1: f32,
    beta2: f32
) -> (Tensor, AdamState) {
    // Update timestep
    state.t = state.t + 1

    // Update moments
    let m = beta1 * state.m + (1.0 - beta1) * gradients
    let v = beta2 * state.v + (1.0 - beta2) * (gradients * gradients)

    // Bias correction
    let m_hat = m / (1.0 - pow(beta1, state.t))
    let v_hat = v / (1.0 - pow(beta2, state.t))

    // Update weights
    let new_weights = weights - learning_rate * m_hat / (sqrt(v_hat) + 1e-8)

    // Update state
    state.m = m
    state.v = v

    return (new_weights, state)
}
```

---

## 10. Example: MNIST Digit Classification

### Full Working Code in .flow

```flow
// Load MNIST data
let train_images = read_csv("mnist_train.csv")
let train_labels = read_csv("mnist_labels.csv")

// Normalize to [0, 1]
let X = map_each(train_images, fn(pixel) pixel / 255.0 end)
let Y = to_one_hot(train_labels, num_classes=10)

// Define model (784 → 128 → 64 → 10)
struct MnistModel {
    W1: Tensor,  // [784, 128]
    b1: Tensor,  // [128]
    W2: Tensor,  // [128, 64]
    b2: Tensor,  // [64]
    W3: Tensor,  // [64, 10]
    b3: Tensor   // [10]
}

fn mnist_forward(model: MnistModel, input: Tensor) -> Tensor {
    let h1 = relu(input @ model.W1 + model.b1)
    let h2 = relu(h1 @ model.W2 + model.b2)
    return h2 @ model.W3 + model.b3  // logits
}

// Initialize weights (Xavier initialization)
let mut model = MnistModel {
    W1: random_normal(784, 128) * sqrt(2.0 / 784.0),
    b1: zeros(128),
    W2: random_normal(128, 64) * sqrt(2.0 / 128.0),
    b2: zeros(64),
    W3: random_normal(64, 10) * sqrt(2.0 / 64.0),
    b3: zeros(10)
}

// Train
model = train(model, X, Y, num_epochs=100, learning_rate=0.01)

// Test
let test_images = read_csv("mnist_test.csv") / 255.0
let predictions = mnist_forward(model, test_images)
let predicted_classes = argmax(predictions, dim=-1)

print("Test accuracy: {accuracy(predicted_classes, test_labels):.2}")
```

**This is pure .flow.** No Python. No external libraries. Just OctoFlow primitives + stdlib/ml modules.

---

## 11. Example: Node Classification on Citation Graph

### Cora Dataset (Citation Graph, 2708 Papers)

```flow
// Load citation graph
let graph = load_citation_graph("cora.graph")
// graph.B: [2708 vertices, ~10000 hyperedges]
// vertex_features: [2708, 1433] (bag-of-words)
// labels: [2708] (7 classes)

// Split data
let train_mask = range_array(0, 140)  // 20 per class
let val_mask = range_array(140, 640)
let test_mask = range_array(1708, 2708)

// Define 2-layer GCN
struct GCN {
    W1: Tensor,  // [1433, 16]
    W2: Tensor   // [16, 7]
}

fn gcn_forward(model: GCN, features: Tensor, graph: HyperGraph) -> Tensor {
    let h1 = gcn_layer(features, graph, model.W1)  // [2708, 16]
    return gcn_layer(h1, graph, model.W2)          // [2708, 7]
}

// Train
let mut model = init_gcn()

for epoch in range(200)
    let logits = gcn_forward(model, graph.vertex_features, graph)

    // Loss only on training nodes
    let loss = cross_entropy(logits[train_mask], graph.labels[train_mask])

    // Backward
    let grads = autograd(loss, model)

    // Update
    model.W1 = model.W1 - 0.01 * grads.W1
    model.W2 = model.W2 - 0.01 * grads.W2

    // Validation
    if epoch % 20 == 0
        let val_acc = accuracy(logits[val_mask], graph.labels[val_mask])
        print("Epoch {epoch}: Loss={loss:.4}, Val Acc={val_acc:.2}")
    end
end

// Test
let test_logits = gcn_forward(model, graph.vertex_features, graph)
let test_acc = accuracy(test_logits[test_mask], graph.labels[test_mask])
print("Test Accuracy: {test_acc:.2}")
```

**Expected result:** ~80% test accuracy (GCN on Cora is a well-known benchmark).

---

## 12. Example: Text Classification with Transformer

### Sentiment Analysis

```flow
// Simplified transformer for sequence classification
struct TransformerClassifier {
    embedding: Tensor,       // [vocab_size, d_model]
    W_Q: Tensor, W_K: Tensor, W_V: Tensor, W_O: Tensor,
    W_ff1: Tensor, W_ff2: Tensor,
    W_classifier: Tensor     // [d_model, num_classes]
}

fn transformer_forward(
    model: TransformerClassifier,
    token_ids: [u32; seq_len]
) -> Tensor {
    // Embedding lookup
    let embedded = lookup(model.embedding, token_ids)  // [seq_len, d_model]

    // Self-attention block
    let attended = transformer_block(
        embedded,
        model.W_Q, model.W_K, model.W_V, model.W_O,
        model.W_ff1, model.W_ff2
    )

    // Global pooling (mean over sequence)
    let pooled = mean(attended, dim=0)  // [d_model]

    // Classification head
    return pooled @ model.W_classifier  // [num_classes]
}

// Train on sentiment dataset
let train_texts = load_imdb_train()
let train_sentiments = load_imdb_labels()  // 0=negative, 1=positive

let mut model = init_transformer_classifier(vocab_size, d_model=256, num_classes=2)

for epoch in range(50)
    let mut total_loss = 0.0

    for i in range(len(train_texts))
        let tokens = tokenize(train_texts[i])
        let logits = transformer_forward(model, tokens)
        let loss = cross_entropy(logits, train_sentiments[i])

        let grads = autograd(loss, model)
        model = adam_update(model, grads, learning_rate=0.001)

        total_loss = total_loss + loss
    end

    print("Epoch {epoch}: Avg Loss = {total_loss / len(train_texts):.4}")
end
```

---

## 13. Performance vs PyTorch

### Expected Benchmarks (Post-Phase 52)

| Task | PyTorch (CUDA) | OctoFlow | Difference |
|------|---------------|----------|------------|
| MNIST training (1 epoch) | ~5s | ~3-5s | Competitive (same GPU kernels) |
| GCN on Cora (200 epochs) | ~15s | ~10-15s | **Faster** (fused kernels) |
| Transformer inference (batch=32) | ~50ms | ~30-50ms | Competitive |
| SpMM (100M edges) | ~10ms (DGL) | ~10ms | Same (cuSPARSE) |

**Why competitive:**
- Same GPU (NVIDIA, same hardware)
- Same kernels (cuBLAS GEMM, cuSPARSE SpMM)
- Less Python overhead (OctoFlow → SPIR-V → GPU directly)
- Kernel fusion (scatter + gather in one kernel)

**Why potentially faster:**
- RT-GNN (2024 paper) shows SpMM + GEMM fusion gives 1.5-2x speedup
- OctoFlow's compiler can fuse at compile time (PyTorch fuses at runtime via JIT)
- No Python interpreter overhead

**Why might be slower:**
- PyTorch has 10+ years of CUDA kernel optimization
- OctoFlow is new — kernels will improve over time

**The point:** Being competitive is enough. The differentiator is **simplicity and unification**, not raw speed.

---

## 14. What This Proves

### If Neural Networks Work in Pure .flow

**It proves:**
1. OctoFlow's primitives are sufficient for production ML
2. The hypergraph abstraction is genuinely expressive
3. The sparse matrix foundation unifies graphs + neural networks
4. GPU acceleration works for real AI/ML workloads
5. **OctoFlow is not a DSL. It is a general-purpose language that can build PyTorch from scratch.**

### The Demo That Launches OctoFlow

**Phase 52 public release demo:**

```
"Here is a complete neural network framework — feedforward,
GNNs, transformers, autograd, optimizers — built in 2,000
lines of OctoFlow code.

No Python. No CUDA. No PyTorch dependency.

Just .flow files, GPU primitives, and hypergraph operations.

The same incidence matrix B that stores the knowledge graph
is the same structure that routes gradients during backprop.

Database queries and neural network training are the same
kernel (SpMM). Different semantics. Same execution.

This is what GPU-native computing looks like."
```

**THAT is the launch message.** Not "faster than PyTorch." Not "easier to use."

**"We built PyTorch in OctoFlow to prove the language is real."**

---

## Implementation Roadmap

See `docs/domain-foundation-map.md` for full phase details.

**Phase 47:** Sparse matrix primitives (CSR, SpMM, transpose)
**Phase 48:** Type system (entity-relation types, polymorphism)
**Phase 49:** Incidence matrix + scatter/gather
**Phase 50:** Pattern matching queries + message passing
**Phase 51:** Neural network layers (GCN, GAT, attention)
**Phase 52:** Autograd, training loop, optimizers → **PUBLIC RELEASE**

**Target:** ~2,000 lines of .flow code in `stdlib/ml/`
**Tests:** ~100 across 6 phases
**Outcome:** Full neural network framework competitive with PyTorch for common architectures.

---

*"In Python, you import PyTorch. In OctoFlow, PyTorch is just .flow modules. The hypergraph is the framework."*
