//! Compute pipeline creation, descriptor binding, dispatch, and readback.
//!
//! Phase 119: Consolidated from dispatch.rs + gpu_ops.rs + matmul.rs + types.rs.
//! All GPU operation types, kernel routing, and dispatch functions in one module.

use crate::device::{VulkanCompute, VulkanError};
use crate::vk_sys::*;

// ── Operation enums ─────────────────────────────────────────────────────────
// Previously in types.rs. Used for dispatch routing and CPU fallbacks.

/// Element-wise unary map operation.
#[derive(Copy, Clone, Debug)]
pub enum MapOp {
    Multiply(f32),
    Add(f32),
    Subtract(f32),
    Divide(f32),
    Abs,
    Sqrt,
    Negate,
    Mod(f32),
    Pow(f32),
    Exp,
    Log,
    Floor,
    Ceil,
    Round,
    Min(f32),
    Max(f32),
    Clamp(f32, f32),
    Sin,
    Cos,
}

/// Element-wise binary operation on two arrays.
#[derive(Copy, Clone, Debug)]
pub enum BinaryOp {
    Add,
    Sub,
    Mul,
    Div,
}

/// Reduction operation to scalar.
#[derive(Copy, Clone, Debug)]
pub enum ReduceOp {
    Sum,
    Min,
    Max,
    Mul,
}

impl ReduceOp {
    /// Identity element for padding.
    pub fn identity(self) -> f32 {
        match self {
            ReduceOp::Sum => 0.0,
            ReduceOp::Min => f32::INFINITY,
            ReduceOp::Max => f32::NEG_INFINITY,
            ReduceOp::Mul => 1.0,
        }
    }
}

/// Fused multi-op kernel (combined in one dispatch).
#[derive(Copy, Clone, Debug)]
pub enum FusedOp {
    Normalize { min: f32, max: f32 },
    ScaleShift { scale: f32, bias: f32 },
}

/// Temporal (sequential) operation.
#[derive(Copy, Clone, Debug)]
pub enum TemporalOp {
    Ema(f32),
    Decay(f32),
}

// ── Embedded SPIR-V kernels ─────────────────────────────────────────────────
// Previously in gpu_ops.rs. Generated by ir.flow (self-hosted SPIR-V emitter).

// Binary ops (2 inputs → 1 output)
static KERNEL_BINOP_ADD: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/add.spv");
static KERNEL_BINOP_SUB: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/sub.spv");
static KERNEL_BINOP_MUL: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/mul.spv");
static KERNEL_BINOP_DIV: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/div.spv");

// Select (3 inputs → 1 output)
static KERNEL_SELECT: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/where.spv");

// Unary map ops (no parameters)
static KERNEL_ABS: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/abs.spv");
static KERNEL_SQRT: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/sqrt.spv");
static KERNEL_NEGATE: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/negate.spv");
static KERNEL_EXP: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/exp.spv");
static KERNEL_LOG: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/log.spv");
static KERNEL_FLOOR: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/floor.spv");
static KERNEL_CEIL: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/ceil.spv");
static KERNEL_ROUND: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/round.spv");
static KERNEL_SIN: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/sin.spv");
static KERNEL_COS: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/cos.spv");

// Unary map ops (push-constant parameterized)
static KERNEL_MULTIPLY_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/multiply_pc.spv");
static KERNEL_ADD_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/add_pc.spv");
static KERNEL_SUBTRACT_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/subtract_pc.spv");
static KERNEL_DIVIDE_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/divide_pc.spv");
static KERNEL_MOD_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/mod_pc.spv");
static KERNEL_POW_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/pow_pc.spv");
static KERNEL_MIN_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/min_pc.spv");
static KERNEL_MAX_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/max_pc.spv");
static KERNEL_CLAMP_PC: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/clamp_pc.spv");

// ── Kernel routing ──────────────────────────────────────────────────────────

fn binop_kernel(op: BinaryOp) -> &'static [u8] {
    match op {
        BinaryOp::Add => KERNEL_BINOP_ADD,
        BinaryOp::Sub => KERNEL_BINOP_SUB,
        BinaryOp::Mul => KERNEL_BINOP_MUL,
        BinaryOp::Div => KERNEL_BINOP_DIV,
    }
}

/// Returns (kernel_bytes, push_constants) for a map operation.
fn map_op_kernel(op: MapOp) -> (&'static [u8], Vec<f32>) {
    match op {
        MapOp::Abs => (KERNEL_ABS, vec![]),
        MapOp::Sqrt => (KERNEL_SQRT, vec![]),
        MapOp::Negate => (KERNEL_NEGATE, vec![]),
        MapOp::Exp => (KERNEL_EXP, vec![]),
        MapOp::Log => (KERNEL_LOG, vec![]),
        MapOp::Floor => (KERNEL_FLOOR, vec![]),
        MapOp::Ceil => (KERNEL_CEIL, vec![]),
        MapOp::Round => (KERNEL_ROUND, vec![]),
        MapOp::Sin => (KERNEL_SIN, vec![]),
        MapOp::Cos => (KERNEL_COS, vec![]),
        MapOp::Multiply(s) => (KERNEL_MULTIPLY_PC, vec![s]),
        MapOp::Add(s) => (KERNEL_ADD_PC, vec![s]),
        MapOp::Subtract(s) => (KERNEL_SUBTRACT_PC, vec![s]),
        MapOp::Divide(s) => (KERNEL_DIVIDE_PC, vec![s]),
        MapOp::Mod(s) => (KERNEL_MOD_PC, vec![s]),
        MapOp::Pow(s) => (KERNEL_POW_PC, vec![s]),
        MapOp::Min(s) => (KERNEL_MIN_PC, vec![s]),
        MapOp::Max(s) => (KERNEL_MAX_PC, vec![s]),
        MapOp::Clamp(lo, hi) => (KERNEL_CLAMP_PC, vec![lo, hi]),
    }
}

/// Cached compiled Vulkan pipeline — reused across calls with the same SPIR-V.
/// All fields are `u64` non-dispatchable handles; `Copy` for cheap extraction.
#[derive(Copy, Clone)]
struct CachedPipeline {
    shader_module: VkShaderModule,
    ds_layout: VkDescriptorSetLayout,
    pipeline_layout: VkPipelineLayout,
    pipeline: VkPipeline,
}

thread_local! {
    /// Pipeline cache keyed by `(device_ptr, spirv_hash, binding_count)`.
    ///
    /// Eliminates `vkCreateShaderModule` + `vkCreateComputePipelines` on every
    /// dispatch — the most expensive Vulkan calls (~10-15 ms on first call).
    /// Entries live for the process lifetime (intentional; GPU leak-free on exit).
    static PIPELINE_CACHE: std::cell::RefCell<
        std::collections::HashMap<(u64, u64, u32), CachedPipeline>
    > = std::cell::RefCell::new(std::collections::HashMap::new());

    /// Buffer pool: (device_ptr, size_bytes, mem_flags) → Vec<(VkBuffer, VkDeviceMemory)>.
    /// Eliminates vkCreateBuffer + vkAllocateMemory on repeated dispatches.
    static BUFFER_POOL: std::cell::RefCell<
        std::collections::HashMap<(u64, u64, u64), Vec<(VkBuffer, VkDeviceMemory)>>
    > = std::cell::RefCell::new(std::collections::HashMap::new());

    /// Reusable command pool per device — reset between dispatches.
    static CMD_POOL_CACHE: std::cell::RefCell<Option<(u64, VkCommandPool)>>
        = std::cell::RefCell::new(None);

    /// Reusable fence per device — reset between dispatches.
    static FENCE_CACHE: std::cell::RefCell<Option<(u64, VkFence)>>
        = std::cell::RefCell::new(None);

    /// Descriptor pool cache: (device_ptr, max_descriptors) → VkDescriptorPool.
    /// Reset with vkResetDescriptorPool between dispatches.
    static DESC_POOL_CACHE: std::cell::RefCell<
        std::collections::HashMap<(u64, u32), VkDescriptorPool>
    > = std::cell::RefCell::new(std::collections::HashMap::new());
}

// ── Deferred dispatch queue (Phase 80) ────────────────────────────────────────

/// A queued GPU dispatch waiting for batch execution.
struct PendingDispatch {
    pipeline: VkPipeline,
    pipeline_layout: VkPipelineLayout,
    ds_layout: VkDescriptorSetLayout,
    input_buffers: Vec<(VkBuffer, u64)>,    // (handle, size_bytes)
    output_buffers: Vec<(VkBuffer, u64)>,   // pre-allocated from pool
    n_total_bindings: u32,
    wg_x: u32,
}

thread_local! {
    static PENDING_DISPATCHES: std::cell::RefCell<Vec<PendingDispatch>>
        = std::cell::RefCell::new(Vec::new());
}

/// FNV-1a 64-bit hash of SPIR-V bytes used as the pipeline cache key.
pub(crate) fn spirv_hash(spirv: &[u8]) -> u64 {
    let mut h: u64 = 0xcbf29ce484222325;
    for &b in spirv {
        h ^= b as u64;
        h = h.wrapping_mul(0x100000001b3);
    }
    h
}

// ── Resource pool helpers ────────────────────────────────────────────────────

/// Acquire a buffer from the pool or create a new one.
/// On OOM, flushes the entire buffer pool and retries once.
pub(crate) fn acquire_buffer(
    gpu: &VulkanCompute, size: u64, usage: VkFlags, mem_flags: VkFlags,
) -> Result<(VkBuffer, VkDeviceMemory), VulkanError> {
    let key = (gpu.device as usize as u64, size, mem_flags as u64);
    let cached = BUFFER_POOL.with(|pool| {
        pool.borrow_mut().get_mut(&key).and_then(|v| v.pop())
    });
    if let Some(pair) = cached {
        return Ok(pair);
    }
    match gpu.create_buffer(size, usage, mem_flags) {
        Ok(pair) => Ok(pair),
        Err(VulkanError::Vk(VK_ERROR_OUT_OF_DEVICE_MEMORY)) => {
            // OOM recovery: flush entire pool, free all cached buffers, retry once
            BUFFER_POOL.with(|pool| {
                let mut p = pool.borrow_mut();
                for (_key, entries) in p.drain() {
                    for (buf, mem) in entries {
                        unsafe {
                            vkFreeMemory(gpu.device, mem, std::ptr::null());
                            vkDestroyBuffer(gpu.device, buf, std::ptr::null());
                        }
                    }
                }
            });
            gpu.create_buffer(size, usage, mem_flags)
        }
        Err(e) => Err(e),
    }
}

/// Return a buffer to the pool for future reuse.
pub(crate) fn release_buffer(device: VkDevice, size: u64, mem_flags: VkFlags, buf: VkBuffer, mem: VkDeviceMemory) {
    let key = (device as usize as u64, size, mem_flags as u64);
    BUFFER_POOL.with(|pool| {
        let mut p = pool.borrow_mut();
        let v = p.entry(key).or_insert_with(Vec::new);
        // Cap pool depth to prevent unbounded memory growth
        if v.len() < 8 {
            v.push((buf, mem));
        } else {
            // Pool full — free this one
            unsafe {
                vkFreeMemory(device, mem, std::ptr::null());
                vkDestroyBuffer(device, buf, std::ptr::null());
            }
        }
    });
}

// ── GPU-resident buffer types (Phase 79) ─────────────────────────────────────

/// A GPU-resident VkBuffer handle. Owns the Vulkan resource; Drop returns it
/// to the buffer pool. Used by GPU_ARRAYS to keep data in VRAM between ops.
pub struct GpuBuffer {
    pub(crate) buffer: VkBuffer,
    pub(crate) memory: VkDeviceMemory,
    pub(crate) device: VkDevice,
    pub(crate) size_bytes: u64,
    pub(crate) len: usize,
    pub(crate) mem_flags: VkFlags,
}

impl Drop for GpuBuffer {
    fn drop(&mut self) {
        // Try to return buffer to pool for reuse.
        // During thread-local destruction or after VulkanCompute is dropped,
        // the device handle may be invalid — in that case, do nothing.
        // vkDestroyDevice implicitly frees all device resources.
        let key = (self.device as usize as u64, self.size_bytes, self.mem_flags as u64);
        let _ = BUFFER_POOL.try_with(|pool| {
            let mut p = pool.borrow_mut();
            let v = p.entry(key).or_insert_with(Vec::new);
            if v.len() < 8 {
                v.push((self.buffer, self.memory));
            }
            // Pool full: resource leaks until vkDestroyDevice (acceptable)
        });
        // If try_with fails (TLS destroyed): device is gone, resources already freed
    }
}

impl GpuBuffer {
    /// Lightweight Copy handle for passing to dispatch without ownership.
    pub fn as_ref(&self) -> GpuBufferRef {
        GpuBufferRef {
            buffer: self.buffer,
            size_bytes: self.size_bytes,
            len: self.len,
        }
    }

    /// Number of f32 elements.
    pub fn len(&self) -> usize { self.len }

    /// Download all contents to CPU. Uses the stored device handle — no VulkanCompute needed.
    pub fn download(&self) -> Result<Vec<f32>, String> {
        let size = (self.len * 4) as VkDeviceSize;
        let mut result = vec![0.0f32; self.len];
        unsafe {
            let mut ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let r = vkMapMemory(self.device, self.memory, 0, size, 0, &mut ptr);
            if r != VK_SUCCESS {
                return Err(format!("vkMapMemory failed (code {})", r));
            }
            std::ptr::copy_nonoverlapping(
                ptr as *const u8, result.as_mut_ptr() as *mut u8, size as usize,
            );
            vkUnmapMemory(self.device, self.memory);
        }
        Ok(result)
    }

    /// Read a single f32 element by index. No VulkanCompute needed.
    pub fn read_element(&self, idx: usize) -> Option<f32> {
        if idx >= self.len { return None; }
        let offset = (idx * 4) as VkDeviceSize;
        unsafe {
            let mut ptr: *mut std::ffi::c_void = std::ptr::null_mut();
            let r = vkMapMemory(self.device, self.memory, offset, 4, 0, &mut ptr);
            if r == VK_SUCCESS {
                let val = *(ptr as *const f32);
                vkUnmapMemory(self.device, self.memory);
                Some(val)
            } else {
                None
            }
        }
    }
}

/// Lightweight Copy handle extracted from GpuBuffer for dispatch binding.
/// Does NOT own the Vulkan resource — the source GpuBuffer must outlive usage.
#[derive(Copy, Clone)]
pub struct GpuBufferRef {
    pub buffer: VkBuffer,
    pub size_bytes: u64,
    pub len: usize,
}

/// Acquire a reusable descriptor pool (reset between dispatches).
fn acquire_descriptor_pool(gpu: &VulkanCompute, descriptor_count: u32) -> Result<VkDescriptorPool, VulkanError> {
    let dev_key = gpu.device as usize as u64;
    let cached = DESC_POOL_CACHE.with(|c| c.borrow().get(&(dev_key, descriptor_count)).copied());
    if let Some(pool) = cached {
        vk_check(unsafe {
            vkResetDescriptorPool(gpu.device, pool, 0)
        }).map_err(VulkanError::Vk)?;
        return Ok(pool);
    }
    let pool_size = VkDescriptorPoolSize {
        ty: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
        descriptorCount: descriptor_count,
    };
    let pool_info = VkDescriptorPoolCreateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO,
        pNext: std::ptr::null(),
        flags: 0,
        maxSets: 1,
        poolSizeCount: 1,
        pPoolSizes: &pool_size,
    };
    let mut pool: VkDescriptorPool = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkCreateDescriptorPool(gpu.device, &pool_info, std::ptr::null(), &mut pool)
    }).map_err(VulkanError::Vk)?;
    DESC_POOL_CACHE.with(|c| c.borrow_mut().insert((dev_key, descriptor_count), pool));
    Ok(pool)
}

/// Detect whether this GPU needs staging buffers (discrete GPU without unified memory).
fn should_use_staging(gpu: &VulkanCompute) -> bool {
    // Check if there's a memory type that is DEVICE_LOCAL but NOT HOST_VISIBLE.
    // If so, we have a discrete GPU and should use staging for VRAM-speed compute.
    let count = gpu.memory_properties.memoryTypeCount as usize;
    for i in 0..count {
        let flags = gpu.memory_properties.memoryTypes[i].propertyFlags;
        if flags & VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT != 0
            && flags & VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT == 0
        {
            return true;
        }
    }
    false
}

/// Acquire a reusable command pool (reset between dispatches).
pub(crate) fn acquire_command_pool(gpu: &VulkanCompute) -> Result<VkCommandPool, VulkanError> {
    let dev_key = gpu.device as usize as u64;
    let cached = CMD_POOL_CACHE.with(|c| *c.borrow());
    if let Some((k, pool)) = cached {
        if k == dev_key {
            vk_check(unsafe {
                vkResetCommandPool(gpu.device, pool, 0)
            }).map_err(VulkanError::Vk)?;
            return Ok(pool);
        }
    }
    let cmd_pool_info = VkCommandPoolCreateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_POOL_CREATE_INFO,
        pNext: std::ptr::null(),
        flags: 0,
        queueFamilyIndex: gpu.queue_family_index,
    };
    let mut command_pool: VkCommandPool = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkCreateCommandPool(gpu.device, &cmd_pool_info, std::ptr::null(), &mut command_pool)
    }).map_err(VulkanError::Vk)?;
    CMD_POOL_CACHE.with(|c| *c.borrow_mut() = Some((dev_key, command_pool)));
    Ok(command_pool)
}

/// Acquire a reusable fence (reset between dispatches).
pub(crate) fn acquire_fence(gpu: &VulkanCompute) -> Result<VkFence, VulkanError> {
    let dev_key = gpu.device as usize as u64;
    let cached = FENCE_CACHE.with(|c| *c.borrow());
    if let Some((k, fence)) = cached {
        if k == dev_key {
            vk_check(unsafe {
                vkResetFences(gpu.device, 1, &fence)
            }).map_err(VulkanError::Vk)?;
            return Ok(fence);
        }
    }
    let fence_info = VkFenceCreateInfo {
        sType: VK_STRUCTURE_TYPE_FENCE_CREATE_INFO,
        pNext: std::ptr::null(),
        flags: 0,
    };
    let mut fence: VkFence = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkCreateFence(gpu.device, &fence_info, std::ptr::null(), &mut fence)
    }).map_err(VulkanError::Vk)?;
    FENCE_CACHE.with(|c| *c.borrow_mut() = Some((dev_key, fence)));
    Ok(fence)
}

// ── GPU-resident buffer operations (Phase 79) ───────────────────────────────

const RESIDENT_MEM_FLAGS: VkFlags = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
const RESIDENT_USAGE: VkFlags = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT | VK_BUFFER_USAGE_TRANSFER_SRC_BIT;

/// Upload CPU f32 data to a HOST_VISIBLE VkBuffer, returning a GpuBuffer handle.
pub fn upload_buffer(gpu: &VulkanCompute, data: &[f32]) -> Result<GpuBuffer, VulkanError> {
    let size_bytes = (data.len() * 4) as u64;
    let (buffer, memory) = acquire_buffer(gpu, size_bytes, RESIDENT_USAGE, RESIDENT_MEM_FLAGS)?;
    gpu.upload_f32(memory, data)?;
    Ok(GpuBuffer {
        buffer, memory, device: gpu.device, size_bytes,
        len: data.len(), mem_flags: RESIDENT_MEM_FLAGS,
    })
}

/// Convert f32 to IEEE 754 half-precision (f16) stored as u16.
#[inline]
pub fn f32_to_f16(val: f32) -> u16 {
    let bits = val.to_bits();
    let sign = (bits >> 16) & 0x8000;
    let exp = ((bits >> 23) & 0xFF) as i32;
    let man = bits & 0x007FFFFF;

    if exp == 255 {
        // Inf/NaN
        return (sign | 0x7C00 | if man != 0 { 0x0200 } else { 0 }) as u16;
    }
    let new_exp = exp - 127 + 15;
    if new_exp >= 31 {
        return (sign | 0x7C00) as u16; // overflow -> Inf
    }
    if new_exp <= 0 {
        if new_exp < -10 {
            return sign as u16; // too small -> 0
        }
        // Denormalized
        let man_with_hidden = man | 0x00800000;
        let shift = 1 - new_exp;
        let half_man = man_with_hidden >> (13 + shift);
        return (sign | half_man) as u16;
    }
    (sign | ((new_exp as u32) << 10) | (man >> 13)) as u16
}

/// Convert IEEE 754 half-precision (f16) u16 to f32.
#[inline]
pub fn f16_to_f32(val: u16) -> f32 {
    let sign = ((val as u32 & 0x8000) as u32) << 16;
    let exp = (val >> 10) & 0x1F;
    let man = (val & 0x03FF) as u32;

    if exp == 0 {
        if man == 0 {
            return f32::from_bits(sign); // +/- zero
        }
        // Denormalized: normalize
        let mut e = 0i32;
        let mut m = man;
        while m & 0x0400 == 0 {
            m <<= 1;
            e += 1;
        }
        let new_exp = (127 - 15 + 1 - e) as u32;
        let new_man = (m & 0x03FF) << 13;
        return f32::from_bits(sign | (new_exp << 23) | new_man);
    }
    if exp == 31 {
        // Inf/NaN
        return f32::from_bits(sign | 0x7F800000 | (man << 13));
    }
    let new_exp = ((exp as u32) + 127 - 15) << 23;
    f32::from_bits(sign | new_exp | (man << 13))
}

/// Upload f32 data to GPU as f16 (half-precision), saving 50% VRAM.
/// The GPU buffer stores u16 values. Shader must use float16_t or unpackHalf2x16.
/// `len` in the returned GpuBuffer is the number of f16 elements (same as data.len()).
pub fn upload_buffer_f16(gpu: &VulkanCompute, data: &[f32]) -> Result<GpuBuffer, VulkanError> {
    let f16_data: Vec<u16> = data.iter().map(|&v| f32_to_f16(v)).collect();
    let size_bytes = (f16_data.len() * 2) as u64;
    let (buffer, memory) = acquire_buffer(gpu, size_bytes, RESIDENT_USAGE, RESIDENT_MEM_FLAGS)?;
    // Upload raw u16 bytes
    unsafe {
        let mut ptr: *mut std::ffi::c_void = std::ptr::null_mut();
        let result = vkMapMemory(gpu.device, memory, 0, size_bytes, 0, &mut ptr);
        vk_check(result).map_err(|_| VulkanError::MapFailed)?;
        std::ptr::copy_nonoverlapping(
            f16_data.as_ptr() as *const u8,
            ptr as *mut u8,
            size_bytes as usize,
        );
        vkUnmapMemory(gpu.device, memory);
    }
    Ok(GpuBuffer {
        buffer, memory, device: gpu.device, size_bytes,
        len: data.len(), mem_flags: RESIDENT_MEM_FLAGS,
    })
}

/// Upload CPU f32 data to HOST_VISIBLE buffer with extra usage flags.
/// Like upload_buffer() but adds extra_usage (e.g. INDIRECT_BUFFER_BIT for Control).
pub fn upload_buffer_with_usage(gpu: &VulkanCompute, data: &[f32], extra_usage: VkFlags) -> Result<GpuBuffer, VulkanError> {
    let size_bytes = (data.len() * 4) as u64;
    let usage = RESIDENT_USAGE | extra_usage;
    let (buffer, memory) = acquire_buffer(gpu, size_bytes, usage, RESIDENT_MEM_FLAGS)?;
    gpu.upload_f32(memory, data)?;
    Ok(GpuBuffer {
        buffer, memory, device: gpu.device, size_bytes,
        len: data.len(), mem_flags: RESIDENT_MEM_FLAGS,
    })
}

/// Upload CPU f32 data to DEVICE_LOCAL VRAM via staging buffer.
/// Uses HOST_VISIBLE staging for the DMA copy, then frees staging.
/// Result buffer is in fast VRAM (not HOST_VISIBLE BAR memory).
pub fn upload_buffer_vram(gpu: &VulkanCompute, data: &[f32]) -> Result<GpuBuffer, VulkanError> {
    let size_bytes = (data.len() * 4) as u64;
    let stg_usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
    let stg_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
    let dev_usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
        | VK_BUFFER_USAGE_TRANSFER_DST_BIT
        | VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
    let dev_mem = VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;

    // Allocate staging (HOST_VISIBLE) and upload data
    let (stg_buf, stg_memory) = acquire_buffer(gpu, size_bytes, stg_usage, stg_mem)?;
    gpu.upload_f32(stg_memory, data)?;

    // Allocate device-local (VRAM) target
    let (dev_buf, dev_memory) = acquire_buffer(gpu, size_bytes, dev_usage, dev_mem)?;

    // DMA copy staging → device-local via command buffer
    let command_pool = acquire_command_pool(gpu)?;
    let fence = acquire_fence(gpu)?;
    unsafe {
        let alloc_info = VkCommandBufferAllocateInfo {
            sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
            pNext: std::ptr::null(),
            commandPool: command_pool,
            level: 0, // PRIMARY
            commandBufferCount: 1,
        };
        let mut cmd: VkCommandBuffer = std::ptr::null_mut();
        vk_check(vkAllocateCommandBuffers(gpu.device, &alloc_info, &mut cmd))
            .map_err(VulkanError::Vk)?;

        let begin_info = VkCommandBufferBeginInfo {
            sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
            pNext: std::ptr::null(),
            flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
            pInheritanceInfo: std::ptr::null(),
        };
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;

        let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: size_bytes };
        vkCmdCopyBuffer(cmd, stg_buf, dev_buf, 1, &copy);

        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;

        let submit = VkSubmitInfo {
            sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
            pNext: std::ptr::null(),
            waitSemaphoreCount: 0,
            pWaitSemaphores: std::ptr::null(),
            pWaitDstStageMask: std::ptr::null(),
            commandBufferCount: 1,
            pCommandBuffers: &cmd,
            signalSemaphoreCount: 0,
            pSignalSemaphores: std::ptr::null(),
        };
        gpu.queue_submit(1, &submit, fence)?;
        vk_check(vkWaitForFences(gpu.device, 1, &fence, 1, u64::MAX)).map_err(VulkanError::Vk)?;

        // Free staging buffer
        vkDestroyBuffer(gpu.device, stg_buf, std::ptr::null());
        vkFreeMemory(gpu.device, stg_memory, std::ptr::null());
    }

    Ok(GpuBuffer {
        buffer: dev_buf, memory: dev_memory, device: gpu.device, size_bytes,
        len: data.len(), mem_flags: dev_mem,
    })
}

/// Upload CPU f32 data to DEVICE_LOCAL VRAM with extra usage flags (e.g. INDIRECT_BUFFER_BIT).
pub fn upload_buffer_vram_with_usage(gpu: &VulkanCompute, data: &[f32], extra_usage: VkFlags) -> Result<GpuBuffer, VulkanError> {
    let size_bytes = (data.len() * 4) as u64;
    let stg_usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT;
    let stg_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
    let dev_usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
        | VK_BUFFER_USAGE_TRANSFER_DST_BIT
        | VK_BUFFER_USAGE_TRANSFER_SRC_BIT
        | extra_usage;
    let dev_mem = VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT;

    let (stg_buf, stg_memory) = acquire_buffer(gpu, size_bytes, stg_usage, stg_mem)?;
    gpu.upload_f32(stg_memory, data)?;
    let (dev_buf, dev_memory) = acquire_buffer(gpu, size_bytes, dev_usage, dev_mem)?;

    let command_pool = acquire_command_pool(gpu)?;
    let fence = acquire_fence(gpu)?;
    unsafe {
        let alloc_info = VkCommandBufferAllocateInfo {
            sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
            pNext: std::ptr::null(),
            commandPool: command_pool,
            level: 0,
            commandBufferCount: 1,
        };
        let mut cmd: VkCommandBuffer = std::ptr::null_mut();
        vk_check(vkAllocateCommandBuffers(gpu.device, &alloc_info, &mut cmd))
            .map_err(VulkanError::Vk)?;
        let begin_info = VkCommandBufferBeginInfo {
            sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
            pNext: std::ptr::null(),
            flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
            pInheritanceInfo: std::ptr::null(),
        };
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: size_bytes };
        vkCmdCopyBuffer(cmd, stg_buf, dev_buf, 1, &copy);
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
        let submit = VkSubmitInfo {
            sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
            pNext: std::ptr::null(),
            waitSemaphoreCount: 0, pWaitSemaphores: std::ptr::null(),
            pWaitDstStageMask: std::ptr::null(),
            commandBufferCount: 1, pCommandBuffers: &cmd,
            signalSemaphoreCount: 0, pSignalSemaphores: std::ptr::null(),
        };
        gpu.queue_submit(1, &submit, fence)?;
        vk_check(vkWaitForFences(gpu.device, 1, &fence, 1, u64::MAX)).map_err(VulkanError::Vk)?;
        vkDestroyBuffer(gpu.device, stg_buf, std::ptr::null());
        vkFreeMemory(gpu.device, stg_memory, std::ptr::null());
    }

    Ok(GpuBuffer {
        buffer: dev_buf, memory: dev_memory, device: gpu.device, size_bytes,
        len: data.len(), mem_flags: dev_mem,
    })
}

/// Download GpuBuffer contents to CPU Vec<f32>.
pub fn download_buffer(gpu: &VulkanCompute, buf: &GpuBuffer) -> Result<Vec<f32>, VulkanError> {
    gpu.download_f32(buf.memory, buf.len)
}

/// Read a single f32 element from a GpuBuffer (map, read, unmap).
pub fn read_buffer_element(gpu: &VulkanCompute, buf: &GpuBuffer, idx: usize) -> Result<f32, VulkanError> {
    if idx >= buf.len {
        return Err(VulkanError::MapFailed); // index OOB
    }
    let offset = (idx * 4) as VkDeviceSize;
    unsafe {
        let mut ptr: *mut std::ffi::c_void = std::ptr::null_mut();
        let result = vkMapMemory(gpu.device, buf.memory, offset, 4, 0, &mut ptr);
        vk_check(result).map_err(|_| VulkanError::MapFailed)?;
        let val = *(ptr as *const f32);
        vkUnmapMemory(gpu.device, buf.memory);
        Ok(val)
    }
}

/// Run a SPIR-V compute shader on the GPU.
///
/// Creates a complete Vulkan compute pipeline, uploads `input` to binding 0,
/// dispatches the shader, and reads back the output from binding 1.
///
/// `workgroup_size` must match the shader's LocalSize declaration.
pub fn dispatch_compute(
    gpu: &VulkanCompute,
    spirv: &[u8],
    input: &[f32],
    workgroup_size: u32,
) -> Result<Vec<f32>, VulkanError> {
    let device = gpu.device;
    let element_count = input.len();
    let buffer_size = (element_count * std::mem::size_of::<f32>()) as VkDeviceSize;

    // ── Pipeline (cached: shader + descriptor layout + compute pipeline) ────────
    // Cache key: (device ptr, FNV-1a SPIR-V hash, binding_count = 2).
    // On hit:  reuses compiled VkPipeline — skips vkCreateComputePipelines.
    // On miss: compiles once, inserts into cache, lives for process lifetime.
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    let cache_key = (device as usize as u64, spirv_hash(spirv), 2u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (_shader_module, ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.shader_module, c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;

        let bindings = [
            VkDescriptorSetLayoutBinding {
                binding: 0,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            },
            VkDescriptorSetLayoutBinding {
                binding: 1,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            },
        ];
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: 2,
            pBindings: bindings.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;

        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: 0,
            pPushConstantRanges: std::ptr::null(),
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;

        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;

        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (shader_module, ds_layout, pipeline_layout, pipeline)
    };
    let layouts = [ds_layout];

    // ── Buffers (pooled: reused across dispatches) ──────────────────────────
    let host_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
    let discrete = should_use_staging(gpu);
    let out_usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
        | if discrete { VK_BUFFER_USAGE_TRANSFER_SRC_BIT } else { 0 };
    let (input_buffer, input_memory) = acquire_buffer(
        gpu, buffer_size, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT, host_mem)?;
    let (output_buffer, output_memory) = acquire_buffer(
        gpu, buffer_size, out_usage, host_mem)?;

    // Readback staging: HOST_CACHED for fast CPU reads on discrete GPUs
    let rb_mem_flags = {
        let cached = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
        let has_cached = {
            let count = gpu.memory_properties.memoryTypeCount as usize;
            (0..count).any(|i| {
                let f = gpu.memory_properties.memoryTypes[i].propertyFlags;
                f & cached == cached
            })
        };
        if has_cached { cached } else { host_mem }
    };
    let (rb_buf, rb_mem) = if discrete {
        let pair = acquire_buffer(gpu, buffer_size, VK_BUFFER_USAGE_TRANSFER_DST_BIT, rb_mem_flags)?;
        (Some(pair.0), Some(pair.1))
    } else {
        (None, None)
    };

    gpu.upload_f32(input_memory, input)?;

    // ── Descriptor pool & set (cached pool) ──────────────────────────────────
    let descriptor_pool = acquire_descriptor_pool(gpu, 2)?;

    let alloc_info = VkDescriptorSetAllocateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        descriptorPool: descriptor_pool,
        descriptorSetCount: 1,
        pSetLayouts: layouts.as_ptr(),
    };
    let mut descriptor_set: VkDescriptorSet = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkAllocateDescriptorSets(device, &alloc_info, &mut descriptor_set)
    }).map_err(VulkanError::Vk)?;

    // Bind buffers to descriptor set
    let input_buf_info = VkDescriptorBufferInfo {
        buffer: input_buffer,
        offset: 0,
        range: buffer_size,
    };
    let output_buf_info = VkDescriptorBufferInfo {
        buffer: output_buffer,
        offset: 0,
        range: buffer_size,
    };
    let writes = [
        VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
            pNext: std::ptr::null(),
            dstSet: descriptor_set,
            dstBinding: 0,
            dstArrayElement: 0,
            descriptorCount: 1,
            descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(),
            pBufferInfo: &input_buf_info,
            pTexelBufferView: std::ptr::null(),
        },
        VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
            pNext: std::ptr::null(),
            dstSet: descriptor_set,
            dstBinding: 1,
            dstArrayElement: 0,
            descriptorCount: 1,
            descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(),
            pBufferInfo: &output_buf_info,
            pTexelBufferView: std::ptr::null(),
        },
    ];
    unsafe { vkUpdateDescriptorSets(device, 2, writes.as_ptr(), 0, std::ptr::null()); }

    // ── Command buffer (pooled command pool) ──────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;

    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };

    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(
            cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline_layout,
            0, 1, &descriptor_set, 0, std::ptr::null(),
        );
        let workgroup_count = ((element_count as u32) + workgroup_size - 1) / workgroup_size;
        vkCmdDispatch(cmd, workgroup_count, 1, 1);

        // Readback staging: DMA output → HOST_CACHED buffer
        if let Some(rb) = rb_buf {
            let barrier = VkBufferMemoryBarrier {
                sType: VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
                pNext: std::ptr::null(),
                srcAccessMask: VK_ACCESS_SHADER_WRITE_BIT,
                dstAccessMask: VK_ACCESS_TRANSFER_READ_BIT,
                srcQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                dstQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                buffer: output_buffer,
                offset: 0,
                size: VK_WHOLE_SIZE,
            };
            vkCmdPipelineBarrier(
                cmd,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_TRANSFER_BIT,
                0, 0, std::ptr::null(),
                1, &barrier,
                0, std::ptr::null(),
            );
            let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: buffer_size };
            vkCmdCopyBuffer(cmd, output_buffer, rb, 1, &copy);
        }

        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    // ── Submit & wait (pooled fence) ──────────────────────────────────────────
    let fence = acquire_fence(gpu)?;

    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0,
        pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1,
        pCommandBuffers: &cmd,
        signalSemaphoreCount: 0,
        pSignalSemaphores: std::ptr::null(),
    };

    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Readback ──────────────────────────────────────────────────────────────
    let read_from = if let Some(m) = rb_mem { m } else { output_memory };
    let output = gpu.download_f32(read_from, element_count)?;

    // ── Cleanup ───────────────────────────────────────────────────────────────
    // Pipeline, command pool, and fence are retained in caches.
    // Buffers return to pool. Descriptor pool is cached.
    release_buffer(device, buffer_size, host_mem, input_buffer, input_memory);
    release_buffer(device, buffer_size, host_mem, output_buffer, output_memory);
    if let (Some(b), Some(m)) = (rb_buf, rb_mem) {
        release_buffer(device, buffer_size, rb_mem_flags, b, m);
    }

    Ok(output)
}

/// Like `dispatch_compute`, but passes push constants to the shader.
///
/// Push constants are declared in the SPIR-V as a PushConstant-decorated struct.
/// Values are uploaded per-dispatch via `vkCmdPushConstants` (zero-copy, no buffer).
///
/// This replaces runtime SPIR-V generation for parameterized ops: instead of baking
/// a scalar into the SPIR-V binary, the pre-built kernel reads it from push constants.
pub fn dispatch_compute_pc(
    gpu: &VulkanCompute,
    spirv: &[u8],
    input: &[f32],
    workgroup_size: u32,
    push_constants: &[f32],
) -> Result<Vec<f32>, VulkanError> {
    let device = gpu.device;
    let element_count = input.len();
    let buffer_size = (element_count * std::mem::size_of::<f32>()) as VkDeviceSize;
    let pc_bytes = (push_constants.len() * 4) as u32;

    // ── Pipeline (cached) ────────────────────────────────────────────────────
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    let cache_key = (device as usize as u64, spirv_hash(spirv), 2u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (_shader_module, ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.shader_module, c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;

        let bindings = [
            VkDescriptorSetLayoutBinding {
                binding: 0,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            },
            VkDescriptorSetLayoutBinding {
                binding: 1,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            },
        ];
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: 2,
            pBindings: bindings.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;

        let pc_range = VkPushConstantRange {
            stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
            offset: 0,
            size: pc_bytes,
        };
        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: if pc_bytes > 0 { 1 } else { 0 },
            pPushConstantRanges: if pc_bytes > 0 { &pc_range } else { std::ptr::null() },
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;

        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;

        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (shader_module, ds_layout, pipeline_layout, pipeline)
    };
    let layouts = [ds_layout];

    // ── Buffers ──────────────────────────────────────────────────────────────
    let host_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
    let discrete = should_use_staging(gpu);
    let out_usage = VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
        | if discrete { VK_BUFFER_USAGE_TRANSFER_SRC_BIT } else { 0 };
    let (input_buffer, input_memory) = acquire_buffer(
        gpu, buffer_size, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT, host_mem)?;
    let (output_buffer, output_memory) = acquire_buffer(
        gpu, buffer_size, out_usage, host_mem)?;

    let rb_mem_flags = {
        let cached = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
        let has_cached = {
            let count = gpu.memory_properties.memoryTypeCount as usize;
            (0..count).any(|i| {
                let f = gpu.memory_properties.memoryTypes[i].propertyFlags;
                f & cached == cached
            })
        };
        if has_cached { cached } else { host_mem }
    };
    let (rb_buf, rb_mem) = if discrete {
        let pair = acquire_buffer(gpu, buffer_size, VK_BUFFER_USAGE_TRANSFER_DST_BIT, rb_mem_flags)?;
        (Some(pair.0), Some(pair.1))
    } else {
        (None, None)
    };

    gpu.upload_f32(input_memory, input)?;

    // ── Descriptor ──────────────────────────────────────────────────────────
    let descriptor_pool = acquire_descriptor_pool(gpu, 2)?;
    let alloc_info = VkDescriptorSetAllocateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        descriptorPool: descriptor_pool,
        descriptorSetCount: 1,
        pSetLayouts: layouts.as_ptr(),
    };
    let mut descriptor_set: VkDescriptorSet = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkAllocateDescriptorSets(device, &alloc_info, &mut descriptor_set)
    }).map_err(VulkanError::Vk)?;

    let input_buf_info = VkDescriptorBufferInfo { buffer: input_buffer, offset: 0, range: buffer_size };
    let output_buf_info = VkDescriptorBufferInfo { buffer: output_buffer, offset: 0, range: buffer_size };
    let writes = [
        VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET, pNext: std::ptr::null(),
            dstSet: descriptor_set, dstBinding: 0, dstArrayElement: 0,
            descriptorCount: 1, descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(), pBufferInfo: &input_buf_info, pTexelBufferView: std::ptr::null(),
        },
        VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET, pNext: std::ptr::null(),
            dstSet: descriptor_set, dstBinding: 1, dstArrayElement: 0,
            descriptorCount: 1, descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(), pBufferInfo: &output_buf_info, pTexelBufferView: std::ptr::null(),
        },
    ];
    unsafe { vkUpdateDescriptorSets(device, 2, writes.as_ptr(), 0, std::ptr::null()); }

    // ── Command buffer ──────────────────────────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;
    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };

    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(
            cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline_layout,
            0, 1, &descriptor_set, 0, std::ptr::null(),
        );
        if pc_bytes > 0 {
            vkCmdPushConstants(
                cmd, pipeline_layout, VK_SHADER_STAGE_COMPUTE_BIT,
                0, pc_bytes, push_constants.as_ptr() as *const _,
            );
        }
        let workgroup_count = ((element_count as u32) + workgroup_size - 1) / workgroup_size;
        vkCmdDispatch(cmd, workgroup_count, 1, 1);

        if let Some(rb) = rb_buf {
            let barrier = VkBufferMemoryBarrier {
                sType: VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
                pNext: std::ptr::null(),
                srcAccessMask: VK_ACCESS_SHADER_WRITE_BIT,
                dstAccessMask: VK_ACCESS_TRANSFER_READ_BIT,
                srcQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                dstQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                buffer: output_buffer,
                offset: 0,
                size: VK_WHOLE_SIZE,
            };
            vkCmdPipelineBarrier(
                cmd,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_TRANSFER_BIT,
                0, 0, std::ptr::null(),
                1, &barrier,
                0, std::ptr::null(),
            );
            let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: buffer_size };
            vkCmdCopyBuffer(cmd, output_buffer, rb, 1, &copy);
        }
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    // ── Submit & wait ────────────────────────────────────────────────────────
    let fence = acquire_fence(gpu)?;
    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0, pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1, pCommandBuffers: &cmd,
        signalSemaphoreCount: 0, pSignalSemaphores: std::ptr::null(),
    };
    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Readback ────────────────────────────────────────────────────────────
    let read_from = if let Some(m) = rb_mem { m } else { output_memory };
    let output = gpu.download_f32(read_from, element_count)?;

    // ── Cleanup ─────────────────────────────────────────────────────────────
    release_buffer(device, buffer_size, host_mem, input_buffer, input_memory);
    release_buffer(device, buffer_size, host_mem, output_buffer, output_memory);
    if let (Some(b), Some(m)) = (rb_buf, rb_mem) {
        release_buffer(device, buffer_size, rb_mem_flags, b, m);
    }

    Ok(output)
}

/// Buffer specification for N-buffer dispatch.
pub struct BufferSpec<'a> {
    /// Size in bytes.
    pub size_bytes: u64,
    /// Optional initial data to upload. If None, buffer is uninitialized.
    pub initial_data: Option<&'a [f32]>,
}

/// Generic N-buffer dispatch: creates N storage buffers at bindings 0..N-1,
/// dispatches the shader, and reads back specified buffers.
///
/// Returns a Vec<f32> for each buffer index in `readback_indices`.
pub fn dispatch_with_buffers(
    gpu: &VulkanCompute,
    spirv: &[u8],
    buffers: &[BufferSpec<'_>],
    workgroup_count: u32,
    readback_indices: &[usize],
) -> Result<Vec<Vec<f32>>, VulkanError> {
    // 1D dispatch — single-pass access, no staging benefit
    dispatch_with_buffers_nd(gpu, spirv, buffers, workgroup_count, 1, 1, readback_indices, false)
}

/// 2D dispatch variant for matrix operations.
///
/// Same as `dispatch_with_buffers` but dispatches `(wg_x, wg_y, 1)` workgroups.
pub fn dispatch_with_buffers_2d(
    gpu: &VulkanCompute,
    spirv: &[u8],
    buffers: &[BufferSpec<'_>],
    wg_x: u32,
    wg_y: u32,
    readback_indices: &[usize],
) -> Result<Vec<Vec<f32>>, VulkanError> {
    // 2D dispatch — matmul/convolution with data reuse, staging helps
    dispatch_with_buffers_nd(gpu, spirv, buffers, wg_x, wg_y, 1, readback_indices, true)
}

/// Dispatch compute using existing GPU-resident buffers as inputs.
///
/// Inputs are bound at binding 0..N, new output buffers at N..N+M.
/// Returns newly allocated GpuBuffers for each output. No upload, no readback.
/// Reuses pipeline cache, command pool, fence, and descriptor pool.
pub fn dispatch_resident(
    gpu: &VulkanCompute,
    spirv: &[u8],
    inputs: &[GpuBufferRef],
    output_sizes: &[(u64, usize)],
    wg_x: u32,
) -> Result<Vec<GpuBuffer>, VulkanError> {
    let device = gpu.device;
    let n_inputs = inputs.len();
    let n_outputs = output_sizes.len();
    let n_total = n_inputs + n_outputs;

    // ── Pipeline (cached) ──────────────────────────────────────────────
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    let cache_key = (device as usize as u64, spirv_hash(spirv), n_total as u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (_shader_module, ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.shader_module, c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;

        let binding_descs: Vec<VkDescriptorSetLayoutBinding> = (0..n_total)
            .map(|i| VkDescriptorSetLayoutBinding {
                binding: i as u32,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            })
            .collect();
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: n_total as u32,
            pBindings: binding_descs.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;

        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: 0,
            pPushConstantRanges: std::ptr::null(),
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;

        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;

        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (shader_module, ds_layout, pipeline_layout, pipeline)
    };
    let layouts = [ds_layout];

    // ── Allocate output buffers ──────────────────────────────────────────
    let mut out_bufs: Vec<(VkBuffer, VkDeviceMemory, u64, usize)> = Vec::with_capacity(n_outputs);
    for &(size_bytes, len) in output_sizes {
        let (buf, mem) = acquire_buffer(gpu, size_bytes, RESIDENT_USAGE, RESIDENT_MEM_FLAGS)?;
        out_bufs.push((buf, mem, size_bytes, len));
    }

    // ── Descriptor set ───────────────────────────────────────────────────
    let descriptor_pool = acquire_descriptor_pool(gpu, n_total as u32)?;
    let alloc_info = VkDescriptorSetAllocateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        descriptorPool: descriptor_pool,
        descriptorSetCount: 1,
        pSetLayouts: layouts.as_ptr(),
    };
    let mut descriptor_set: VkDescriptorSet = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkAllocateDescriptorSets(device, &alloc_info, &mut descriptor_set)
    }).map_err(VulkanError::Vk)?;

    // Build buffer info: inputs first, then outputs
    let mut buf_infos: Vec<VkDescriptorBufferInfo> = Vec::with_capacity(n_total);
    for inp in inputs {
        buf_infos.push(VkDescriptorBufferInfo { buffer: inp.buffer, offset: 0, range: inp.size_bytes });
    }
    for &(buf, _, size_bytes, _) in &out_bufs {
        buf_infos.push(VkDescriptorBufferInfo { buffer: buf, offset: 0, range: size_bytes });
    }

    let writes: Vec<VkWriteDescriptorSet> = buf_infos.iter().enumerate()
        .map(|(i, info)| VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
            pNext: std::ptr::null(),
            dstSet: descriptor_set,
            dstBinding: i as u32,
            dstArrayElement: 0,
            descriptorCount: 1,
            descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(),
            pBufferInfo: info,
            pTexelBufferView: std::ptr::null(),
        })
        .collect();
    unsafe { vkUpdateDescriptorSets(device, writes.len() as u32, writes.as_ptr(), 0, std::ptr::null()); }

    // ── Command buffer + dispatch ────────────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;
    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };
    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(
            cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline_layout,
            0, 1, &descriptor_set, 0, std::ptr::null(),
        );
        vkCmdDispatch(cmd, wg_x, 1, 1);
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    // ── Submit + wait ────────────────────────────────────────────────────
    let fence = acquire_fence(gpu)?;
    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0,
        pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1,
        pCommandBuffers: &cmd,
        signalSemaphoreCount: 0,
        pSignalSemaphores: std::ptr::null(),
    };
    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Return output GpuBuffers (no readback, no input cleanup) ─────────
    Ok(out_bufs.into_iter().map(|(buffer, memory, size_bytes, len)| {
        GpuBuffer { buffer, memory, device, size_bytes, len, mem_flags: RESIDENT_MEM_FLAGS }
    }).collect())
}

/// Dispatch compute using existing GPU-resident buffers as inputs, with push constants.
///
/// Same as `dispatch_resident` but additionally supports push constants.
/// Inputs are bound at binding 0..N, new output buffers at N..N+M.
/// Returns newly allocated GpuBuffers for each output. No upload, no readback.
pub fn dispatch_resident_pc(
    gpu: &VulkanCompute,
    spirv: &[u8],
    inputs: &[GpuBufferRef],
    output_sizes: &[(u64, usize)],
    wg_x: u32,
    push_constants: &[f32],
) -> Result<Vec<GpuBuffer>, VulkanError> {
    let device = gpu.device;
    let n_inputs = inputs.len();
    let n_outputs = output_sizes.len();
    let n_total = n_inputs + n_outputs;
    let pc_bytes = (push_constants.len() * 4) as u32;

    // ── Pipeline (cached — key includes pc_bytes for distinct layouts) ──
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    // Use a distinct cache key that includes pc_bytes to avoid collisions
    // with dispatch_resident (which has no push constants).
    let cache_key = (device as usize as u64, spirv_hash(spirv).wrapping_add(pc_bytes as u64), n_total as u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (_shader_module, ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.shader_module, c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;

        let binding_descs: Vec<VkDescriptorSetLayoutBinding> = (0..n_total)
            .map(|i| VkDescriptorSetLayoutBinding {
                binding: i as u32,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            })
            .collect();
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: n_total as u32,
            pBindings: binding_descs.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;

        let pc_range = VkPushConstantRange {
            stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
            offset: 0,
            size: pc_bytes,
        };
        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: if pc_bytes > 0 { 1 } else { 0 },
            pPushConstantRanges: if pc_bytes > 0 { &pc_range } else { std::ptr::null() },
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;

        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;

        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (shader_module, ds_layout, pipeline_layout, pipeline)
    };
    let layouts = [ds_layout];

    // ── Allocate output buffers ──────────────────────────────────────────
    let mut out_bufs: Vec<(VkBuffer, VkDeviceMemory, u64, usize)> = Vec::with_capacity(n_outputs);
    for &(size_bytes, len) in output_sizes {
        let (buf, mem) = acquire_buffer(gpu, size_bytes, RESIDENT_USAGE, RESIDENT_MEM_FLAGS)?;
        out_bufs.push((buf, mem, size_bytes, len));
    }

    // ── Descriptor set ───────────────────────────────────────────────────
    let descriptor_pool = acquire_descriptor_pool(gpu, n_total as u32)?;
    let alloc_info = VkDescriptorSetAllocateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        descriptorPool: descriptor_pool,
        descriptorSetCount: 1,
        pSetLayouts: layouts.as_ptr(),
    };
    let mut descriptor_set: VkDescriptorSet = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkAllocateDescriptorSets(device, &alloc_info, &mut descriptor_set)
    }).map_err(VulkanError::Vk)?;

    let mut buf_infos: Vec<VkDescriptorBufferInfo> = Vec::with_capacity(n_total);
    for inp in inputs {
        buf_infos.push(VkDescriptorBufferInfo { buffer: inp.buffer, offset: 0, range: inp.size_bytes });
    }
    for &(buf, _, size_bytes, _) in &out_bufs {
        buf_infos.push(VkDescriptorBufferInfo { buffer: buf, offset: 0, range: size_bytes });
    }

    let writes: Vec<VkWriteDescriptorSet> = buf_infos.iter().enumerate()
        .map(|(i, info)| VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
            pNext: std::ptr::null(),
            dstSet: descriptor_set,
            dstBinding: i as u32,
            dstArrayElement: 0,
            descriptorCount: 1,
            descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(),
            pBufferInfo: info,
            pTexelBufferView: std::ptr::null(),
        })
        .collect();
    unsafe { vkUpdateDescriptorSets(device, writes.len() as u32, writes.as_ptr(), 0, std::ptr::null()); }

    // ── Command buffer + dispatch ────────────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;
    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };
    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(
            cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline_layout,
            0, 1, &descriptor_set, 0, std::ptr::null(),
        );
        if pc_bytes > 0 {
            vkCmdPushConstants(
                cmd, pipeline_layout, VK_SHADER_STAGE_COMPUTE_BIT,
                0, pc_bytes, push_constants.as_ptr() as *const std::ffi::c_void,
            );
        }
        vkCmdDispatch(cmd, wg_x, 1, 1);
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    // ── Submit + wait ────────────────────────────────────────────────────
    let fence = acquire_fence(gpu)?;
    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0,
        pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1,
        pCommandBuffers: &cmd,
        signalSemaphoreCount: 0,
        pSignalSemaphores: std::ptr::null(),
    };
    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Return output GpuBuffers (no readback, no input cleanup) ─────────
    Ok(out_bufs.into_iter().map(|(buffer, memory, size_bytes, len)| {
        GpuBuffer { buffer, memory, device, size_bytes, len, mem_flags: RESIDENT_MEM_FLAGS }
    }).collect())
}

// ── Deferred dispatch: queue now, execute later in batch (Phase 80) ──────────

/// Deferred version of dispatch_resident. Pre-allocates output buffers from
/// the pool and queues the dispatch for later batch execution via flush_pending.
/// Returns GpuBuffers with valid VkBuffer handles but data is NOT valid until
/// flush_pending() is called.
pub fn dispatch_resident_deferred(
    gpu: &VulkanCompute,
    spirv: &[u8],
    inputs: &[GpuBufferRef],
    output_sizes: &[(u64, usize)],
    wg_x: u32,
) -> Result<Vec<GpuBuffer>, VulkanError> {
    let device = gpu.device;
    let n_inputs = inputs.len();
    let n_outputs = output_sizes.len();
    let n_total = n_inputs + n_outputs;

    // ── Pipeline (cached, same as dispatch_resident) ─────────────────────
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    let cache_key = (device as usize as u64, spirv_hash(spirv), n_total as u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;
        let binding_descs: Vec<VkDescriptorSetLayoutBinding> = (0..n_total)
            .map(|i| VkDescriptorSetLayoutBinding {
                binding: i as u32,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            })
            .collect();
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: n_total as u32,
            pBindings: binding_descs.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;
        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: 0,
            pPushConstantRanges: std::ptr::null(),
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;
        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;
        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (ds_layout, pipeline_layout, pipeline)
    };

    // ── Pre-allocate output buffers from pool ────────────────────────────
    let mut out_bufs: Vec<(VkBuffer, VkDeviceMemory, u64, usize)> = Vec::with_capacity(n_outputs);
    for &(size_bytes, len) in output_sizes {
        let (buf, mem) = acquire_buffer(gpu, size_bytes, RESIDENT_USAGE, RESIDENT_MEM_FLAGS)?;
        out_bufs.push((buf, mem, size_bytes, len));
    }

    // ── Queue the dispatch ───────────────────────────────────────────────
    let input_buffers: Vec<(VkBuffer, u64)> = inputs.iter()
        .map(|inp| (inp.buffer, inp.size_bytes))
        .collect();
    let output_buffers: Vec<(VkBuffer, u64)> = out_bufs.iter()
        .map(|&(buf, _, size_bytes, _)| (buf, size_bytes))
        .collect();
    PENDING_DISPATCHES.with(|q| q.borrow_mut().push(PendingDispatch {
        pipeline, pipeline_layout, ds_layout,
        input_buffers, output_buffers,
        n_total_bindings: n_total as u32,
        wg_x,
    }));

    // ── Return pre-allocated GpuBuffers (handles valid, data pending) ────
    Ok(out_bufs.into_iter().map(|(buffer, memory, size_bytes, len)| {
        GpuBuffer { buffer, memory, device, size_bytes, len, mem_flags: RESIDENT_MEM_FLAGS }
    }).collect())
}

/// Flush all pending deferred dispatches as a single batched command buffer.
/// No-op if the queue is empty. Uses one fence wait for the entire batch.
pub fn flush_pending(gpu: &VulkanCompute) -> Result<(), VulkanError> {
    let pending: Vec<PendingDispatch> = PENDING_DISPATCHES.with(|q| {
        let mut queue = q.borrow_mut();
        std::mem::take(&mut *queue)
    });
    if pending.is_empty() { return Ok(()); }

    let device = gpu.device;
    let n_dispatches = pending.len();

    // ── Create batch descriptor pool (N sets, sum of all bindings) ───────
    let total_descriptors: u32 = pending.iter().map(|p| p.n_total_bindings).sum();
    let pool_size = VkDescriptorPoolSize {
        ty: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
        descriptorCount: total_descriptors,
    };
    let pool_info = VkDescriptorPoolCreateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_POOL_CREATE_INFO,
        pNext: std::ptr::null(),
        flags: 0,
        maxSets: n_dispatches as u32,
        poolSizeCount: 1,
        pPoolSizes: &pool_size,
    };
    let mut batch_pool: VkDescriptorPool = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkCreateDescriptorPool(device, &pool_info, std::ptr::null(), &mut batch_pool)
    }).map_err(VulkanError::Vk)?;

    // ── Allocate and update descriptor sets for each dispatch ─────────────
    let mut descriptor_sets: Vec<VkDescriptorSet> = Vec::with_capacity(n_dispatches);
    // Keep all buf_infos alive until after vkUpdateDescriptorSets
    let mut all_buf_infos: Vec<Vec<VkDescriptorBufferInfo>> = Vec::with_capacity(n_dispatches);
    let mut all_writes: Vec<VkWriteDescriptorSet> = Vec::new();

    for p in &pending {
        let layouts = [p.ds_layout];
        let alloc_info = VkDescriptorSetAllocateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
            pNext: std::ptr::null(),
            descriptorPool: batch_pool,
            descriptorSetCount: 1,
            pSetLayouts: layouts.as_ptr(),
        };
        let mut ds: VkDescriptorSet = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkAllocateDescriptorSets(device, &alloc_info, &mut ds)
        }).map_err(VulkanError::Vk)?;
        descriptor_sets.push(ds);

        let mut buf_infos: Vec<VkDescriptorBufferInfo> = Vec::with_capacity(p.n_total_bindings as usize);
        for &(buffer, size_bytes) in &p.input_buffers {
            buf_infos.push(VkDescriptorBufferInfo { buffer, offset: 0, range: size_bytes });
        }
        for &(buffer, size_bytes) in &p.output_buffers {
            buf_infos.push(VkDescriptorBufferInfo { buffer, offset: 0, range: size_bytes });
        }
        all_buf_infos.push(buf_infos);
    }

    // Build write descriptors referencing stable buf_infos
    for (i, p) in pending.iter().enumerate() {
        let ds = descriptor_sets[i];
        let buf_infos = &all_buf_infos[i];
        for (j, info) in buf_infos.iter().enumerate() {
            all_writes.push(VkWriteDescriptorSet {
                sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
                pNext: std::ptr::null(),
                dstSet: ds,
                dstBinding: j as u32,
                dstArrayElement: 0,
                descriptorCount: 1,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                pImageInfo: std::ptr::null(),
                pBufferInfo: info,
                pTexelBufferView: std::ptr::null(),
            });
        }
        let _ = p; // suppress unused warning
    }
    unsafe { vkUpdateDescriptorSets(device, all_writes.len() as u32, all_writes.as_ptr(), 0, std::ptr::null()); }

    // ── Record batched command buffer ────────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;
    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };

    let barrier = VkMemoryBarrier {
        sType: VK_STRUCTURE_TYPE_MEMORY_BARRIER,
        pNext: std::ptr::null(),
        srcAccessMask: VK_ACCESS_SHADER_WRITE_BIT,
        dstAccessMask: VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT,
    };

    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        for (i, p) in pending.iter().enumerate() {
            vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, p.pipeline);
            vkCmdBindDescriptorSets(
                cmd, VK_PIPELINE_BIND_POINT_COMPUTE, p.pipeline_layout,
                0, 1, &descriptor_sets[i], 0, std::ptr::null(),
            );
            vkCmdDispatch(cmd, p.wg_x, 1, 1);
            // Memory barrier between dispatches (not after the last one)
            if i + 1 < n_dispatches {
                vkCmdPipelineBarrier(
                    cmd,
                    VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                    VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                    0,
                    1, &barrier as *const VkMemoryBarrier as *const std::ffi::c_void,
                    0, std::ptr::null(),
                    0, std::ptr::null(),
                );
            }
        }
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    // ── Submit + single fence wait ──────────────────────────────────────
    let fence = acquire_fence(gpu)?;
    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0,
        pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1,
        pCommandBuffers: &cmd,
        signalSemaphoreCount: 0,
        pSignalSemaphores: std::ptr::null(),
    };
    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Destroy batch descriptor pool (not cached — batch-specific) ──────
    unsafe { vkDestroyDescriptorPool(device, batch_pool, std::ptr::null()); }

    Ok(())
}

/// Check if there are pending deferred dispatches waiting to be flushed.
pub fn has_pending() -> bool {
    PENDING_DISPATCHES.with(|q| !q.borrow().is_empty())
}

fn dispatch_with_buffers_nd(
    gpu: &VulkanCompute,
    spirv: &[u8],
    buffers: &[BufferSpec<'_>],
    wg_x: u32,
    wg_y: u32,
    wg_z: u32,
    readback_indices: &[usize],
    staging_hint: bool,
) -> Result<Vec<Vec<f32>>, VulkanError> {
    let device = gpu.device;
    let n = buffers.len();

    // ── Pipeline (cached: shader + descriptor layout + compute pipeline) ────────
    // Cache key: (device ptr, FNV-1a SPIR-V hash, binding_count = n).
    // Same SPIR-V with same binding count always reuses the compiled pipeline.
    assert!(spirv.len() % 4 == 0, "SPIR-V binary must be 4-byte aligned");
    let cache_key = (device as usize as u64, spirv_hash(spirv), n as u32);
    let cached = PIPELINE_CACHE.with(|c| c.borrow().get(&cache_key).copied());
    let (_shader_module, ds_layout, pipeline_layout, pipeline) = if let Some(c) = cached {
        (c.shader_module, c.ds_layout, c.pipeline_layout, c.pipeline)
    } else {
        let spirv_words: Vec<u32> = spirv
            .chunks_exact(4)
            .map(|c| u32::from_le_bytes([c[0], c[1], c[2], c[3]]))
            .collect();
        let shader_info = VkShaderModuleCreateInfo {
            sType: VK_STRUCTURE_TYPE_SHADER_MODULE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            codeSize: spirv_words.len() * 4,
            pCode: spirv_words.as_ptr(),
        };
        let mut shader_module: VkShaderModule = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateShaderModule(device, &shader_info, std::ptr::null(), &mut shader_module)
        }).map_err(VulkanError::Vk)?;

        let binding_descs: Vec<VkDescriptorSetLayoutBinding> = (0..n)
            .map(|i| VkDescriptorSetLayoutBinding {
                binding: i as u32,
                descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
                descriptorCount: 1,
                stageFlags: VK_SHADER_STAGE_COMPUTE_BIT,
                pImmutableSamplers: std::ptr::null(),
            })
            .collect();
        let ds_layout_info = VkDescriptorSetLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            bindingCount: n as u32,
            pBindings: binding_descs.as_ptr(),
        };
        let mut ds_layout: VkDescriptorSetLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateDescriptorSetLayout(device, &ds_layout_info, std::ptr::null(), &mut ds_layout)
        }).map_err(VulkanError::Vk)?;

        let dsl_arr = [ds_layout];
        let pipeline_layout_info = VkPipelineLayoutCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_LAYOUT_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            setLayoutCount: 1,
            pSetLayouts: dsl_arr.as_ptr(),
            pushConstantRangeCount: 0,
            pPushConstantRanges: std::ptr::null(),
        };
        let mut pipeline_layout: VkPipelineLayout = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreatePipelineLayout(device, &pipeline_layout_info, std::ptr::null(), &mut pipeline_layout)
        }).map_err(VulkanError::Vk)?;

        let entry_name = b"main\0";
        let stage = VkPipelineShaderStageCreateInfo {
            sType: VK_STRUCTURE_TYPE_PIPELINE_SHADER_STAGE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage: VK_SHADER_STAGE_COMPUTE_BIT,
            module: shader_module,
            pName: entry_name.as_ptr() as *const _,
            pSpecializationInfo: std::ptr::null(),
        };
        let pipeline_info = VkComputePipelineCreateInfo {
            sType: VK_STRUCTURE_TYPE_COMPUTE_PIPELINE_CREATE_INFO,
            pNext: std::ptr::null(),
            flags: 0,
            stage,
            layout: pipeline_layout,
            basePipelineHandle: VK_NULL_HANDLE,
            basePipelineIndex: -1,
        };
        let mut pipeline: VkPipeline = VK_NULL_HANDLE;
        vk_check(unsafe {
            vkCreateComputePipelines(device, VK_NULL_HANDLE, 1, &pipeline_info, std::ptr::null(), &mut pipeline)
        }).map_err(VulkanError::Vk)?;

        PIPELINE_CACHE.with(|c| c.borrow_mut().insert(cache_key, CachedPipeline {
            shader_module, ds_layout, pipeline_layout, pipeline,
        }));
        (shader_module, ds_layout, pipeline_layout, pipeline)
    };
    let layouts = [ds_layout];

    // ── Detect discrete GPU → staging mode ──────────────────────────────────
    // Upload staging: only for data-reuse ops (matmul, convolution) where
    //   DEVICE_LOCAL compute is faster than HOST_VISIBLE compute.
    // Readback staging: ALWAYS on discrete GPUs — CPU reads from HOST_VISIBLE
    //   (write-combining) memory are ~200 MB/s vs ~20 GB/s from cached memory.
    //   Using HOST_CACHED staging for readback is a 10-100x win.
    let discrete = should_use_staging(gpu);
    let use_upload_staging = staging_hint && discrete;
    let use_readback_staging = discrete && !readback_indices.is_empty();

    let (dev_usage, dev_mem_flags) = if use_upload_staging {
        // VRAM for compute, needs transfer flags for DMA
        (VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
            | VK_BUFFER_USAGE_TRANSFER_SRC_BIT
            | VK_BUFFER_USAGE_TRANSFER_DST_BIT,
         VK_MEMORY_PROPERTY_DEVICE_LOCAL_BIT)
    } else {
        // HOST_VISIBLE for direct CPU upload + GPU compute
        // Add TRANSFER_SRC so we can DMA copy to readback staging
        (VK_BUFFER_USAGE_STORAGE_BUFFER_BIT
            | if use_readback_staging { VK_BUFFER_USAGE_TRANSFER_SRC_BIT } else { 0 },
         VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT)
    };
    let upload_stg_usage = VK_BUFFER_USAGE_TRANSFER_SRC_BIT | VK_BUFFER_USAGE_TRANSFER_DST_BIT;
    let upload_stg_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;

    // Readback staging: prefer HOST_CACHED for fast CPU reads; fall back to HOST_COHERENT
    let readback_stg_usage = VK_BUFFER_USAGE_TRANSFER_DST_BIT;
    let readback_stg_mem = {
        let cached = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
        // Check if HOST_CACHED is available
        let has_cached = {
            let count = gpu.memory_properties.memoryTypeCount as usize;
            (0..count).any(|i| {
                let f = gpu.memory_properties.memoryTypes[i].propertyFlags;
                f & cached == cached
            })
        };
        if has_cached { cached } else { upload_stg_mem }
    };

    // ── Allocate device buffers (VRAM on discrete, unified on integrated) ────
    let mut dev_bufs: Vec<VkBuffer> = Vec::with_capacity(n);
    let mut dev_mems: Vec<VkDeviceMemory> = Vec::with_capacity(n);
    let mut sizes: Vec<u64> = Vec::with_capacity(n);

    // Upload staging buffers (only for 2D dispatch on discrete GPUs)
    let mut stg_bufs: Vec<VkBuffer> = Vec::with_capacity(if use_upload_staging { n } else { 0 });
    let mut stg_mems: Vec<VkDeviceMemory> = Vec::with_capacity(if use_upload_staging { n } else { 0 });

    // Readback staging buffers (HOST_CACHED, only for readback indices)
    let mut rb_bufs: Vec<(usize, VkBuffer, VkDeviceMemory)> = Vec::new();

    for spec in buffers {
        let (buf, mem) = acquire_buffer(gpu, spec.size_bytes, dev_usage, dev_mem_flags)?;
        dev_bufs.push(buf);
        dev_mems.push(mem);
        sizes.push(spec.size_bytes);

        if use_upload_staging {
            let (s_buf, s_mem) = acquire_buffer(gpu, spec.size_bytes, upload_stg_usage, upload_stg_mem)?;
            if let Some(ref data) = spec.initial_data {
                gpu.upload_f32(s_mem, data)?;
            }
            stg_bufs.push(s_buf);
            stg_mems.push(s_mem);
        } else {
            if let Some(ref data) = spec.initial_data {
                gpu.upload_f32(mem, data)?;
            }
        }
    }

    // Allocate readback staging buffers (HOST_CACHED for fast CPU reads)
    if use_readback_staging {
        for &idx in readback_indices {
            let (rb_buf, rb_mem) = acquire_buffer(gpu, sizes[idx], readback_stg_usage, readback_stg_mem)?;
            rb_bufs.push((idx, rb_buf, rb_mem));
        }
    }


    // ── Descriptor pool & set (cached pool) ──────────────────────────────────
    let descriptor_pool = acquire_descriptor_pool(gpu, n as u32)?;

    let alloc_info = VkDescriptorSetAllocateInfo {
        sType: VK_STRUCTURE_TYPE_DESCRIPTOR_SET_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        descriptorPool: descriptor_pool,
        descriptorSetCount: 1,
        pSetLayouts: layouts.as_ptr(),
    };
    let mut descriptor_set: VkDescriptorSet = VK_NULL_HANDLE;
    vk_check(unsafe {
        vkAllocateDescriptorSets(device, &alloc_info, &mut descriptor_set)
    }).map_err(VulkanError::Vk)?;

    // Bind device buffers to descriptor set
    let buf_infos: Vec<VkDescriptorBufferInfo> = dev_bufs
        .iter()
        .zip(sizes.iter())
        .map(|(&buf, &sz)| VkDescriptorBufferInfo {
            buffer: buf,
            offset: 0,
            range: sz,
        })
        .collect();

    let writes: Vec<VkWriteDescriptorSet> = buf_infos
        .iter()
        .enumerate()
        .map(|(i, info)| VkWriteDescriptorSet {
            sType: VK_STRUCTURE_TYPE_WRITE_DESCRIPTOR_SET,
            pNext: std::ptr::null(),
            dstSet: descriptor_set,
            dstBinding: i as u32,
            dstArrayElement: 0,
            descriptorCount: 1,
            descriptorType: VK_DESCRIPTOR_TYPE_STORAGE_BUFFER,
            pImageInfo: std::ptr::null(),
            pBufferInfo: info,
            pTexelBufferView: std::ptr::null(),
        })
        .collect();

    unsafe { vkUpdateDescriptorSets(device, writes.len() as u32, writes.as_ptr(), 0, std::ptr::null()); }


    // ── Command buffer (pooled command pool) ──────────────────────────────────
    let command_pool = acquire_command_pool(gpu)?;

    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };

    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;

        if use_upload_staging {
            // ── Upload: staging → device for buffers with input data ──────────
            for (i, spec) in buffers.iter().enumerate() {
                if spec.initial_data.is_some() {
                    let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: sizes[i] };
                    vkCmdCopyBuffer(cmd, stg_bufs[i], dev_bufs[i], 1, &copy);
                }
            }
            // Barrier: transfer writes complete before compute reads
            let upload_barriers: Vec<VkBufferMemoryBarrier> = buffers.iter().enumerate()
                .filter(|(_, spec)| spec.initial_data.is_some())
                .map(|(i, _)| VkBufferMemoryBarrier {
                    sType: VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
                    pNext: std::ptr::null(),
                    srcAccessMask: VK_ACCESS_TRANSFER_WRITE_BIT,
                    dstAccessMask: VK_ACCESS_SHADER_READ_BIT | VK_ACCESS_SHADER_WRITE_BIT,
                    srcQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    dstQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    buffer: dev_bufs[i],
                    offset: 0,
                    size: VK_WHOLE_SIZE,
                })
                .collect();
            if !upload_barriers.is_empty() {
                vkCmdPipelineBarrier(
                    cmd,
                    VK_PIPELINE_STAGE_TRANSFER_BIT,
                    VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                    0, 0, std::ptr::null(),
                    upload_barriers.len() as u32, upload_barriers.as_ptr(),
                    0, std::ptr::null(),
                );
            }
        }

        vkCmdBindPipeline(cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline);
        vkCmdBindDescriptorSets(
            cmd, VK_PIPELINE_BIND_POINT_COMPUTE, pipeline_layout,
            0, 1, &descriptor_set, 0, std::ptr::null(),
        );
        vkCmdDispatch(cmd, wg_x, wg_y, wg_z);

        // ── Readback staging: DMA compute output → HOST_CACHED buffer ────
        if use_readback_staging {
            // Barrier: compute writes complete before transfer reads
            let readback_barriers: Vec<VkBufferMemoryBarrier> = readback_indices.iter()
                .map(|&i| VkBufferMemoryBarrier {
                    sType: VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
                    pNext: std::ptr::null(),
                    srcAccessMask: VK_ACCESS_SHADER_WRITE_BIT,
                    dstAccessMask: VK_ACCESS_TRANSFER_READ_BIT,
                    srcQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    dstQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    buffer: dev_bufs[i],
                    offset: 0,
                    size: VK_WHOLE_SIZE,
                })
                .collect();
            vkCmdPipelineBarrier(
                cmd,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_TRANSFER_BIT,
                0, 0, std::ptr::null(),
                readback_barriers.len() as u32, readback_barriers.as_ptr(),
                0, std::ptr::null(),
            );
            // DMA copy: device/host-visible compute buffer → HOST_CACHED readback buffer
            for &(idx, rb_buf, _) in &rb_bufs {
                let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: sizes[idx] };
                vkCmdCopyBuffer(cmd, dev_bufs[idx], rb_buf, 1, &copy);
            }
        } else if use_upload_staging && !readback_indices.is_empty() {
            // Upload-staging mode: DMA compute output → upload staging buffer
            let readback_barriers: Vec<VkBufferMemoryBarrier> = readback_indices.iter()
                .map(|&i| VkBufferMemoryBarrier {
                    sType: VK_STRUCTURE_TYPE_BUFFER_MEMORY_BARRIER,
                    pNext: std::ptr::null(),
                    srcAccessMask: VK_ACCESS_SHADER_WRITE_BIT,
                    dstAccessMask: VK_ACCESS_TRANSFER_READ_BIT,
                    srcQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    dstQueueFamilyIndex: VK_QUEUE_FAMILY_IGNORED,
                    buffer: dev_bufs[i],
                    offset: 0,
                    size: VK_WHOLE_SIZE,
                })
                .collect();
            vkCmdPipelineBarrier(
                cmd,
                VK_PIPELINE_STAGE_COMPUTE_SHADER_BIT,
                VK_PIPELINE_STAGE_TRANSFER_BIT,
                0, 0, std::ptr::null(),
                readback_barriers.len() as u32, readback_barriers.as_ptr(),
                0, std::ptr::null(),
            );
            for &i in readback_indices {
                let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: sizes[i] };
                vkCmdCopyBuffer(cmd, dev_bufs[i], stg_bufs[i], 1, &copy);
            }
        }

        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }


    // ── Submit & wait (pooled fence) ──────────────────────────────────────────
    let fence = acquire_fence(gpu)?;

    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0,
        pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1,
        pCommandBuffers: &cmd,
        signalSemaphoreCount: 0,
        pSignalSemaphores: std::ptr::null(),
    };

    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // ── Readback ──────────────────────────────────────────────────────────────
    let mut results = Vec::new();
    if use_readback_staging {
        // Read from HOST_CACHED staging buffers (fast CPU reads)
        for &(idx, _, rb_mem) in &rb_bufs {
            let count = (sizes[idx] / 4) as usize;
            let data = gpu.download_f32(rb_mem, count)?;
            results.push(data);
        }
    } else if use_upload_staging {
        // Read from upload staging buffers (WC, but upload-staging mode)
        for &idx in readback_indices {
            let count = (sizes[idx] / 4) as usize;
            let data = gpu.download_f32(stg_mems[idx], count)?;
            results.push(data);
        }
    } else {
        // Read directly from compute buffers (integrated GPU, fast)
        for &idx in readback_indices {
            let count = (sizes[idx] / 4) as usize;
            let data = gpu.download_f32(dev_mems[idx], count)?;
            results.push(data);
        }
    }

    // ── Cleanup ───────────────────────────────────────────────────────────────
    // Pipeline, command pool, fence, and descriptor pool are retained in caches.
    // Buffers return to pool.
    for i in 0..n {
        release_buffer(device, sizes[i], dev_mem_flags, dev_bufs[i], dev_mems[i]);
    }
    if use_upload_staging {
        for i in 0..n {
            release_buffer(device, sizes[i], upload_stg_mem, stg_bufs[i], stg_mems[i]);
        }
    }
    for (idx, rb_buf, rb_mem) in rb_bufs {
        release_buffer(device, sizes[idx], readback_stg_mem, rb_buf, rb_mem);
    }

    Ok(results)
}

// dispatch_random_fill removed — CPU fallback inlined into compiler.rs

/// Multi-pass workgroup reduction.
///
/// Handles arrays of any size by iteratively reducing by factor 256.
/// Each pass reduces N elements to ceil(N/256) elements, padding with identity.
pub fn dispatch_reduce(
    gpu: &VulkanCompute,
    op: ReduceOp,
    input: &[f32],
) -> Result<f32, VulkanError> {
    use ReduceOp;

    static KERNEL_REDUCE_SUM: &[u8] = include_bytes!("../../../stdlib/loom/kernels/reduce/reduce_sum.spv");
    static KERNEL_REDUCE_MIN: &[u8] = include_bytes!("../../../stdlib/loom/kernels/reduce/reduce_min.spv");
    static KERNEL_REDUCE_MAX: &[u8] = include_bytes!("../../../stdlib/loom/kernels/reduce/reduce_max.spv");
    static KERNEL_REDUCE_MUL: &[u8] = include_bytes!("../../../stdlib/loom/kernels/reduce/reduce_mul.spv");

    let spirv: &[u8] = match op {
        ReduceOp::Sum => KERNEL_REDUCE_SUM,
        ReduceOp::Min => KERNEL_REDUCE_MIN,
        ReduceOp::Max => KERNEL_REDUCE_MAX,
        ReduceOp::Mul => KERNEL_REDUCE_MUL,
    };
    let identity = op.identity();

    let mut current = input.to_vec();

    while current.len() > 1 {
        let padded_len = current.len().div_ceil(256) * 256;
        current.resize(padded_len, identity);

        let wg_count = padded_len / 256;
        let input_size = (padded_len * 4) as u64;
        let output_size = (wg_count * 4) as u64;

        let result = dispatch_with_buffers(
            gpu,
            &spirv,
            &[
                BufferSpec { size_bytes: input_size, initial_data: Some(&current) },
                BufferSpec { size_bytes: output_size, initial_data: None },
            ],
            wg_count as u32,
            &[1],
        )?;

        current = result.into_iter().next().unwrap();
    }

    Ok(current[0])
}

/// Download a GpuBuffer using HOST_CACHED staging for fast CPU reads.
///
/// Direct reads from HOST_VISIBLE (write-combining) memory are ~200 MB/s.
/// HOST_CACHED staging gives ~20 GB/s — 100x faster for large readbacks.
pub fn download_buffer_fast(gpu: &VulkanCompute, buf: &GpuBuffer) -> Result<Vec<f32>, VulkanError> {
    if buf.len == 0 { return Ok(vec![]); }

    let discrete = should_use_staging(gpu);
    if !discrete {
        // Integrated GPU: HOST_VISIBLE IS device-local, reads are fast
        return gpu.download_f32(buf.memory, buf.len);
    }

    // Allocate HOST_CACHED staging buffer
    let stg_mem_flags = {
        let cached = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_CACHED_BIT;
        let count = gpu.memory_properties.memoryTypeCount as usize;
        let has_cached = (0..count).any(|i| {
            let f = gpu.memory_properties.memoryTypes[i].propertyFlags;
            f & cached == cached
        });
        if has_cached { cached } else { RESIDENT_MEM_FLAGS }
    };
    let stg_usage = VK_BUFFER_USAGE_TRANSFER_DST_BIT;
    let (stg_buf, stg_mem) = acquire_buffer(gpu, buf.size_bytes, stg_usage, stg_mem_flags)?;

    // DMA copy: resident buffer → staging buffer
    let command_pool = acquire_command_pool(gpu)?;
    let cmd_alloc_info = VkCommandBufferAllocateInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_ALLOCATE_INFO,
        pNext: std::ptr::null(),
        commandPool: command_pool,
        level: VK_COMMAND_BUFFER_LEVEL_PRIMARY,
        commandBufferCount: 1,
    };
    let mut cmd: VkCommandBuffer = std::ptr::null_mut();
    vk_check(unsafe {
        vkAllocateCommandBuffers(gpu.device, &cmd_alloc_info, &mut cmd)
    }).map_err(VulkanError::Vk)?;

    let begin_info = VkCommandBufferBeginInfo {
        sType: VK_STRUCTURE_TYPE_COMMAND_BUFFER_BEGIN_INFO,
        pNext: std::ptr::null(),
        flags: VK_COMMAND_BUFFER_USAGE_ONE_TIME_SUBMIT_BIT,
        pInheritanceInfo: std::ptr::null(),
    };
    unsafe {
        vk_check(vkBeginCommandBuffer(cmd, &begin_info)).map_err(VulkanError::Vk)?;
        let copy = VkBufferCopy { srcOffset: 0, dstOffset: 0, size: buf.size_bytes };
        vkCmdCopyBuffer(cmd, buf.buffer, stg_buf, 1, &copy);
        vk_check(vkEndCommandBuffer(cmd)).map_err(VulkanError::Vk)?;
    }

    let fence = acquire_fence(gpu)?;
    let submit_info = VkSubmitInfo {
        sType: VK_STRUCTURE_TYPE_SUBMIT_INFO,
        pNext: std::ptr::null(),
        waitSemaphoreCount: 0, pWaitSemaphores: std::ptr::null(),
        pWaitDstStageMask: std::ptr::null(),
        commandBufferCount: 1, pCommandBuffers: &cmd,
        signalSemaphoreCount: 0, pSignalSemaphores: std::ptr::null(),
    };
    unsafe {
        gpu.queue_submit(1, &submit_info, fence)?;
        vk_check(vkWaitForFences(gpu.device, 1, &fence, 1, u64::MAX))
            .map_err(VulkanError::Vk)?;
    }

    // Read from HOST_CACHED staging buffer (fast CPU reads)
    let result = gpu.download_f32(stg_mem, buf.len)?;
    release_buffer(gpu.device, buf.size_bytes, stg_mem_flags, stg_buf, stg_mem);
    Ok(result)
}

// dispatch_scan removed — CPU fallback inlined into compiler.rs (cpu_prefix_sum)
// dispatch_temporal removed — CPU fallback inlined into compiler.rs (cpu_temporal)

// ── High-level GPU operation dispatchers ────────────────────────────────────
// Previously in gpu_ops.rs. Thin wrappers over core dispatch functions.

/// Element-wise binary operation: C[i] = A[i] op B[i].
pub fn dispatch_binop(
    gpu: &VulkanCompute,
    op: BinaryOp,
    a: &[f32],
    b: &[f32],
) -> Result<Vec<f32>, VulkanError> {
    assert_eq!(a.len(), b.len(), "binop arrays must have same length");
    let n = a.len();
    let spirv = binop_kernel(op);
    let buffers = [
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(a) },
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(b) },
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: None },
    ];
    let wg_count = n.div_ceil(256) as u32;
    let results = dispatch_with_buffers(gpu, spirv, &buffers, wg_count, &[2])?;
    Ok(results.into_iter().next().unwrap()[..n].to_vec())
}

/// Element-wise conditional select: out[i] = cond[i] != 0 ? a[i] : b[i].
pub fn dispatch_select(
    gpu: &VulkanCompute,
    cond: &[f32],
    a: &[f32],
    b: &[f32],
) -> Result<Vec<f32>, VulkanError> {
    assert_eq!(cond.len(), a.len());
    assert_eq!(a.len(), b.len());
    let n = a.len();
    let buffers = [
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(cond) },
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(a) },
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(b) },
        BufferSpec { size_bytes: (n * 4) as u64, initial_data: None },
    ];
    let wg_count = n.div_ceil(256) as u32;
    let results = dispatch_with_buffers(gpu, KERNEL_SELECT, &buffers, wg_count, &[3])?;
    Ok(results.into_iter().next().unwrap()[..n].to_vec())
}

/// Element-wise unary map operation.
pub fn dispatch_map_op(
    gpu: &VulkanCompute,
    op: MapOp,
    input: &[f32],
) -> Result<Vec<f32>, VulkanError> {
    let (kernel, pc) = map_op_kernel(op);
    if pc.is_empty() {
        dispatch_compute(gpu, kernel, input, 256)
    } else {
        dispatch_compute_pc(gpu, kernel, input, 256, &pc)
    }
}

/// Reduce to scalar.
pub fn dispatch_reduce_op(
    gpu: &VulkanCompute,
    op: ReduceOp,
    input: &[f32],
) -> Result<f32, VulkanError> {
    dispatch_reduce(gpu, op, input)
}

// ── GPU-resident dispatch variants ──────────────────────────────────────────

/// Element-wise binary op on GPU-resident buffers. Zero PCIe round-trip.
pub fn dispatch_binop_resident(
    gpu: &VulkanCompute,
    op: BinaryOp,
    a: GpuBufferRef,
    b: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    assert_eq!(a.len, b.len, "binop resident arrays must have same length");
    let n = a.len;
    let spirv = binop_kernel(op);
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident(gpu, spirv, &[a, b], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

/// Element-wise unary map on a GPU-resident buffer. Zero PCIe round-trip.
pub fn dispatch_map_resident(
    gpu: &VulkanCompute,
    op: MapOp,
    input: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    let n = input.len;
    let (kernel, _pc) = map_op_kernel(op);
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident(gpu, kernel, &[input], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

/// Conditional select on GPU-resident buffers. Zero PCIe round-trip.
pub fn dispatch_select_resident(
    gpu: &VulkanCompute,
    cond: GpuBufferRef,
    a: GpuBufferRef,
    b: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    assert_eq!(cond.len, a.len);
    assert_eq!(a.len, b.len);
    let n = a.len;
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident(gpu, KERNEL_SELECT, &[cond, a, b], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

// ── Deferred dispatch variants ──────────────────────────────────────────────

/// Deferred binary op — queued for batch execution, zero fence wait.
pub fn dispatch_binop_deferred(
    gpu: &VulkanCompute,
    op: BinaryOp,
    a: GpuBufferRef,
    b: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    assert_eq!(a.len, b.len, "binop deferred arrays must have same length");
    let n = a.len;
    let spirv = binop_kernel(op);
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident_deferred(gpu, spirv, &[a, b], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

/// Deferred unary map — queued for batch execution, zero fence wait.
pub fn dispatch_map_deferred(
    gpu: &VulkanCompute,
    op: MapOp,
    input: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    let n = input.len;
    let (kernel, _pc) = map_op_kernel(op);
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident_deferred(gpu, kernel, &[input], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

/// Deferred conditional select — queued for batch execution, zero fence wait.
pub fn dispatch_select_deferred(
    gpu: &VulkanCompute,
    cond: GpuBufferRef,
    a: GpuBufferRef,
    b: GpuBufferRef,
) -> Result<GpuBuffer, VulkanError> {
    assert_eq!(cond.len, a.len);
    assert_eq!(a.len, b.len);
    let n = a.len;
    let wg = n.div_ceil(256) as u32;
    let mut results = dispatch_resident_deferred(gpu, KERNEL_SELECT, &[cond, a, b], &[((n * 4) as u64, n)], wg)?;
    Ok(results.remove(0))
}

/// Matrix multiply: C = A × B (CPU fallback).
///
/// A is m×k, B is k×n, C is m×n (all row-major, f32).
pub fn dispatch_matmul(
    _gpu: &VulkanCompute,
    a: &[f32],
    b: &[f32],
    m: u32,
    n: u32,
    k: u32,
) -> Result<Vec<f32>, VulkanError> {
    let m = m as usize;
    let n = n as usize;
    let k = k as usize;
    if a.len() != m * k {
        return Err(VulkanError::Other(format!(
            "matrix A has {} elements but m×k = {}×{} = {} expected",
            a.len(), m, k, m * k
        )));
    }
    if b.len() != k * n {
        return Err(VulkanError::Other(format!(
            "matrix B has {} elements but k×n = {}×{} = {} expected",
            b.len(), k, n, k * n
        )));
    }
    let mut c = vec![0.0f32; m * n];
    for i in 0..m {
        for j in 0..n {
            let mut sum = 0.0f32;
            for p in 0..k {
                sum += a[i * k + p] * b[p * n + j];
            }
            c[i * n + j] = sum;
        }
    }
    Ok(c)
}

#[cfg(test)]
mod tests {
    use super::*;

    fn get_gpu() -> Option<VulkanCompute> {
        match VulkanCompute::new() {
            Ok(g) => {
                eprintln!("GPU: {}", g.device_name());
                Some(g)
            }
            Err(e) => {
                eprintln!("Skipping GPU test — no Vulkan device: {e}");
                None
            }
        }
    }

    fn assert_close(got: &[f32], expected: &[f32], tol: f32, label: &str) {
        assert_eq!(got.len(), expected.len(), "{label}: length mismatch");
        for (i, (&g, &e)) in got.iter().zip(expected.iter()).enumerate() {
            let diff = (g - e).abs();
            let rel = if e.abs() > 1e-6 { diff / e.abs() } else { diff };
            assert!(
                rel < tol || diff < tol,
                "{label} mismatch at [{i}]: got {g}, expected {e}, diff {diff}"
            );
        }
    }

    // ── Map tests ──────────────────────────────────────────────────────────────

    #[test]
    fn test_element_wise_multiply_gpu() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let spirv: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/double.spv");
        let input: Vec<f32> = (1..=1024).map(|i| i as f32).collect();

        let output = dispatch_compute(&gpu, &spirv, &input, 256)
            .expect("dispatch_compute failed");

        let expected: Vec<f32> = input.iter().map(|x| x * 2.0).collect();
        assert_close(&output, &expected, 1e-6, "multiply");
    }

    #[test]
    fn test_map_all_ops() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        for size in [16, 1024] {
            let input: Vec<f32> = (1..=size).map(|i| i as f32).collect();

            let output = dispatch_compute_pc(&gpu,
                include_bytes!("../../../stdlib/loom/kernels/math/add_pc.spv"),
                &input, 256, &[10.0]).unwrap();
            let expected: Vec<f32> = input.iter().map(|x| x + 10.0).collect();
            assert_close(&output, &expected, 1e-6, &format!("add size={size}"));

            let output = dispatch_compute(&gpu,
                include_bytes!("../../../stdlib/loom/kernels/math/sqrt.spv"),
                &input, 256).unwrap();
            let expected: Vec<f32> = input.iter().map(|x| x.sqrt()).collect();
            assert_close(&output, &expected, 1e-5, &format!("sqrt size={size}"));

            let neg_input: Vec<f32> = input.iter().map(|x| -x).collect();
            let output = dispatch_compute(&gpu,
                include_bytes!("../../../stdlib/loom/kernels/math/abs.spv"),
                &neg_input, 256).unwrap();
            assert_close(&output, &input, 1e-6, &format!("abs size={size}"));
        }
    }

    #[test]
    fn test_map_new_vanilla_ops() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let input: Vec<f32> = (1..=1024).map(|i| i as f32 * 0.1).collect();

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/negate.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| -x).collect();
        assert_close(&output, &expected, 1e-6, "negate");

        let output = dispatch_compute_pc(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/mod_pc.spv"), &input, 256, &[3.0]).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x % 3.0).collect();
        assert_close(&output, &expected, 1e-5, "mod");

        let output = dispatch_compute_pc(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/pow_pc.spv"), &input, 256, &[2.0]).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.powf(2.0)).collect();
        assert_close(&output, &expected, 1e-4, "pow");

        let small_input: Vec<f32> = (0..1024).map(|i| i as f32 * 0.005).collect();
        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/exp.spv"), &small_input, 256).unwrap();
        let expected: Vec<f32> = small_input.iter().map(|x| x.exp()).collect();
        assert_close(&output, &expected, 1e-4, "exp");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/log.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.ln()).collect();
        assert_close(&output, &expected, 1e-5, "log");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/floor.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.floor()).collect();
        assert_close(&output, &expected, 1e-6, "floor");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/ceil.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.ceil()).collect();
        assert_close(&output, &expected, 1e-6, "ceil");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/round.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| {
            let r = x.round();
            if (x - x.floor() - 0.5).abs() < 1e-6 {
                if r as i32 % 2 != 0 { r - 1.0 } else { r }
            } else { r }
        }).collect();
        assert_close(&output, &expected, 1e-6, "round");

        let output = dispatch_compute_pc(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/min_pc.spv"), &input, 256, &[50.0]).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.min(50.0)).collect();
        assert_close(&output, &expected, 1e-6, "min");

        let output = dispatch_compute_pc(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/max_pc.spv"), &input, 256, &[50.0]).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.max(50.0)).collect();
        assert_close(&output, &expected, 1e-6, "max");

        let output = dispatch_compute_pc(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/clamp_pc.spv"), &input, 256, &[20.0, 80.0]).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.clamp(20.0, 80.0)).collect();
        assert_close(&output, &expected, 1e-6, "clamp");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/sin.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.sin()).collect();
        assert_close(&output, &expected, 1e-4, "sin");

        let output = dispatch_compute(&gpu, include_bytes!("../../../stdlib/loom/kernels/math/cos.spv"), &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.cos()).collect();
        assert_close(&output, &expected, 1e-4, "cos");

        eprintln!("All 13 new vanilla ops verified on GPU ✓");
    }

    // ── Reduce tests ──────────────────────────────────────────────────────────

    // test_random_fill_unique removed — dispatch_random_fill moved to compiler.rs

    #[test]
    fn test_reduce_sum_small() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        for size in [16, 256, 1024] {
            let input: Vec<f32> = (1..=size).map(|i| i as f32).collect();
            let expected: f32 = input.iter().sum();
            let got = dispatch_reduce(
                &gpu, ReduceOp::Sum, &input).unwrap();
            let diff = (got - expected).abs();
            assert!(diff < expected * 1e-5,
                "reduce sum size={size}: got {got}, expected {expected}, diff {diff}");
        }
    }

    #[test]
    fn test_reduce_min_max() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        for size in [16, 1024] {
            let input: Vec<f32> = (1..=size).map(|i| i as f32 * 0.7 - 3.0).collect();

            let got_min = dispatch_reduce(
                &gpu, ReduceOp::Min, &input).unwrap();
            let got_max = dispatch_reduce(
                &gpu, ReduceOp::Max, &input).unwrap();

            let exp_min = input.iter().cloned().fold(f32::INFINITY, f32::min);
            let exp_max = input.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

            assert!((got_min - exp_min).abs() < 1e-5, "min size={size}");
            assert!((got_max - exp_max).abs() < 1e-5, "max size={size}");
        }
    }

    // ── Scan tests ────────────────────────────────────────────────────────────

    // test_scan_small and test_scan_multi_workgroup removed — dispatch_scan moved to compiler.rs

    // ── Temporal tests ────────────────────────────────────────────────────────

    // test_temporal_ema removed — dispatch_temporal moved to compiler.rs

    // ── Fused tests ───────────────────────────────────────────────────────────

    #[test]
    fn test_fused_normalize() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        for size in [16, 1024] {
            let input: Vec<f32> = (0..size).map(|i| i as f32 * 3.0 + 5.0).collect();
            let min_val = input.iter().cloned().fold(f32::INFINITY, f32::min);
            let max_val = input.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

            let output = dispatch_compute_pc(&gpu,
                include_bytes!("../../../stdlib/loom/kernels/math/normalize_pc.spv"),
                &input, 256, &[min_val, max_val]).unwrap();
            let range = max_val - min_val;
            let expected: Vec<f32> = input.iter().map(|x| (x - min_val) / range).collect();
            assert_close(&output, &expected, 1e-5, &format!("normalize size={size}"));
        }
    }

    // ── 1M-element stress tests ───────────────────────────────────────────────

    #[test]
    fn test_map_1m() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let n = 1_000_000;
        let input: Vec<f32> = (0..n).map(|i| (i as f32) * 0.001).collect();

        let output = dispatch_compute_pc(&gpu,
            include_bytes!("../../../stdlib/loom/kernels/math/add_pc.spv"),
            &input, 256, &[42.0]).unwrap();

        assert_eq!(output.len(), n);
        for i in (0..n).step_by(1000) {
            let expected = input[i] + 42.0;
            let diff = (output[i] - expected).abs();
            assert!(diff < 1e-4, "map 1M: [{i}] got {}, expected {expected}", output[i]);
        }
        eprintln!("map 1M: OK");
    }

    #[test]
    fn test_elementwise_10m_benchmark() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let n = 10_000_000usize;
        let a: Vec<f32> = (0..n).map(|i| (i as f32) * 0.0001).collect();
        let b: Vec<f32> = (0..n).map(|i| ((i * 3 + 7) % 10000) as f32 * 0.0001).collect();

        let spirv_add: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/add.spv");

        let wg = n.div_ceil(256) as u32;

        // Warmup (pipeline creation)
        let bufs_w = [
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&a) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&b) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: None },
        ];
        let _ = dispatch_with_buffers(&gpu, &spirv_add, &bufs_w, wg, &[2]).unwrap();

        // Timed: full (buffer alloc + upload + compute + readback)
        let bufs_t = [
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&a) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&b) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: None },
        ];
        let t0 = std::time::Instant::now();
        let result = dispatch_with_buffers(&gpu, &spirv_add, &bufs_t, wg, &[2]).unwrap();
        let full_ms = t0.elapsed().as_secs_f64() * 1000.0;
        let r = &result[0];
        eprintln!("gpu_add 10M: {:.1} ms  (full: alloc+transfer+compute+readback)", full_ms);
        eprintln!("  data: 3x40MB = 120MB total buffer traffic");
        eprintln!("  result[0]={:.6}", r[0]);
    }

    #[test]
    fn test_profile_10m_breakdown() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let n = 10_000_000usize;
        let a: Vec<f32> = (0..n).map(|i| (i as f32) * 0.0001).collect();
        let b: Vec<f32> = (0..n).map(|i| ((i * 3 + 7) % 10000) as f32 * 0.0001).collect();
        let spirv: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/add.spv");
        let wg = n.div_ceil(256) as u32;

        // Warmup (pipeline + pool priming)
        let bufs = [
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&a) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: Some(&b) },
            BufferSpec { size_bytes: (n * 4) as u64, initial_data: None },
        ];
        let _ = dispatch_with_buffers(&gpu, &spirv, &bufs, wg, &[2]).unwrap();

        // Profile: CPU memcpy 40MB (raw throughput baseline)
        let mut dest = vec![0.0f32; n];
        let t_mc = std::time::Instant::now();
        unsafe { std::ptr::copy_nonoverlapping(a.as_ptr(), dest.as_mut_ptr(), n); }
        std::hint::black_box(&dest);
        let memcpy_us = t_mc.elapsed().as_nanos() / 1000;

        // Profile: upload 2x40MB to HOST_VISIBLE mapped memory
        let host_mem = VK_MEMORY_PROPERTY_HOST_VISIBLE_BIT | VK_MEMORY_PROPERTY_HOST_COHERENT_BIT;
        let buf_size = (n * 4) as u64;
        let (b0, m0) = acquire_buffer(&gpu, buf_size, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT, host_mem).unwrap();
        let (b1, m1) = acquire_buffer(&gpu, buf_size, VK_BUFFER_USAGE_STORAGE_BUFFER_BIT, host_mem).unwrap();
        let t_up = std::time::Instant::now();
        gpu.upload_f32(m0, &a).unwrap();
        gpu.upload_f32(m1, &b).unwrap();
        let upload_us = t_up.elapsed().as_micros();
        release_buffer(gpu.device, buf_size, host_mem, b0, m0);
        release_buffer(gpu.device, buf_size, host_mem, b1, m1);

        // Full warm dispatch: 3 runs, take median
        let mut runs = Vec::new();
        for _ in 0..5 {
            let bufs2 = [
                BufferSpec { size_bytes: buf_size, initial_data: Some(&a) },
                BufferSpec { size_bytes: buf_size, initial_data: Some(&b) },
                BufferSpec { size_bytes: buf_size, initial_data: None },
            ];
            let t = std::time::Instant::now();
            let _ = dispatch_with_buffers(&gpu, &spirv, &bufs2, wg, &[2]).unwrap();
            runs.push(t.elapsed().as_micros());
        }
        runs.sort();
        let median = runs[2];

        eprintln!("=== 10M add PROFILE ===");
        eprintln!("  memcpy 40MB:       {:>8} us  (CPU baseline)", memcpy_us);
        eprintln!("  upload 2x40MB:     {:>8} us  (HOST_VISIBLE mapped, write-combined)", upload_us);
        eprintln!("  full dispatch 5x:  {} us  (median of {:?})", median, runs);
        eprintln!("");
        eprintln!("  DIAGNOSIS: Every gpu_add re-uploads 80MB + reads back 40MB = 120MB PCIe");
        eprintln!("  FIX: GPU-resident buffers — keep results on GPU, download only on CPU access");
    }

    #[test]
    fn test_reduce_sum_1m() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let n = 1_000_000;
        let input: Vec<f32> = (0..n).map(|i| (i % 100) as f32 * 0.01).collect();
        let expected: f64 = input.iter().map(|&x| x as f64).sum();

        let got = dispatch_reduce(
            &gpu, ReduceOp::Sum, &input).unwrap();

        let rel_err = ((got as f64 - expected) / expected).abs();
        assert!(rel_err < 0.01, "reduce sum 1M: got {got}, expected {expected}, rel_err {rel_err}");
        eprintln!("reduce sum 1M: OK");
    }

    #[test]
    fn test_reduce_min_max_1m() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let n = 1_000_000;
        let input: Vec<f32> = (0..n).map(|i| (i as f32 - 500_000.0) * 0.1).collect();

        let got_min = dispatch_reduce(&gpu, ReduceOp::Min, &input).unwrap();
        let got_max = dispatch_reduce(&gpu, ReduceOp::Max, &input).unwrap();

        let exp_min = input.iter().cloned().fold(f32::INFINITY, f32::min);
        let exp_max = input.iter().cloned().fold(f32::NEG_INFINITY, f32::max);

        assert!((got_min - exp_min).abs() < 1e-3, "min 1M: got {got_min}, expected {exp_min}");
        assert!((got_max - exp_max).abs() < 1e-3, "max 1M: got {got_max}, expected {exp_max}");
        eprintln!("reduce min/max 1M: OK");
    }

    // test_scan_1m and test_temporal_ema_1m removed — CPU fallbacks moved to compiler.rs

    #[test]
    fn test_fused_normalize_1m() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let n = 1_000_000;
        let input: Vec<f32> = (0..n).map(|i| i as f32 * 0.003 - 1500.0).collect();

        let min_val = dispatch_reduce(&gpu, ReduceOp::Min, &input).unwrap();
        let max_val = dispatch_reduce(&gpu, ReduceOp::Max, &input).unwrap();

        let output = dispatch_compute_pc(&gpu,
            include_bytes!("../../../stdlib/loom/kernels/math/normalize_pc.spv"),
            &input, 256, &[min_val, max_val]).unwrap();
        assert_eq!(output.len(), n);

        let range = max_val - min_val;
        for i in (0..n).step_by(1000) {
            let expected = (input[i] - min_val) / range;
            let diff = (output[i] - expected).abs();
            assert!(diff < 1e-4, "normalize 1M: [{i}]");
        }

        assert!(output[0].abs() < 1e-5, "normalize 1M: first should be ~0");
        assert!((output[n - 1] - 1.0).abs() < 1e-5, "normalize 1M: last should be ~1");
        eprintln!("fused normalize 1M: OK ✓");
    }

    // ── GLSL-compiled SPIR-V test battery (Phase 67) ──────────────────────────
    // Tests dispatch_compute against pre-compiled GLSL shaders covering:
    // branching, nested conditionals, loops, large dispatch, edge alignment,
    // int casts, chained ops, negative values, zero handling.

    fn load_test_spv(name: &str) -> Vec<u8> {
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let path = root.join("tests").join("gpu_shaders").join(name);
        std::fs::read(&path).unwrap_or_else(|e| panic!("cannot read {}: {}", path.display(), e))
    }

    #[test]
    fn test_glsl_01_double() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("01_double.spv");
        let input: Vec<f32> = (1..=256).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x * 2.0).collect();
        assert_close(&output, &expected, 1e-6, "glsl_double");
    }

    #[test]
    fn test_glsl_02_branch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("02_branch.spv");
        let input: Vec<f32> = (1..=256).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            if x > 5.0 { x * 10.0 } else { x + 100.0 }
        }).collect();
        assert_close(&output, &expected, 1e-6, "glsl_branch");
    }

    #[test]
    fn test_glsl_03_nested_cond() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("03_nested_cond.spv");
        let input: Vec<f32> = (1..=256).map(|i| i as f32 * 0.04).collect(); // 0.04..10.24
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            if x < 3.0 { x + 1000.0 }
            else if x < 7.0 {
                if x < 5.0 { x * 2.0 } else { x * 3.0 }
            } else { x - 1.0 }
        }).collect();
        assert_close(&output, &expected, 1e-4, "glsl_nested_cond");
    }

    #[test]
    fn test_glsl_04_loop() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("04_loop.spv");
        let input: Vec<f32> = (0..256).map(|i| i as f32 * 0.05).collect(); // 0..12.75
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            let n = (x as i32).min(10).max(0);
            (1..=n).map(|i| i as f32).sum::<f32>()
        }).collect();
        assert_close(&output, &expected, 1e-4, "glsl_loop");
    }

    #[test]
    fn test_glsl_05_large_dispatch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("05_large_dispatch.spv");
        // 65536 elements = 256 workgroups of 256 threads
        let input: Vec<f32> = (1..=65536).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x.sqrt()).collect();
        assert_close(&output, &expected, 1e-4, "glsl_large_dispatch");
    }

    #[test]
    fn test_glsl_06_edge_alignment() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("06_edge_alignment.spv");
        // Non-power-of-2 sizes
        for size in [1, 3, 7, 15, 31, 63, 100, 255, 257, 1000] {
            let input: Vec<f32> = (0..size).map(|i| i as f32 + 0.5).collect();
            let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
            assert_close(&output, &input, 1e-6, &format!("glsl_edge_align_n={size}"));
        }
    }

    #[test]
    fn test_glsl_07_int_cast() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("07_int_cast.spv");
        let input: Vec<f32> = vec![0.0, 0.5, 1.0, 1.9, 2.1, 3.7, -0.5, -1.9, 100.3, 255.9,
                                   0.001, 0.999, -0.001, -0.999, 10.0, 256.0];
        // Pad to at least workgroup size
        let mut padded = input.clone();
        while padded.len() < 256 { padded.push(0.0); }
        let output = dispatch_compute(&gpu, &spirv, &padded, 256).unwrap();
        for (i, &x) in input.iter().enumerate() {
            let expected = (x as i32) as f32;
            assert!((output[i] - expected).abs() < 1e-6,
                "int_cast [{i}]: {x} -> expected {expected}, got {}", output[i]);
        }
    }

    #[test]
    fn test_glsl_08_chain_ops() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("08_chain_ops.spv");
        let input: Vec<f32> = (0..256).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| (x + 1.0) * 3.0 - 2.0).collect();
        assert_close(&output, &expected, 1e-4, "glsl_chain_ops");
    }

    #[test]
    fn test_glsl_09_negative_values() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("09_negative_values.spv");
        let input: Vec<f32> = (-128..128).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            // GLSL sign(0.0) = 0.0, unlike Rust signum(0.0) = 1.0
            let sign = if x > 0.0 { 1.0 } else if x < 0.0 { -1.0 } else { 0.0 };
            x.abs() + sign
        }).collect();
        assert_close(&output, &expected, 1e-6, "glsl_negative");
    }

    #[test]
    fn test_glsl_10_zero_handling() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("10_zero_handling.spv");
        let mut input: Vec<f32> = (1..=256).map(|i| i as f32).collect();
        input[0] = 0.0;  // zero at index 0
        input[128] = 0.0; // zero in the middle
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            if x != 0.0 { 1.0 / x } else { 0.0 }
        }).collect();
        assert_close(&output, &expected, 1e-4, "glsl_zero_handling");
    }

    // ── Randomized property test ──────────────────────────────────────────────
    #[test]
    fn test_glsl_random_data_double() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let spirv = load_test_spv("01_double.spv");
        // Pseudo-random input (deterministic seed)
        let mut rng: u64 = 0xDEADBEEF_CAFEBABE;
        for trial in 0..5 {
            let size = 256 * (trial + 1); // 256, 512, 768, 1024, 1280
            let input: Vec<f32> = (0..size).map(|_| {
                rng ^= rng << 13;
                rng ^= rng >> 7;
                rng ^= rng << 17;
                (rng as f32 / u64::MAX as f32) * 200.0 - 100.0
            }).collect();
            let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
            let expected: Vec<f32> = input.iter().map(|x| x * 2.0).collect();
            assert_close(&output, &expected, 1e-4, &format!("random_double_trial={trial}_n={size}"));
        }
    }

    // ── .flow SPIR-V emitter validation (Phase 67) ────────────────────────────
    // Tests that spirv_emit.flow produces functionally correct SPIR-V that matches
    // the Rust SPIR-V emitter output for the same operations.

    /// Run spirv_emit.flow via subprocess and return the generated SPIR-V bytes.
    fn run_flow_emitter_subprocess(output_path: &std::path::Path) -> Vec<u8> {
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let emit_path = root.join("stdlib/compiler/spirv_emit.flow");

        // Read, patch output path, write to temp
        let source = std::fs::read_to_string(&emit_path).expect("spirv_emit.flow");
        let out_str = output_path.to_str().unwrap().replace('\\', "/");
        let patched = source.replace(
            "write_bytes(\"spirv_test_double.spv\", buf)",
            &format!("write_bytes(\"{}\", buf)", out_str),
        );
        let tmp_flow = std::env::temp_dir().join("octoflow_emit_patched.flow");
        std::fs::write(&tmp_flow, &patched).unwrap();

        // Run via subprocess
        let exe = if cfg!(target_os = "windows") {
            root.join("target/release/octoflow.exe")
        } else {
            root.join("target/release/octoflow")
        };
        let status = std::process::Command::new(&exe)
            .arg("run")
            .arg(tmp_flow.to_str().unwrap())
            .arg("--allow-write")
            .current_dir(&root)
            .status()
            .expect("failed to run octoflow");
        assert!(status.success(), "octoflow failed");

        std::fs::remove_file(&tmp_flow).ok();
        std::fs::read(output_path).expect("read .spv")
    }

    /// Patch a SPIR-V opcode in the binary.
    fn patch_spirv_opcode(spirv: &mut [u8], old_opcode: u16, new_opcode: u16) {
        for i in (0..spirv.len()).step_by(4) {
            let word = u32::from_le_bytes([spirv[i], spirv[i+1], spirv[i+2], spirv[i+3]]);
            let wc = (word >> 16) as u16;
            let op = (word & 0xFFFF) as u16;
            if op == old_opcode && wc == 5 {
                let new_word = ((wc as u32) << 16) | (new_opcode as u32);
                spirv[i..i+4].copy_from_slice(&new_word.to_le_bytes());
                return;
            }
        }
        panic!("opcode {} not found in SPIR-V", old_opcode);
    }

    /// Patch a float constant in the SPIR-V binary (identified by result ID).
    fn patch_spirv_constant(spirv: &mut [u8], old_val: f32, new_val: f32) {
        let old_bytes = old_val.to_le_bytes();
        let new_bytes = new_val.to_le_bytes();
        for i in 0..spirv.len()-3 {
            if spirv[i..i+4] == old_bytes {
                if i >= 4 {
                    let prev_word = u32::from_le_bytes([spirv[i-4], spirv[i-3], spirv[i-2], spirv[i-1]]);
                    if prev_word == 18 { // %18 = our float constant
                        spirv[i..i+4].copy_from_slice(&new_bytes);
                        return;
                    }
                }
            }
        }
        panic!("constant {} not found in SPIR-V", old_val);
    }

    #[test]
    fn test_flow_emitter_validate_and_dispatch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let tmp = std::env::temp_dir().join("octoflow_emit_base.spv");
        let spirv = run_flow_emitter_subprocess(&tmp);
        assert!(spirv.len() > 100, "SPIR-V too small: {} bytes", spirv.len());

        // Validate with spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val: PASS ✓");
        }

        // Dispatch on GPU: multiply by 2.0
        let input: Vec<f32> = (1..=1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|x| x * 2.0).collect();
        assert_close(&output, &expected, 1e-6, "flow_emit_multiply");
        eprintln!("GPU dispatch multiply: PASS ✓");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_flow_emitter_patched_operations() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let tmp = std::env::temp_dir().join("octoflow_emit_patch.spv");
        let base_spirv = run_flow_emitter_subprocess(&tmp);
        let input: Vec<f32> = (1..=1024).map(|i| i as f32).collect();

        // Test 4 operations by patching the base SPIR-V
        let ops: Vec<(&str, u16, f32, Box<dyn Fn(f32) -> f32>)> = vec![
            ("add_10",  129, 10.0, Box::new(|x| x + 10.0)),
            ("sub_5",   131,  5.0, Box::new(|x| x - 5.0)),
            ("div_4",   136,  4.0, Box::new(|x| x / 4.0)),
            ("mul_3",   133,  3.0, Box::new(|x| x * 3.0)),
        ];

        for (label, opcode, constant, compute_expected) in &ops {
            let mut spirv = base_spirv.clone();
            patch_spirv_opcode(&mut spirv, 133, *opcode);
            patch_spirv_constant(&mut spirv, 2.0, *constant);
            std::fs::write(&tmp, &spirv).unwrap();

            if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
                assert!(o.status.success(), "spirv-val failed on {}: {}",
                    label, String::from_utf8_lossy(&o.stderr));
            }

            let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
            let expected: Vec<f32> = input.iter().map(|x| compute_expected(*x)).collect();
            assert_close(&output, &expected, 1e-4, &format!("flow_emit_{}", label));
            eprintln!("Patched {}: PASS ✓", label);
        }

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_flow_emitter_vs_rust_emitter() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        // .flow emitter
        let tmp = std::env::temp_dir().join("octoflow_emit_cmp.spv");
        let flow_spirv = run_flow_emitter_subprocess(&tmp);
        std::fs::remove_file(&tmp).ok();

        // Embedded pre-built kernel (replaces Rust emitter)
        let embedded_spirv: &[u8] = include_bytes!("../../../stdlib/loom/kernels/math/double.spv");

        // Both should produce same GPU results on same input
        let input: Vec<f32> = (1..=2048).map(|i| i as f32 * 0.1).collect();
        let flow_output = dispatch_compute(&gpu, &flow_spirv, &input, 256).unwrap();
        let embedded_output = dispatch_compute(&gpu, embedded_spirv, &input, 256).unwrap();

        assert_close(&flow_output, &embedded_output, 1e-6, "flow_vs_embedded_emitter");
        eprintln!("Flow emitter vs embedded kernel: identical output on 2048 elements ✓");
    }

    /// Run a .flow emitter via subprocess using SPIRV_OUTPUT env var.
    fn run_flow_emitter_with_env(flow_file: &str, output_path: &std::path::Path) -> Vec<u8> {
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let emit_path = root.join(flow_file);
        assert!(emit_path.exists(), "{} must exist at {:?}", flow_file, emit_path);

        let exe = root.join("target/release/octoflow");
        let out_str = output_path.to_str().unwrap().replace('\\', "/");
        let status = std::process::Command::new(&exe)
            .arg("run")
            .arg(emit_path.to_str().unwrap())
            .arg("--allow-write")
            .env("SPIRV_OUTPUT", &out_str)
            .current_dir(&root)
            .status()
            .expect("failed to run octoflow");
        assert!(status.success(), "octoflow failed for {}", flow_file);

        std::fs::read(output_path).expect("read .spv")
    }

    #[test]
    fn test_flow_emitter_branch_selection_merge() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let tmp = std::env::temp_dir().join("octoflow_emit_branch.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/spirv_emit_branch.flow", &tmp);
        assert!(spirv.len() > 100, "branch SPIR-V too small: {} bytes", spirv.len());

        // 1. Validate with spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on branch shader: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [branch+phi]: PASS");
        }

        // 2. Dispatch — if val > 5.0: val * 10.0; else: val + 100.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            if x > 5.0 { x * 10.0 } else { x + 100.0 }
        }).collect();
        assert_close(&output, &expected, 1e-6, "flow_emit_branch");
        eprintln!("GPU dispatch [branch+phi]: PASS — {} elements", input.len());

        // 3. Cross-validate against GLSL reference
        let glsl_spv = include_bytes!(concat!(env!("CARGO_MANIFEST_DIR"),
            "/../../tests/gpu_shaders/02_branch.spv"));
        let glsl_output = dispatch_compute(&gpu, glsl_spv, &input, 256).unwrap();
        assert_close(&output, &glsl_output, 1e-6, "flow_branch_vs_glsl");
        eprintln!("Cross-validation [.flow vs GLSL branch]: PASS");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_flow_emitter_loop_merge() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let tmp = std::env::temp_dir().join("octoflow_emit_loop.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/spirv_emit_loop.flow", &tmp);
        assert!(spirv.len() > 100, "loop SPIR-V too small: {} bytes", spirv.len());

        // 1. Validate with spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on loop shader: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [loop+phi]: PASS");
        }

        // 2. Dispatch — sum from 1 to min(uint(val), 10)
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            let n = (x as u32).min(10);
            (1..=n).map(|i| i as f32).sum::<f32>()
        }).collect();
        assert_close(&output, &expected, 1e-6, "flow_emit_loop");
        eprintln!("GPU dispatch [loop+phi]: PASS — {} elements", input.len());

        // 3. Cross-validate against GLSL reference
        let glsl_spv = include_bytes!(concat!(env!("CARGO_MANIFEST_DIR"),
            "/../../tests/gpu_shaders/04_loop.spv"));
        let glsl_output = dispatch_compute(&gpu, glsl_spv, &input, 256).unwrap();
        assert_close(&output, &glsl_output, 1e-6, "flow_loop_vs_glsl");
        eprintln!("Cross-validation [.flow vs GLSL loop]: PASS");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_flow_emitter_nested_loop_branch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };

        let tmp = std::env::temp_dir().join("octoflow_emit_nested.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/spirv_emit_nested.flow", &tmp);
        assert!(spirv.len() > 100, "nested SPIR-V too small: {} bytes", spirv.len());

        // 1. Validate with spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on nested shader: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [nested loop>branch>loop]: PASS");
        }

        // 2. Dispatch — nested: outer loop > branch > inner loop
        //   n = min(uint(val), 6); acc = 0
        //   for i = 1 to n:
        //     if float(i) > 3.0: s = sum(1..i); acc += s
        //     else: acc += float(i)
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| {
            let n = (x as u32).min(6);
            let mut acc = 0.0f32;
            for i in 1..=n {
                let fi = i as f32;
                if fi > 3.0 {
                    let s: f32 = (1..=i).map(|j| j as f32).sum();
                    acc += s;
                } else {
                    acc += fi;
                }
            }
            acc
        }).collect();
        assert_close(&output, &expected, 1e-6, "flow_emit_nested");
        eprintln!("GPU dispatch [nested loop>branch>loop]: PASS — {} elements", input.len());

        // 3. Cross-validate against GLSL reference
        let glsl_spv = include_bytes!(concat!(env!("CARGO_MANIFEST_DIR"),
            "/../../tests/gpu_shaders/11_nested_loop_branch.spv"));
        let glsl_output = dispatch_compute(&gpu, glsl_spv, &input, 256).unwrap();
        assert_close(&output, &glsl_output, 1e-6, "flow_nested_vs_glsl");
        eprintln!("Cross-validation [.flow vs GLSL nested]: PASS");

        std::fs::remove_file(&tmp).ok();
    }

    /// Phase 69: IR-generated SPIR-V — output[gid] = input[gid] * 2.0 + 1.0
    #[test]
    fn test_ir_linear() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_ir_linear.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/ir_test_linear.flow", &tmp);
        assert!(spirv.len() > 100, "IR SPIR-V too small: {} bytes", spirv.len());

        // 1. spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on IR linear: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [IR linear]: PASS");
        }

        // 2. GPU dispatch
        let input: Vec<f32> = (0..1024).map(|i| i as f32 * 0.5).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| x * 2.0 + 1.0).collect();
        for (i, (got, exp)) in output.iter().zip(expected.iter()).enumerate() {
            assert!((got - exp).abs() < 1e-6,
                "IR linear: mismatch at [{}]: got {} expected {}", i, got, exp);
        }
        eprintln!("IR linear: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_lower_arith() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_lower_arith.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/lower_test_arith.flow", &tmp);
        assert!(spirv.len() > 100, "Lower arith SPIR-V too small: {} bytes", spirv.len());

        // 1. spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on lower arith: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [lower arith]: PASS");
        }

        // 2. GPU dispatch — expected: input[gid] * 3.0 + 1.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32 * 0.5).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        let expected: Vec<f32> = input.iter().map(|&x| x * 3.0 + 1.0).collect();
        for (i, (got, exp)) in output.iter().zip(expected.iter()).enumerate() {
            assert!((got - exp).abs() < 1e-4,
                "Lower arith: mismatch at [{}]: got {} expected {}", i, got, exp);
        }
        eprintln!("Lower arith: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_lower_branch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_lower_branch.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/lower_test_branch.flow", &tmp);
        assert!(spirv.len() > 100, "Lower branch SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on lower branch: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [lower branch]: PASS");
        }

        // Expected: if input > 0.5 then input*2.0 else 0.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32 / 1024.0).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, (&got, &inp)) in output.iter().zip(input.iter()).enumerate() {
            let exp = if inp > 0.5 { inp * 2.0 } else { 0.0 };
            assert!((got - exp).abs() < 1e-4,
                "Lower branch: mismatch at [{}]: input={} got {} expected {}", i, inp, got, exp);
        }
        eprintln!("Lower branch: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_lower_vars() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_lower_vars.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/lower_test_vars.flow", &tmp);
        assert!(spirv.len() > 100, "Lower vars SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on lower vars: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [lower vars]: PASS");
        }

        // Expected: output[gid] = input[gid] * 2.0 + input[gid] = input[gid] * 3.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32 * 0.5).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, (got, exp)) in output.iter().zip(input.iter().map(|&x| x * 3.0)).enumerate() {
            assert!((got - exp).abs() < 1e-4,
                "Lower vars: mismatch at [{}]: got {} expected {}", i, got, exp);
        }
        eprintln!("Lower vars: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_lower_loop() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_lower_loop.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/lower_test_loop.flow", &tmp);
        assert!(spirv.len() > 100, "Lower loop SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on lower loop: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [lower loop]: PASS");
        }

        // Expected: every element = 45.0 (sum of 0..9)
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, &got) in output.iter().enumerate() {
            assert!((got - 45.0).abs() < 1e-4,
                "Lower loop: mismatch at [{}]: got {} expected 45.0", i, got);
        }
        eprintln!("Lower loop: GPU dispatch PASS (1024 elements, all 45.0)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_ir_loop() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_ir_loop.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/ir_test_loop.flow", &tmp);
        assert!(spirv.len() > 100, "IR loop SPIR-V too small: {} bytes", spirv.len());

        // 1. spirv-val
        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on IR loop: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [IR loop]: PASS");
        }

        // 2. GPU dispatch — every thread computes sum(0..9) = 45.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, &got) in output.iter().enumerate() {
            assert!((got - 45.0).abs() < 1e-6,
                "IR loop: mismatch at [{}]: got {} expected 45.0", i, got);
        }
        eprintln!("IR loop: GPU dispatch PASS (1024 elements, all 45.0)");

        std::fs::remove_file(&tmp).ok();
    }

    // ========== Phase 71: End-to-end compile tests ==========

    #[test]
    fn test_compile_arith() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_compile_arith.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/compile_test_arith.flow", &tmp);
        assert!(spirv.len() > 100, "Compile arith SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on compile arith: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [compile arith]: PASS");
        }

        // Expected: output[i] = input[i] * 3.0 + 1.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, &got) in output.iter().enumerate() {
            let expected = input[i] * 3.0 + 1.0;
            assert!((got - expected).abs() < 1e-4,
                "Compile arith: mismatch at [{}]: got {} expected {}", i, got, expected);
        }
        eprintln!("Compile arith: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_compile_branch() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_compile_branch.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/compile_test_branch.flow", &tmp);
        assert!(spirv.len() > 100, "Compile branch SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on compile branch: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [compile branch]: PASS");
        }

        // Expected: input > 0.5 → input * 2.0, else → 0.0
        let input: Vec<f32> = (0..1024).map(|i| i as f32 / 1024.0).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, &got) in output.iter().enumerate() {
            let v = input[i];
            let expected = if v > 0.5 { v * 2.0 } else { 0.0 };
            assert!((got - expected).abs() < 1e-4,
                "Compile branch: mismatch at [{}]: got {} expected {} (input={})", i, got, expected, v);
        }
        eprintln!("Compile branch: GPU dispatch PASS (1024 elements)");

        std::fs::remove_file(&tmp).ok();
    }

    #[test]
    fn test_compile_loop() {
        let gpu = match get_gpu() { Some(g) => g, None => return };
        let tmp = std::env::temp_dir().join("octoflow_compile_loop.spv");
        let spirv = run_flow_emitter_with_env(
            "stdlib/compiler/compile_test_loop.flow", &tmp);
        assert!(spirv.len() > 100, "Compile loop SPIR-V too small: {} bytes", spirv.len());

        if let Ok(o) = std::process::Command::new("spirv-val").arg(tmp.to_str().unwrap()).output() {
            assert!(o.status.success(), "spirv-val failed on compile loop: {}",
                String::from_utf8_lossy(&o.stderr));
            eprintln!("spirv-val [compile loop]: PASS");
        }

        // Expected: every element = 45.0 (sum of 0..9)
        let input: Vec<f32> = (0..1024).map(|i| i as f32).collect();
        let output = dispatch_compute(&gpu, &spirv, &input, 256).unwrap();
        for (i, &got) in output.iter().enumerate() {
            assert!((got - 45.0).abs() < 1e-4,
                "Compile loop: mismatch at [{}]: got {} expected 45.0", i, got);
        }
        eprintln!("Compile loop: GPU dispatch PASS (1024 elements, all 45.0)");

        std::fs::remove_file(&tmp).ok();
    }

    // ========== Phase 71: Compile + Run integration tests ==========
    // These test the full pipeline: .flow source → kparse → lower → ir → SPIR-V → gpu_compute → verify

    #[test]
    fn test_compile_and_run() {
        if get_gpu().is_none() { eprintln!("skip: no GPU"); return; }
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let flow_path = root.join("stdlib/compiler/compile_and_run.flow");
        assert!(flow_path.exists(), "compile_and_run.flow must exist");

        let exe = root.join("target/release/octoflow");
        let output = std::process::Command::new(&exe)
            .arg("run")
            .arg(flow_path.to_str().unwrap())
            .arg("--allow-write")
            .arg("--allow-read")
            .current_dir(&root)
            .output()
            .expect("failed to run octoflow");
        let stdout = String::from_utf8_lossy(&output.stdout);
        assert!(output.status.success(), "compile_and_run.flow failed:\n{}\n{}",
            stdout, String::from_utf8_lossy(&output.stderr));
        assert!(stdout.contains("COMPILE AND RUN: ALL PASS"),
            "Expected PASS in output:\n{}", stdout);
        eprintln!("Compile+Run: PASS (self-contained .flow compiles+dispatches GPU kernel)");

        // Cleanup
        let _ = std::fs::remove_file(root.join("compile_and_run_kernel.spv"));
    }

    #[test]
    fn test_compile_and_run_loop() {
        if get_gpu().is_none() { eprintln!("skip: no GPU"); return; }
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let flow_path = root.join("stdlib/compiler/compile_and_run_loop.flow");
        assert!(flow_path.exists(), "compile_and_run_loop.flow must exist");

        let exe = root.join("target/release/octoflow");
        let output = std::process::Command::new(&exe)
            .arg("run")
            .arg(flow_path.to_str().unwrap())
            .arg("--allow-write")
            .arg("--allow-read")
            .current_dir(&root)
            .output()
            .expect("failed to run octoflow");
        let stdout = String::from_utf8_lossy(&output.stdout);
        assert!(output.status.success(), "compile_and_run_loop.flow failed:\n{}\n{}",
            stdout, String::from_utf8_lossy(&output.stderr));
        assert!(stdout.contains("COMPILE+RUN LOOP: ALL PASS"),
            "Expected PASS in output:\n{}", stdout);
        eprintln!("Compile+Run Loop: PASS (while loop kernel compiled+dispatched from .flow)");

        // Cleanup
        let _ = std::fs::remove_file(root.join("compile_run_loop.spv"));
    }

    /// Phase 72b: Pure .flow Vulkan FFI dispatch — no Rust in the GPU call path.
    /// test_vk_dispatch.flow creates VkInstance, picks GPU, builds pipeline,
    /// dispatches 01_double.spv, and verifies output[i] == input[i] * 2.0.
    #[test]
    fn test_vk_ffi_dispatch() {
        if get_gpu().is_none() { eprintln!("skip: no GPU"); return; }
        let manifest = std::env::var("CARGO_MANIFEST_DIR").unwrap_or_else(|_| ".".to_string());
        let root = std::path::Path::new(&manifest).parent().unwrap().parent().unwrap();
        let flow_path = root.join("stdlib/loom/tests/misc/test_vk_dispatch.flow");
        assert!(flow_path.exists(), "test_vk_dispatch.flow must exist at {:?}", flow_path);

        let debug_exe = root.join(if cfg!(target_os = "windows") { "target/debug/octoflow.exe" } else { "target/debug/octoflow" });
        let release_exe = root.join(if cfg!(target_os = "windows") { "target/release/octoflow.exe" } else { "target/release/octoflow" });
        let exe = if debug_exe.exists() { debug_exe } else { release_exe };
        let output = std::process::Command::new(&exe)
            .arg("run")
            .arg(flow_path.to_str().unwrap())
            .arg("--allow-ffi")
            .arg("--allow-read")
            .current_dir(&root)
            .output()
            .expect("failed to run octoflow");
        let stdout = String::from_utf8_lossy(&output.stdout);
        assert!(output.status.success(), "test_vk_dispatch.flow failed:\n{}\n{}",
            stdout, String::from_utf8_lossy(&output.stderr));
        assert!(stdout.contains("VK FFI DISPATCH: ALL PASS"),
            "Expected PASS in output:\n{}", stdout);
        eprintln!("VK FFI dispatch: PASS (pure .flow Vulkan compute via FFI)");
    }

    #[test]
    fn test_f16_conversion_roundtrip() {
        let test_values: &[f32] = &[
            0.0, 1.0, -1.0, 0.5, -0.5, 3.14, 65504.0, -65504.0,
            0.00006103515625, // smallest normal f16
            f32::INFINITY, f32::NEG_INFINITY,
        ];
        for &val in test_values {
            let f16 = f32_to_f16(val);
            let back = f16_to_f32(f16);
            if val.is_infinite() {
                assert!(back.is_infinite(), "Inf roundtrip failed for {}", val);
                assert_eq!(val.is_sign_positive(), back.is_sign_positive());
            } else {
                let err = (val - back).abs();
                let tol = val.abs() * 0.001 + 1e-4; // f16 has ~0.1% relative error
                assert!(err < tol, "f16 roundtrip: {} -> {} -> {}, err={}", val, f16, back, err);
            }
        }
    }

    #[test]
    fn test_f16_special_values() {
        assert_eq!(f32_to_f16(0.0), 0x0000);
        assert_eq!(f32_to_f16(f32::INFINITY), 0x7C00);
        assert_eq!(f32_to_f16(f32::NEG_INFINITY), 0xFC00);
        assert_eq!(f16_to_f32(0x0000), 0.0);
        assert!(f16_to_f32(0x7C00).is_infinite());
        assert!(f16_to_f32(0x7C00).is_sign_positive());
    }

    #[test]
    fn test_upload_buffer_f16() {
        let gpu = match get_gpu() {
            Some(g) => g,
            None => { eprintln!("skip: no GPU"); return; }
        };
        let data = vec![1.0f32, 2.0, 3.0, 4.0, 0.5, -0.5, 100.0, 0.001];
        let buf = upload_buffer_f16(&gpu, &data).unwrap();
        // f16 buffer should be half the size of f32
        assert_eq!(buf.size_bytes, (data.len() * 2) as u64);
        assert_eq!(buf.len, data.len());
        eprintln!("f16 upload: {} elements, {} bytes (vs {} f32 bytes)",
            buf.len, buf.size_bytes, data.len() * 4);
    }

    #[test]
    fn test_gpu_supports_f16_query() {
        let gpu = match get_gpu() {
            Some(g) => g,
            None => { eprintln!("skip: no GPU"); return; }
        };
        eprintln!("GPU f16 support: {}", gpu.supports_f16);
        // Just verify the field is accessible and doesn't crash
    }

    #[test]
    fn test_int64_device_query() {
        let gpu = match get_gpu() {
            Some(g) => g,
            None => { eprintln!("skip: no GPU"); return; }
        };
        let info = gpu.gpu_properties();
        eprintln!("GPU int64 support: {}", info.supports_int64);
        // Turing+ GPUs (GTX 1660, RTX 20xx+) support shaderInt64
        // Just verify the field is queryable and doesn't crash
    }
}
